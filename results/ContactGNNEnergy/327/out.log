#### ARCHITECTURE ####
Node Encoder:
 Sequential(
  (0): Linear(1, 1000, bias=True)
  (1): PReLU(num_parameters=1)
  (2): Linear(1000, 1000, bias=True)
  (3): PReLU(num_parameters=1)
  (4): Linear(1000, 64, bias=True)
  (5): PReLU(num_parameters=1)
) 

Edge Encoder:
 None 

Model:
 Sequential(
  (0): WeightedGATv2Conv(64, 8, heads=8)
  (1): MLP(
  (model): Sequential(
    (0): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=64, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (1): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (2): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=64, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
  )
)
  (2): WeightedGATv2Conv(64, 8, heads=8)
  (3): MLP(
  (model): Sequential(
    (0): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=64, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (1): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (2): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=64, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
  )
)
  (4): WeightedGATv2Conv(64, 8, heads=8)
  (5): MLP(
  (model): Sequential(
    (0): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=64, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (1): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (2): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=64, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
  )
)
  (6): WeightedGATv2Conv(64, 8, heads=8)
  (7): MLP(
  (model): Sequential(
    (0): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=64, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (1): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (2): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=64, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
  )
)
) 

Head 1:
 Bilinear 

Head 2:
 Sequential(
  (0): MLP(
    (model): Sequential(
      (0): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=32768, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (1): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (2): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (3): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (4): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (5): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (6): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1024, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
    )
  )
  (1): FillDiagonalsFromArray()
) 

Namespace(GNN_mode=True, transforms=[], pre_transforms=['constant', 'ContactDistance', 'GeneticDistance_norm'], sparsify_threshold=None, sparsify_threshold_upper=None, top_k=None, use_node_features=False, use_edge_weights=False, use_edge_attr=True, keep_zero_edges=False, data_folder=['/project2/depablo/erschultz/dataset_12_20_22'], scratch='/scratch/midway2/erschultz', root_name='ContactGNNEnergy10', delete_root=False, toxx=False, toxx_mode='mean', y_preprocessing='sweeprand_log_inf', y_zero_diag_count=0, log_preprocessing=None, output_preprocesing=None, kr=False, mean_filt=None, rescale=2, gated=False, preprocessing_norm='mean', min_subtraction=True, x_reshape=True, ydtype=torch.float32, y_reshape=True, crop=None, classes=10, use_scratch=False, use_scratch_parallel=True, split_percents=[0.9, 0.1, 0.0], split_sizes=None, random_split=True, shuffle=True, batch_size=1, num_workers=4, start_epoch=1, n_epochs=100, save_mod=5, print_mod=2, lr=0.0001, weight_decay=0.0, gpus=1, milestones=[50], gamma=0.1, loss='mse', w_reg=None, reg_lambda=0.1, autoencoder_mode=False, verbose=False, print_params=True, output_mode='energy_sym_diag', model_type='ContactGNNEnergy', id=327, pretrained=False, resume_training=False, k=None, m=1024, seed=42, act='prelu', inner_act='prelu', out_act='prelu', training_norm=None, dropout=0.0, parameter_sharing=False, use_bias=True, message_passing='weighted_GAT', head_architecture='bilinear', head_architecture_2='fc-fill', head_hidden_sizes_list=[1000, 1000, 1000, 1000, 1000, 1000, 1024], encoder_hidden_sizes_list=[1000, 1000, 64], inner_hidden_sizes_list=None, edge_encoder_hidden_sizes_list=None, update_hidden_sizes_list=[1000, 1000, 64], head_act='prelu', num_heads=8, concat_heads=True, max_diagonal=None, mlp_model_id=None, kernel_w_list=None, hidden_sizes_list=[8, 8, 8, 8], nf=None, dilation_list=None, dilation_list_trunk=None, bottleneck=None, dilation_list_head=None, down_sampling=None, plot=True, plot_predictions=True, ofile_folder='/home/erschultz/sequences_to_contact_maps/results/ContactGNNEnergy/327', log_file_path='/home/erschultz/sequences_to_contact_maps/results/ContactGNNEnergy/327/out.log', log_file=<_io.TextIOWrapper name='/home/erschultz/sequences_to_contact_maps/results/ContactGNNEnergy/327/out.log' mode='a' encoding='UTF-8'>, param_file=<_io.TextIOWrapper name='/home/erschultz/sequences_to_contact_maps/results/ContactGNNEnergy/327/params.log' mode='a' encoding='UTF-8'>, split_neg_pos_edges=False, criterion=<function mse_loss at 0x7f9c895119d0>, channels=1, node_feature_size=1, input_m=512, edge_transforms=["'ContactDistance'", 'ContactDistance(norm=False)', 'GeneticDistance(norm=True)'], node_transforms=['Constant(value=1.0)'], edge_dim=2, transforms_processed=None, diag=False, pre_transforms_processed=Compose([
  Constant(value=1.0),
  ContactDistance(norm=False),
  GeneticDistance(norm=True)
]), cuda=True, use_parallel=False, device=device(type='cuda'))

Dataset construction time: 0.099 minutes
Average num edges per graph:  nan
split sizes: train=4500, val=500, test=0, N=5000
Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0.0
)
#### TRAINING/VALIDATION ####
Epoch 2, loss = 0.3475
Mean test/val loss: 0.3138
[25, 50, 75] quantiles test/val loss: [0.132  0.2442 0.4153]

Epoch 4, loss = 0.2829
Mean test/val loss: 0.2710
[25, 50, 75] quantiles test/val loss: [0.1269 0.2084 0.3313]

Epoch 6, loss = 0.2491
Mean test/val loss: 0.2585
[25, 50, 75] quantiles test/val loss: [0.1045 0.1901 0.3254]

Epoch 8, loss = 0.2323
Mean test/val loss: 0.2501
[25, 50, 75] quantiles test/val loss: [0.1024 0.188  0.3237]

Epoch 10, loss = 0.2264
Mean test/val loss: 0.2387
[25, 50, 75] quantiles test/val loss: [0.1012 0.1798 0.2967]

Epoch 12, loss = 0.2151
Mean test/val loss: 0.2154
[25, 50, 75] quantiles test/val loss: [0.0785 0.1508 0.2709]

Epoch 14, loss = 0.2095
Mean test/val loss: 0.2237
[25, 50, 75] quantiles test/val loss: [0.0769 0.158  0.2919]

Epoch 16, loss = 0.2182
Mean test/val loss: 0.2133
[25, 50, 75] quantiles test/val loss: [0.0801 0.1516 0.28  ]

Epoch 18, loss = 0.2029
Mean test/val loss: 0.2005
[25, 50, 75] quantiles test/val loss: [0.0721 0.1448 0.2543]

Epoch 20, loss = 0.1976
Mean test/val loss: 0.2129
[25, 50, 75] quantiles test/val loss: [0.0824 0.1571 0.2723]

Epoch 22, loss = 0.1938
Mean test/val loss: 0.2008
[25, 50, 75] quantiles test/val loss: [0.0759 0.1454 0.2595]

Epoch 24, loss = 0.1879
Mean test/val loss: 0.2658
[25, 50, 75] quantiles test/val loss: [0.0689 0.1367 0.2375]

Epoch 26, loss = 0.1935
Mean test/val loss: 0.1954
[25, 50, 75] quantiles test/val loss: [0.0695 0.1456 0.2468]

Epoch 28, loss = 0.1910
Mean test/val loss: 0.1926
[25, 50, 75] quantiles test/val loss: [0.0702 0.1391 0.2382]

Epoch 30, loss = 0.1826
Mean test/val loss: 0.1897
[25, 50, 75] quantiles test/val loss: [0.0657 0.1354 0.2397]

Epoch 32, loss = 0.2455
Mean test/val loss: 0.2390
[25, 50, 75] quantiles test/val loss: [0.1026 0.1816 0.3113]

Epoch 34, loss = 0.2132
Mean test/val loss: 0.2162
[25, 50, 75] quantiles test/val loss: [0.0895 0.1596 0.2749]

Epoch 36, loss = 0.1998
Mean test/val loss: 0.2043
[25, 50, 75] quantiles test/val loss: [0.0797 0.1496 0.2581]

Epoch 38, loss = 0.1818
Mean test/val loss: 0.1893
[25, 50, 75] quantiles test/val loss: [0.0721 0.1365 0.2394]

Epoch 40, loss = 0.1880
Mean test/val loss: 0.1860
[25, 50, 75] quantiles test/val loss: [0.0677 0.1335 0.2305]

Epoch 42, loss = 63132.2084
Mean test/val loss: 0.3141
[25, 50, 75] quantiles test/val loss: [0.1247 0.2252 0.4224]

Epoch 44, loss = 0.2639
Mean test/val loss: 0.2632
[25, 50, 75] quantiles test/val loss: [0.1108 0.1962 0.3362]

Epoch 46, loss = 0.2259
Mean test/val loss: 0.2201
[25, 50, 75] quantiles test/val loss: [0.0863 0.1662 0.2853]

Epoch 48, loss = 0.1860
Mean test/val loss: 0.1878
[25, 50, 75] quantiles test/val loss: [0.0695 0.1354 0.2324]

Epoch 50, loss = 0.8989
Mean test/val loss: 0.2713
[25, 50, 75] quantiles test/val loss: [0.104  0.2027 0.3572]

Epoch 52, loss = 0.2199
Mean test/val loss: 0.2212
[25, 50, 75] quantiles test/val loss: [0.0814 0.1619 0.2781]

Epoch 54, loss = 0.1866
Mean test/val loss: 0.1874
[25, 50, 75] quantiles test/val loss: [0.0637 0.1371 0.2317]

Epoch 56, loss = 0.1700
Mean test/val loss: 0.1791
[25, 50, 75] quantiles test/val loss: [0.064  0.1278 0.2218]

Epoch 58, loss = 0.1643
Mean test/val loss: 0.1754
[25, 50, 75] quantiles test/val loss: [0.0593 0.1259 0.2209]

Epoch 60, loss = 0.1616
Mean test/val loss: 0.1742
[25, 50, 75] quantiles test/val loss: [0.0586 0.1241 0.2178]

Epoch 62, loss = 0.1597
Mean test/val loss: 0.1730
[25, 50, 75] quantiles test/val loss: [0.0579 0.1219 0.216 ]

Epoch 64, loss = 0.1582
Mean test/val loss: 0.1714
[25, 50, 75] quantiles test/val loss: [0.0572 0.1205 0.2151]

Epoch 66, loss = 0.1567
Mean test/val loss: 0.1716
[25, 50, 75] quantiles test/val loss: [0.0575 0.1194 0.2137]

Epoch 68, loss = 0.1557
Mean test/val loss: 0.1707
[25, 50, 75] quantiles test/val loss: [0.0563 0.1215 0.2135]

Epoch 70, loss = 0.1546
Mean test/val loss: 0.1717
[25, 50, 75] quantiles test/val loss: [0.0576 0.1193 0.2126]

Epoch 72, loss = 0.1535
Mean test/val loss: 0.1709
[25, 50, 75] quantiles test/val loss: [0.0567 0.1195 0.2128]

Epoch 74, loss = 0.1525
Mean test/val loss: 0.1698
[25, 50, 75] quantiles test/val loss: [0.0558 0.1179 0.2108]

Epoch 76, loss = 0.1517
Mean test/val loss: 0.1698
[25, 50, 75] quantiles test/val loss: [0.056  0.117  0.2113]

Epoch 78, loss = 0.1508
Mean test/val loss: 0.1695
[25, 50, 75] quantiles test/val loss: [0.0567 0.118  0.2096]

Epoch 80, loss = 0.1499
Mean test/val loss: 0.1689
[25, 50, 75] quantiles test/val loss: [0.0558 0.1155 0.2099]

Epoch 82, loss = 0.1491
Mean test/val loss: 0.1678
[25, 50, 75] quantiles test/val loss: [0.0566 0.1149 0.2096]

Epoch 84, loss = 0.1483
Mean test/val loss: 0.1688
[25, 50, 75] quantiles test/val loss: [0.0557 0.1161 0.2086]

Epoch 86, loss = 0.1475
Mean test/val loss: 0.1678
[25, 50, 75] quantiles test/val loss: [0.0547 0.1154 0.2081]

Epoch 88, loss = 0.1467
Mean test/val loss: 0.1678
[25, 50, 75] quantiles test/val loss: [0.0545 0.1153 0.2093]

Epoch 90, loss = 0.1460
Mean test/val loss: 0.1681
[25, 50, 75] quantiles test/val loss: [0.0571 0.1162 0.2093]

Epoch 92, loss = 0.1453
Mean test/val loss: 0.1670
[25, 50, 75] quantiles test/val loss: [0.055  0.1152 0.2098]

Epoch 94, loss = 0.1446
Mean test/val loss: 0.1671
[25, 50, 75] quantiles test/val loss: [0.0549 0.1165 0.2081]

Epoch 96, loss = 0.1439
Mean test/val loss: 0.1666
[25, 50, 75] quantiles test/val loss: [0.0544 0.1149 0.2094]

Epoch 98, loss = 0.1433
Mean test/val loss: 0.1668
[25, 50, 75] quantiles test/val loss: [0.0556 0.1147 0.2064]

Epoch 100, loss = 0.1427
Mean test/val loss: 0.1666
[25, 50, 75] quantiles test/val loss: [0.0549 0.1163 0.208 ]


Total parameters: 44424748
Total training + validation time: 21.0 hours, 35.0 mins, and 55.39999999999418 secs
Final val loss: 0.1666274784039706

split sizes: train=4500, val=500, test=0, N=5000
#### Plotting Script ####
Prediction Results:
dataset_12_20_22 sample981: 0.15622523427009583
dataset_12_20_22 sample324: 0.38907355070114136
dataset_12_20_22 sample3464: 0.023064984008669853
dataset_12_20_22 sample2834: 0.1749604344367981
dataset_12_20_22 sample1936: 0.28440871834754944
Loss: 0.206 +- 0.124

Downsampling (40%) Results:
dataset_12_20_22 sample1936-downsampling: 11.877349853515625
dataset_12_20_22 sample2834-downsampling: 9.691475868225098
dataset_12_20_22 sample324-downsampling: 7.492387771606445
dataset_12_20_22 sample3464-downsampling: 63.45924377441406
dataset_12_20_22 sample981-downsampling: 16.01376724243164
Loss: 21.707 +- 21.065

Removing /project2/depablo/erschultz/dataset_12_20_22/ContactGNNEnergy10downsample
Original sampling (100%) Results:
dataset_12_20_22 sample1936-regular: 11.186086654663086
dataset_12_20_22 sample2834-regular: 9.787578582763672
dataset_12_20_22 sample324-regular: 7.170381546020508
dataset_12_20_22 sample3464-regular: 63.243873596191406
dataset_12_20_22 sample981-regular: 15.812543869018555
Loss: 21.44 +- 21.089

Removing /project2/depablo/erschultz/dataset_12_20_22/ContactGNNEnergy10regsample
Upsampling (200%) Results:
