#### ARCHITECTURE ####
Node Encoder:
 Sequential(
  (0): Linear(1, 1000, bias=True)
  (1): PReLU(num_parameters=1)
  (2): Linear(1000, 1000, bias=True)
  (3): PReLU(num_parameters=1)
  (4): Linear(1000, 64, bias=True)
  (5): PReLU(num_parameters=1)
) 

Edge Encoder:
 None 

Linear:
 Linear(in_features=128, out_features=64, bias=True) 

Model:
 Sequential(
  (0): WeightedGATv2Conv(64, 8, heads=8)
  (1): MLP(
  (model): Sequential(
    (0): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=64, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (1): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (2): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=64, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
  )
)
  (2): WeightedGATv2Conv(64, 8, heads=8)
  (3): MLP(
  (model): Sequential(
    (0): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=64, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (1): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (2): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=64, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
  )
)
  (4): WeightedGATv2Conv(64, 8, heads=8)
  (5): MLP(
  (model): Sequential(
    (0): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=64, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (1): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (2): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=64, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
  )
)
  (6): WeightedGATv2Conv(64, 8, heads=8)
  (7): MLP(
  (model): Sequential(
    (0): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=64, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (1): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (2): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=64, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
  )
)
) 

Head 1:
 Bilinear 

Head 2:
 Sequential(
  (0): MLP(
    (model): Sequential(
      (0): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=32768, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (1): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (2): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (3): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (4): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (5): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (6): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1024, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
    )
  )
  (1): FillDiagonalsFromArray()
) 

#### ARCHITECTURE ####
Sign Net:
 SignNet(
  (phi): GNN3d(
    (convs): ModuleList(
      (0): MaskedGINConv(
        (nn): MaskedMLP(
          (layers): ModuleList(
            (0): Linear(in_features=1, out_features=64, bias=False)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (norms): ModuleList(
            (0): MaskedBN(
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): MaskedBN(
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
        (layer): GINConv(nn=Identity())
      )
      (1): MaskedGINConv(
        (nn): MaskedMLP(
          (layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=False)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (norms): ModuleList(
            (0): MaskedBN(
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): MaskedBN(
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
        (layer): GINConv(nn=Identity())
      )
      (2): MaskedGINConv(
        (nn): MaskedMLP(
          (layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=False)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (norms): ModuleList(
            (0): MaskedBN(
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): MaskedBN(
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
        (layer): GINConv(nn=Identity())
      )
      (3): MaskedGINConv(
        (nn): MaskedMLP(
          (layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=False)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (norms): ModuleList(
            (0): MaskedBN(
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): MaskedBN(
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
        (layer): GINConv(nn=Identity())
      )
      (4): MaskedGINConv(
        (nn): MaskedMLP(
          (layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=False)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (norms): ModuleList(
            (0): MaskedBN(
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): MaskedBN(
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
        (layer): GINConv(nn=Identity())
      )
      (5): MaskedGINConv(
        (nn): MaskedMLP(
          (layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=False)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (norms): ModuleList(
            (0): MaskedBN(
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): MaskedBN(
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
        (layer): GINConv(nn=Identity())
      )
      (6): MaskedGINConv(
        (nn): MaskedMLP(
          (layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=False)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (norms): ModuleList(
            (0): MaskedBN(
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): MaskedBN(
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
        (layer): GINConv(nn=Identity())
      )
      (7): MaskedGINConv(
        (nn): MaskedMLP(
          (layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=False)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (norms): ModuleList(
            (0): MaskedBN(
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): MaskedBN(
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
        (layer): GINConv(nn=Identity())
      )
    )
    (norms): ModuleList(
      (0): MaskedBN(
        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): MaskedBN(
        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): MaskedBN(
        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): MaskedBN(
        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): MaskedBN(
        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (5): MaskedBN(
        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (6): MaskedBN(
        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (7): MaskedBN(
        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (rho): SetTransformer(
    (transformer_layers): ModuleList(
      (0): TransformerEncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=64, out_features=64, bias=False)
          (w_ks): Linear(in_features=64, out_features=64, bias=False)
          (w_vs): Linear(in_features=64, out_features=64, bias=False)
          (fc): Linear(in_features=64, out_features=64, bias=False)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0, inplace=False)
          (norm): MaskedLN(
            (ln): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          )
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=64, out_features=64, bias=True)
          (w_2): Linear(in_features=64, out_features=64, bias=True)
          (norm): MaskedLN(
            (ln): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          )
          (dropout): Dropout(p=0, inplace=False)
        )
      )
      (1): TransformerEncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=64, out_features=64, bias=False)
          (w_ks): Linear(in_features=64, out_features=64, bias=False)
          (w_vs): Linear(in_features=64, out_features=64, bias=False)
          (fc): Linear(in_features=64, out_features=64, bias=False)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0, inplace=False)
          (norm): MaskedLN(
            (ln): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          )
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=64, out_features=64, bias=True)
          (w_2): Linear(in_features=64, out_features=64, bias=True)
          (norm): MaskedLN(
            (ln): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          )
          (dropout): Dropout(p=0, inplace=False)
        )
      )
      (2): TransformerEncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=64, out_features=64, bias=False)
          (w_ks): Linear(in_features=64, out_features=64, bias=False)
          (w_vs): Linear(in_features=64, out_features=64, bias=False)
          (fc): Linear(in_features=64, out_features=64, bias=False)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0, inplace=False)
          (norm): MaskedLN(
            (ln): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          )
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=64, out_features=64, bias=True)
          (w_2): Linear(in_features=64, out_features=64, bias=True)
          (norm): MaskedLN(
            (ln): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          )
          (dropout): Dropout(p=0, inplace=False)
        )
      )
      (3): TransformerEncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=64, out_features=64, bias=False)
          (w_ks): Linear(in_features=64, out_features=64, bias=False)
          (w_vs): Linear(in_features=64, out_features=64, bias=False)
          (fc): Linear(in_features=64, out_features=64, bias=False)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0, inplace=False)
          (norm): MaskedLN(
            (ln): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          )
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=64, out_features=64, bias=True)
          (w_2): Linear(in_features=64, out_features=64, bias=True)
          (norm): MaskedLN(
            (ln): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          )
          (dropout): Dropout(p=0, inplace=False)
        )
      )
      (4): TransformerEncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=64, out_features=64, bias=False)
          (w_ks): Linear(in_features=64, out_features=64, bias=False)
          (w_vs): Linear(in_features=64, out_features=64, bias=False)
          (fc): Linear(in_features=64, out_features=64, bias=False)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0, inplace=False)
          (norm): MaskedLN(
            (ln): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          )
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=64, out_features=64, bias=True)
          (w_2): Linear(in_features=64, out_features=64, bias=True)
          (norm): MaskedLN(
            (ln): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          )
          (dropout): Dropout(p=0, inplace=False)
        )
      )
      (5): TransformerEncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=64, out_features=64, bias=False)
          (w_ks): Linear(in_features=64, out_features=64, bias=False)
          (w_vs): Linear(in_features=64, out_features=64, bias=False)
          (fc): Linear(in_features=64, out_features=64, bias=False)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0, inplace=False)
          (norm): MaskedLN(
            (ln): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          )
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=64, out_features=64, bias=True)
          (w_2): Linear(in_features=64, out_features=64, bias=True)
          (norm): MaskedLN(
            (ln): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          )
          (dropout): Dropout(p=0, inplace=False)
        )
      )
      (6): TransformerEncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=64, out_features=64, bias=False)
          (w_ks): Linear(in_features=64, out_features=64, bias=False)
          (w_vs): Linear(in_features=64, out_features=64, bias=False)
          (fc): Linear(in_features=64, out_features=64, bias=False)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0, inplace=False)
          (norm): MaskedLN(
            (ln): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          )
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=64, out_features=64, bias=True)
          (w_2): Linear(in_features=64, out_features=64, bias=True)
          (norm): MaskedLN(
            (ln): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          )
          (dropout): Dropout(p=0, inplace=False)
        )
      )
      (7): TransformerEncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=64, out_features=64, bias=False)
          (w_ks): Linear(in_features=64, out_features=64, bias=False)
          (w_vs): Linear(in_features=64, out_features=64, bias=False)
          (fc): Linear(in_features=64, out_features=64, bias=False)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0, inplace=False)
          (norm): MaskedLN(
            (ln): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          )
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=64, out_features=64, bias=True)
          (w_2): Linear(in_features=64, out_features=64, bias=True)
          (norm): MaskedLN(
            (ln): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          )
          (dropout): Dropout(p=0, inplace=False)
        )
      )
    )
    (out): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=False)
      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (eigen_encoder): MaskedMLP(
    (layers): ModuleList(
      (0): Linear(in_features=1, out_features=1, bias=False)
      (1): Linear(in_features=1, out_features=64, bias=False)
    )
    (norms): ModuleList(
      (0): MaskedBN(
        (bn): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): MaskedBN(
        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
) 

Namespace(GNN_mode=True, transforms=[], pre_transforms=['constant', 'ContactDistance', 'GeneticDistance_norm', 'AdjPCs_8'], sparsify_threshold=None, sparsify_threshold_upper=None, top_k=None, use_node_features=False, use_edge_weights=False, use_edge_attr=True, keep_zero_edges=False, data_folder=['/project/depablo/erschultz/dataset_11_18_22', '/project/depablo/erschultz/dataset_11_21_22'], scratch='/scratch/midway2/erschultz', root_name='ContactGNNEnergy0', delete_root=False, toxx=False, toxx_mode='mean', y_preprocessing='sweeprand_log_inf', y_zero_diag_count=0, log_preprocessing=None, output_preprocesing=None, kr=False, mean_filt=None, rescale=2, gated=False, preprocessing_norm='mean', min_subtraction=True, x_reshape=True, ydtype=torch.float32, y_reshape=True, crop=None, classes=10, use_scratch=False, use_scratch_parallel=True, split_percents=[0.9, 0.1, 0.0], split_sizes=None, random_split=True, shuffle=True, batch_size=2, num_workers=4, start_epoch=1, n_epochs=100, save_mod=5, print_mod=2, lr=0.0001, weight_decay=0.0, gpus=1, milestones=[50], gamma=0.1, loss='mse', w_reg=None, reg_lambda=0.1, autoencoder_mode=False, verbose=False, print_params=True, output_mode='energy_sym_diag', model_type='ContactGNNEnergy', id=336, pretrained=False, resume_training=False, k=8, m=1024, seed=42, act='prelu', inner_act='prelu', out_act='prelu', training_norm=None, dropout=0.0, parameter_sharing=False, use_bias=True, use_sign_net=True, message_passing='weighted_GAT', head_architecture='bilinear_triu', head_architecture_2='fc-fill', head_hidden_sizes_list=[1000, 1000, 1000, 1000, 1000, 1000, 1024], encoder_hidden_sizes_list=[1000, 1000, 64], inner_hidden_sizes_list=None, edge_encoder_hidden_sizes_list=None, update_hidden_sizes_list=[1000, 1000, 64], head_act='prelu', num_heads=8, concat_heads=True, max_diagonal=None, mlp_model_id=None, kernel_w_list=None, hidden_sizes_list=[8, 8, 8, 8], nf=None, dilation_list=None, dilation_list_trunk=None, bottleneck=None, dilation_list_head=None, down_sampling=None, plot=True, plot_predictions=True, ofile_folder='/home/erschultz/sequences_to_contact_maps/results/ContactGNNEnergy/336', log_file_path='/home/erschultz/sequences_to_contact_maps/results/ContactGNNEnergy/336/out.log', log_file=<_io.TextIOWrapper name='/home/erschultz/sequences_to_contact_maps/results/ContactGNNEnergy/336/out.log' mode='a' encoding='UTF-8'>, param_file=<_io.TextIOWrapper name='/home/erschultz/sequences_to_contact_maps/results/ContactGNNEnergy/336/params.log' mode='a' encoding='UTF-8'>, split_neg_pos_edges=False, criterion=<function mse_loss at 0x7f0d0f82a280>, channels=1, node_feature_size=1, input_m=512, edge_transforms=["'ContactDistance'", 'ContactDistance(norm=False)', 'GeneticDistance(norm=True)'], node_transforms=['AdjPCs(k=8, normalize=False, sign_net=True)', 'Constant(value=1.0)'], edge_dim=2, transforms_processed=None, diag=True, pre_transforms_processed=Compose([
  Constant(value=1.0),
  ContactDistance(norm=False),
  GeneticDistance(norm=True),
  AdjPCs(k=8, normalize=False, sign_net=True)
]), cuda=True, use_parallel=False, device=device(type='cuda'))

Dataset construction time: 39.205 minutes
Average num edges per graph:  222294.36527196653
Mean degree: [362.66 512.   449.47 ... 329.12 511.93 511.26] +- [70.54  0.   53.89 ... 80.99  0.29  1.69]

split sizes: train=4302, val=478, test=0, N=4780
Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0.0
)
#### TRAINING/VALIDATION ####
Epoch 2, loss = 11.1182
Mean test/val loss: 10.5433
[25, 50, 75] quantiles test/val loss: [ 7.6864  9.83   12.2257]

Epoch 4, loss = 9.9831
Mean test/val loss: 10.1499
[25, 50, 75] quantiles test/val loss: [ 7.1752  9.0231 11.4092]

Epoch 6, loss = 9.2086
Mean test/val loss: 9.4406
[25, 50, 75] quantiles test/val loss: [ 6.9242  8.6684 10.4429]

Epoch 8, loss = 8.7131
Mean test/val loss: 9.2331
[25, 50, 75] quantiles test/val loss: [ 6.3204  8.2809 10.5995]

Epoch 10, loss = 8.3282
Mean test/val loss: 8.2592
[25, 50, 75] quantiles test/val loss: [5.7233 7.6107 9.2348]

Epoch 12, loss = 8.1898
Mean test/val loss: 8.1164
[25, 50, 75] quantiles test/val loss: [5.8399 7.4142 9.2365]

Epoch 14, loss = 7.9089
Mean test/val loss: 13.3376
[25, 50, 75] quantiles test/val loss: [ 9.4183 12.0094 15.3942]

Epoch 16, loss = 7.7664
Mean test/val loss: 8.0297
[25, 50, 75] quantiles test/val loss: [5.5434 7.2798 8.8309]

Epoch 18, loss = 7.6374
Mean test/val loss: 7.9990
[25, 50, 75] quantiles test/val loss: [5.6707 7.2037 9.0282]

Epoch 20, loss = 7.5401
Mean test/val loss: 8.1855
[25, 50, 75] quantiles test/val loss: [5.7705 7.4404 9.2451]

Epoch 22, loss = 7.3310
Mean test/val loss: 7.5711
[25, 50, 75] quantiles test/val loss: [5.2249 7.0312 8.7079]

Epoch 24, loss = 7.1908
Mean test/val loss: 7.5737
[25, 50, 75] quantiles test/val loss: [5.2597 6.8962 8.8174]

Epoch 26, loss = 7.1596
Mean test/val loss: 8.9383
[25, 50, 75] quantiles test/val loss: [6.9713 8.3053 9.9328]

Epoch 28, loss = 7.0537
Mean test/val loss: 7.9445
[25, 50, 75] quantiles test/val loss: [5.6993 7.2898 9.1419]

Epoch 30, loss = 6.8679
Mean test/val loss: 8.5495
[25, 50, 75] quantiles test/val loss: [6.4035 7.8168 9.3784]

Epoch 32, loss = 6.7912
Mean test/val loss: 7.5480
[25, 50, 75] quantiles test/val loss: [5.5377 6.7104 8.563 ]

Epoch 34, loss = 6.7200
Mean test/val loss: 7.2307
[25, 50, 75] quantiles test/val loss: [5.0227 6.2949 8.2321]

Epoch 36, loss = 6.6751
Mean test/val loss: 7.1765
[25, 50, 75] quantiles test/val loss: [5.024  6.3221 8.0654]

Epoch 38, loss = 6.4888
Mean test/val loss: 7.3272
[25, 50, 75] quantiles test/val loss: [5.2918 6.7062 8.3573]

Epoch 40, loss = 6.5841
Mean test/val loss: 7.3853
[25, 50, 75] quantiles test/val loss: [5.5355 6.5084 8.2821]

Epoch 42, loss = 6.3375
Mean test/val loss: 6.8964
[25, 50, 75] quantiles test/val loss: [4.885  6.0663 7.9562]

Epoch 44, loss = 6.3231
Mean test/val loss: 6.8839
[25, 50, 75] quantiles test/val loss: [4.7427 5.9931 7.5803]

Epoch 46, loss = 7.1335
Mean test/val loss: 7.3419
[25, 50, 75] quantiles test/val loss: [5.1811 6.4864 8.2704]

Epoch 48, loss = 6.3220
Mean test/val loss: 6.7893
[25, 50, 75] quantiles test/val loss: [4.6559 6.1543 7.7061]

Epoch 50, loss = 6.2580
Mean test/val loss: 7.4764
[25, 50, 75] quantiles test/val loss: [5.228  6.8439 8.5815]

Epoch 52, loss = 5.7720
Mean test/val loss: 6.5123
[25, 50, 75] quantiles test/val loss: [4.2556 5.6918 7.5684]

Epoch 54, loss = 5.7132
Mean test/val loss: 6.4936
[25, 50, 75] quantiles test/val loss: [4.3533 5.6506 7.6113]

Epoch 56, loss = 5.6764
Mean test/val loss: 6.4640
[25, 50, 75] quantiles test/val loss: [4.244  5.8175 7.3446]

Epoch 58, loss = 5.6408
Mean test/val loss: 6.4272
[25, 50, 75] quantiles test/val loss: [4.254  5.6488 7.281 ]

Epoch 60, loss = 5.6166
Mean test/val loss: 6.4338
[25, 50, 75] quantiles test/val loss: [4.3251 5.5881 7.416 ]

Epoch 62, loss = 5.5915
Mean test/val loss: 6.4359
[25, 50, 75] quantiles test/val loss: [4.3052 5.7294 7.1475]

Epoch 64, loss = 5.5683
Mean test/val loss: 6.4212
[25, 50, 75] quantiles test/val loss: [4.6063 5.7484 7.3972]

Epoch 66, loss = 5.5479
Mean test/val loss: 6.4021
[25, 50, 75] quantiles test/val loss: [4.1629 5.6404 7.3068]

Epoch 68, loss = 5.5276
Mean test/val loss: 6.3704
[25, 50, 75] quantiles test/val loss: [4.3058 5.724  7.1534]

Epoch 70, loss = 5.5096
Mean test/val loss: 6.3665
[25, 50, 75] quantiles test/val loss: [4.0679 5.7737 7.0689]

Epoch 72, loss = 5.4897
Mean test/val loss: 6.3367
[25, 50, 75] quantiles test/val loss: [4.1156 5.6176 7.3444]

Epoch 74, loss = 5.4747
Mean test/val loss: 6.3912
[25, 50, 75] quantiles test/val loss: [4.1731 5.7145 7.3061]

Epoch 76, loss = 5.4575
Mean test/val loss: 6.3917
[25, 50, 75] quantiles test/val loss: [4.0731 5.6509 7.4652]

Epoch 78, loss = 5.4428
Mean test/val loss: 6.3890
[25, 50, 75] quantiles test/val loss: [4.2405 5.6273 7.4556]

Epoch 80, loss = 5.4268
Mean test/val loss: 6.4133
[25, 50, 75] quantiles test/val loss: [4.1051 5.5009 7.2573]

Epoch 82, loss = 5.4165
Mean test/val loss: 6.3382
[25, 50, 75] quantiles test/val loss: [4.2242 5.5588 7.0378]

Epoch 84, loss = 5.4005
Mean test/val loss: 6.4321
[25, 50, 75] quantiles test/val loss: [4.0607 5.7236 7.158 ]

Epoch 86, loss = 5.3845
Mean test/val loss: 6.3447
[25, 50, 75] quantiles test/val loss: [4.0236 5.501  7.2859]

Epoch 88, loss = 5.3731
Mean test/val loss: 6.4031
[25, 50, 75] quantiles test/val loss: [3.9741 5.6002 7.3705]

Epoch 90, loss = 5.3563
Mean test/val loss: 6.3661
[25, 50, 75] quantiles test/val loss: [4.1485 5.4795 7.2373]

Epoch 92, loss = 5.3473
Mean test/val loss: 6.3460
[25, 50, 75] quantiles test/val loss: [4.0484 5.3952 7.2733]

Epoch 94, loss = 5.3313
Mean test/val loss: 6.3622
[25, 50, 75] quantiles test/val loss: [4.0387 5.4521 7.0119]

Epoch 96, loss = 5.3181
Mean test/val loss: 6.4696
[25, 50, 75] quantiles test/val loss: [3.9881 5.6835 7.4486]

Epoch 98, loss = 5.3088
Mean test/val loss: 6.3349
[25, 50, 75] quantiles test/val loss: [4.0367 5.4599 7.1882]

Epoch 100, loss = 5.2978
Mean test/val loss: 6.3494
[25, 50, 75] quantiles test/val loss: [4.0977 5.5107 7.1757]


Total parameters: 44700183
Total training + validation time: 39.0 hours, 45.0 mins, and 26.10000000000582 secs
Final val loss: 6.349431273079317

split sizes: train=4302, val=478, test=0, N=4780
#### Plotting Script ####
Prediction Results:
dataset_11_21_22 sample939: 7.813112258911133
dataset_11_18_22 sample203: 7.549637794494629
dataset_11_21_22 sample743: 7.9601006507873535
dataset_11_21_22 sample45: 3.1355724334716797
dataset_11_18_22 sample559: 3.011678695678711
Loss: 5.894 +- 2.307

Downsampling (40%) Results:
dataset_11_18_22 sample203-downsampling: 7.8328094482421875
dataset_11_18_22 sample45-downsampling: 17.642539978027344
dataset_11_18_22 sample559-downsampling: 3.0116782188415527
dataset_11_18_22 sample743-downsampling: 11.332555770874023
dataset_11_18_22 sample939-downsampling: 3.8267264366149902
Loss: 6.761 +- 4.84

Removing /project/depablo/erschultz/dataset_11_18_22/ContactGNNEnergy0downsample
Original sampling (100%) Results:
dataset_11_18_22 sample203-regular: 7.381669521331787
dataset_11_18_22 sample45-regular: 14.816793441772461
dataset_11_18_22 sample559-regular: 3.3122243881225586
dataset_11_18_22 sample743-regular: 9.595922470092773
dataset_11_18_22 sample939-regular: 3.565314769744873
Loss: 6.044 +- 4.068

Removing /project/depablo/erschultz/dataset_11_18_22/ContactGNNEnergy0regsample
Upsampling (200%) Results:
dataset_11_18_22 sample203-upsampling: 6.9735236167907715
dataset_11_18_22 sample45-upsampling: 14.656759262084961
dataset_11_18_22 sample559-upsampling: 3.5083930492401123
dataset_11_18_22 sample743-upsampling: 9.472677230834961
dataset_11_18_22 sample939-upsampling: 3.4702301025390625
Loss: 6.016 +- 4.086

Removing /project/depablo/erschultz/dataset_11_18_22/ContactGNNEnergy0upsample
