#### ARCHITECTURE ####
Node Encoder:
 None 

Edge Encoder:
 None 

Model:
 Sequential(
  (0): WeightedGATv2Conv(1, 8, heads=8)
  (1): MLP(
  (model): Sequential(
    (0): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=64, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (1): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (2): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=64, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
  )
)
  (2): WeightedGATv2Conv(64, 8, heads=8)
  (3): MLP(
  (model): Sequential(
    (0): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=64, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (1): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (2): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=64, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
  )
)
  (4): WeightedGATv2Conv(64, 8, heads=8)
  (5): MLP(
  (model): Sequential(
    (0): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=64, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (1): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (2): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=64, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
  )
)
  (6): WeightedGATv2Conv(64, 8, heads=8)
  (7): MLP(
  (model): Sequential(
    (0): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=64, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (1): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (2): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=64, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
  )
)
) 

Head 1:
 Bilinear 

Head 2:
 Sequential(
  (0): MLP(
    (model): Sequential(
      (0): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=32768, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (1): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (2): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (3): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (4): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (5): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (6): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1024, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
    )
  )
  (1): FillDiagonalsFromArray()
) 

Namespace(GNN_mode=True, transforms=[], pre_transforms=['constant', 'ContactDistance', 'GeneticDistance_norm'], sparsify_threshold=None, sparsify_threshold_upper=None, top_k=None, use_node_features=False, use_edge_weights=False, use_edge_attr=True, keep_zero_edges=False, data_folder=['/project2/depablo/erschultz/dataset_11_18_22', '/project2/depablo/erschultz/dataset_11_21_22'], scratch='/scratch/midway2/erschultz', root_name='ContactGNNEnergy0', delete_root=False, toxx=False, toxx_mode='mean', y_preprocessing='sweeprand_log_inf', y_zero_diag_count=0, log_preprocessing=None, output_preprocesing=None, kr=False, mean_filt=None, rescale=2, gated=False, preprocessing_norm='mean', min_subtraction=True, x_reshape=True, ydtype=torch.float32, y_reshape=True, crop=None, classes=10, use_scratch=False, use_scratch_parallel=True, split_percents=[0.9, 0.1, 0.0], split_sizes=None, random_split=True, shuffle=True, batch_size=1, num_workers=4, start_epoch=1, n_epochs=100, save_mod=5, print_mod=2, lr=0.0001, weight_decay=0.0, gpus=1, milestones=[50], gamma=0.1, loss='mse', w_reg='l1', reg_lambda=10.0, autoencoder_mode=False, verbose=False, print_params=True, output_mode='energy_sym_diag', model_type='ContactGNNEnergy', id=324, pretrained=False, resume_training=False, k=None, m=1024, seed=42, act='prelu', inner_act='prelu', out_act='prelu', training_norm=None, dropout=0.0, parameter_sharing=False, use_bias=True, message_passing='weighted_GAT', head_architecture='bilinear_triu', head_architecture_2='fc-fill', head_hidden_sizes_list=[1000, 1000, 1000, 1000, 1000, 1000, 1024], encoder_hidden_sizes_list=None, inner_hidden_sizes_list=None, edge_encoder_hidden_sizes_list=None, update_hidden_sizes_list=[1000, 1000, 64], head_act='prelu', num_heads=8, concat_heads=True, max_diagonal=None, mlp_model_id=None, kernel_w_list=None, hidden_sizes_list=[8, 8, 8, 8], nf=None, dilation_list=None, dilation_list_trunk=None, bottleneck=None, dilation_list_head=None, down_sampling=None, plot=True, plot_predictions=True, ofile_folder='/home/erschultz/sequences_to_contact_maps/results/ContactGNNEnergy/324', log_file_path='/home/erschultz/sequences_to_contact_maps/results/ContactGNNEnergy/324/out.log', log_file=<_io.TextIOWrapper name='/home/erschultz/sequences_to_contact_maps/results/ContactGNNEnergy/324/out.log' mode='a' encoding='UTF-8'>, param_file=<_io.TextIOWrapper name='/home/erschultz/sequences_to_contact_maps/results/ContactGNNEnergy/324/params.log' mode='a' encoding='UTF-8'>, split_neg_pos_edges=False, criterion=<function mse_loss at 0x7f95ee653a60>, channels=1, node_feature_size=1, input_m=512, edge_transforms=["'ContactDistance'", 'ContactDistance(norm=False)', 'GeneticDistance(norm=True)'], node_transforms=['Constant(value=1.0)'], edge_dim=2, transforms_processed=None, diag=False, pre_transforms_processed=Compose([
  Constant(value=1.0),
  ContactDistance(norm=False),
  GeneticDistance(norm=True)
]), cuda=True, use_parallel=False, device=device(type='cuda'))

Dataset construction time: 0.14 minutes
Average num edges per graph:  nan
split sizes: train=4302, val=478, test=0, N=4780
Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0.0
)
#### TRAINING/VALIDATION ####
Epoch 2, loss = 18.0527
Mean test/val loss: 17.1084
[25, 50, 75] quantiles test/val loss: [ 8.4633 13.0386 21.6266]

Epoch 4, loss = 15.7236
Mean test/val loss: 13.2483
[25, 50, 75] quantiles test/val loss: [ 7.9962 10.8298 15.2033]

Epoch 6, loss = 12.7958
Mean test/val loss: 10.8760
[25, 50, 75] quantiles test/val loss: [ 5.9625  8.9158 13.1236]

Epoch 8, loss = 11.1784
Mean test/val loss: 11.4931
[25, 50, 75] quantiles test/val loss: [ 6.4166  9.4534 13.2597]

Epoch 10, loss = 10.8903
Mean test/val loss: 9.9186
[25, 50, 75] quantiles test/val loss: [ 5.4461  8.2685 12.0089]

Epoch 12, loss = 10.4335
Mean test/val loss: 9.8722
[25, 50, 75] quantiles test/val loss: [ 5.5172  8.081  11.824 ]

Epoch 14, loss = 10.2960
Mean test/val loss: 9.9307
[25, 50, 75] quantiles test/val loss: [ 5.5296  8.5045 12.3614]

Epoch 16, loss = 10.3495
Mean test/val loss: 10.1349
[25, 50, 75] quantiles test/val loss: [ 5.8927  8.3965 12.031 ]

Epoch 18, loss = 9.9317
Mean test/val loss: 9.7342
[25, 50, 75] quantiles test/val loss: [ 5.6296  8.0836 11.6567]

Epoch 20, loss = 9.8517
Mean test/val loss: 9.2460
[25, 50, 75] quantiles test/val loss: [ 5.1172  7.8462 11.329 ]

Epoch 22, loss = 9.7484
Mean test/val loss: 9.3499
[25, 50, 75] quantiles test/val loss: [ 5.0981  7.9483 11.6322]

Epoch 24, loss = 9.8681
Mean test/val loss: 9.2358
[25, 50, 75] quantiles test/val loss: [ 5.2086  7.9388 11.4297]

Epoch 26, loss = 9.6310
Mean test/val loss: 9.7652
[25, 50, 75] quantiles test/val loss: [ 5.5254  8.2623 11.6527]

Epoch 28, loss = 9.4904
Mean test/val loss: 9.3946
[25, 50, 75] quantiles test/val loss: [ 5.1127  7.6336 11.0655]

Epoch 30, loss = 9.4697
Mean test/val loss: 11.3967
[25, 50, 75] quantiles test/val loss: [ 6.4835  9.3473 13.0697]

Epoch 32, loss = 9.3902
Mean test/val loss: 8.9986
[25, 50, 75] quantiles test/val loss: [ 5.0199  7.5001 10.8666]

Epoch 34, loss = 9.3200
Mean test/val loss: 9.2821
[25, 50, 75] quantiles test/val loss: [ 5.1053  7.6962 11.0714]

Epoch 36, loss = 9.4038
Mean test/val loss: 9.2789
[25, 50, 75] quantiles test/val loss: [ 5.0487  7.8701 11.182 ]

Epoch 38, loss = 9.2484
Mean test/val loss: 8.8839
[25, 50, 75] quantiles test/val loss: [ 4.9955  7.441  10.9197]

Epoch 40, loss = 9.1990
Mean test/val loss: 9.1174
[25, 50, 75] quantiles test/val loss: [ 5.2084  7.6239 10.8433]

Epoch 42, loss = 9.1547
Mean test/val loss: 9.1600
[25, 50, 75] quantiles test/val loss: [ 5.1935  7.6941 11.0615]

Epoch 44, loss = 9.0780
Mean test/val loss: 13.6001
[25, 50, 75] quantiles test/val loss: [ 8.6784 11.9154 15.9084]

Epoch 46, loss = 9.1368
Mean test/val loss: 8.8376
[25, 50, 75] quantiles test/val loss: [ 4.9976  7.5048 10.6818]

Epoch 48, loss = 9.0098
Mean test/val loss: 8.8692
[25, 50, 75] quantiles test/val loss: [ 4.9411  7.6219 10.7914]

Epoch 50, loss = 9.4694
Mean test/val loss: 8.9162
[25, 50, 75] quantiles test/val loss: [ 4.8876  7.3797 10.7437]

Epoch 52, loss = 8.0032
Mean test/val loss: 8.5364
[25, 50, 75] quantiles test/val loss: [ 4.8026  7.3216 10.3671]

Epoch 54, loss = 7.8996
Mean test/val loss: 8.4292
[25, 50, 75] quantiles test/val loss: [ 4.7484  7.2454 10.2358]

Epoch 56, loss = 7.8100
Mean test/val loss: 8.4006
[25, 50, 75] quantiles test/val loss: [ 4.6875  7.1348 10.1862]

Epoch 58, loss = 7.7751
Mean test/val loss: 8.3837
[25, 50, 75] quantiles test/val loss: [ 4.6991  7.23   10.1611]

Epoch 60, loss = 7.7399
Mean test/val loss: 8.3506
[25, 50, 75] quantiles test/val loss: [ 4.6629  7.0916 10.1172]

Epoch 62, loss = 7.7147
Mean test/val loss: 8.3237
[25, 50, 75] quantiles test/val loss: [ 4.6519  7.1168 10.0629]

Epoch 64, loss = 7.7005
Mean test/val loss: 8.4254
[25, 50, 75] quantiles test/val loss: [ 4.838   7.1535 10.1083]

Epoch 66, loss = 7.6723
Mean test/val loss: 8.2898
[25, 50, 75] quantiles test/val loss: [4.6262 7.0521 9.9887]

Epoch 68, loss = 7.6668
Mean test/val loss: 8.1997
[25, 50, 75] quantiles test/val loss: [4.6091 7.0762 9.9926]

Epoch 70, loss = 7.6513
Mean test/val loss: 8.2559
[25, 50, 75] quantiles test/val loss: [ 4.6384  7.0756 10.0034]

Epoch 72, loss = 7.6357
Mean test/val loss: 8.2848
[25, 50, 75] quantiles test/val loss: [4.6254 7.0919 9.9776]

Epoch 74, loss = 7.6284
Mean test/val loss: 8.1200
[25, 50, 75] quantiles test/val loss: [4.5994 7.0715 9.9592]

Epoch 76, loss = 7.6170
Mean test/val loss: 8.1815
[25, 50, 75] quantiles test/val loss: [4.6266 7.0302 9.9432]

Epoch 78, loss = 7.6078
Mean test/val loss: 8.1216
[25, 50, 75] quantiles test/val loss: [4.6188 7.0656 9.9313]

Epoch 80, loss = 7.6006
Mean test/val loss: 8.2067
[25, 50, 75] quantiles test/val loss: [4.5963 7.0143 9.9957]

Epoch 82, loss = 7.5834
Mean test/val loss: 8.1335
[25, 50, 75] quantiles test/val loss: [4.6045 7.0052 9.9707]

Epoch 84, loss = 7.5792
Mean test/val loss: 8.2340
[25, 50, 75] quantiles test/val loss: [4.6222 7.0824 9.9941]

Epoch 86, loss = 7.5759
Mean test/val loss: 8.2085
[25, 50, 75] quantiles test/val loss: [4.679  7.0217 9.9165]

Epoch 88, loss = 7.5623
Mean test/val loss: 8.1189
[25, 50, 75] quantiles test/val loss: [4.5802 6.9763 9.9078]

Epoch 90, loss = 7.5624
Mean test/val loss: 8.1425
[25, 50, 75] quantiles test/val loss: [4.5901 7.0242 9.8758]

Epoch 92, loss = 7.5491
Mean test/val loss: 8.0885
[25, 50, 75] quantiles test/val loss: [4.5853 7.0406 9.8915]

Epoch 94, loss = 7.5420
Mean test/val loss: 8.1809
[25, 50, 75] quantiles test/val loss: [4.5965 6.9999 9.9044]

Epoch 96, loss = 7.5309
Mean test/val loss: 8.1222
[25, 50, 75] quantiles test/val loss: [4.6475 7.0589 9.9635]

Epoch 98, loss = 7.5227
Mean test/val loss: 8.1737
[25, 50, 75] quantiles test/val loss: [4.6166 7.1523 9.9975]

Epoch 100, loss = 7.5184
Mean test/val loss: 8.1078
[25, 50, 75] quantiles test/val loss: [4.5884 7.0167 9.9166]


Total parameters: 43347604
Total training + validation time: 20.0 hours, 2.0 mins, and 0.19999999999708962 secs
Final val loss: 8.107822719197642

split sizes: train=4302, val=478, test=0, N=4780
#### Plotting Script ####
Prediction Results:
dataset_11_21_22 sample939: 9.643645286560059
dataset_11_18_22 sample203: 8.483992576599121
dataset_11_21_22 sample743: 11.274770736694336
