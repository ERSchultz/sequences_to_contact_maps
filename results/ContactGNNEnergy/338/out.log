#### ARCHITECTURE ####
Node Encoder:
 Sequential(
  (0): Linear(1, 64, bias=True)
  (1): PReLU(num_parameters=1)
) 

Edge Encoder:
 None 

Linear:
 Linear(in_features=72, out_features=64, bias=True) 

Model:
 Sequential(
  (0): WeightedGATv2Conv(64, 8, heads=8)
  (1): MLP(
  (model): Sequential(
    (0): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=64, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (1): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (2): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=64, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
  )
)
  (2): WeightedGATv2Conv(64, 8, heads=8)
  (3): MLP(
  (model): Sequential(
    (0): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=64, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (1): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (2): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=64, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
  )
)
  (4): WeightedGATv2Conv(64, 8, heads=8)
  (5): MLP(
  (model): Sequential(
    (0): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=64, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (1): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (2): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=64, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
  )
)
  (6): WeightedGATv2Conv(64, 8, heads=8)
  (7): MLP(
  (model): Sequential(
    (0): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=64, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (1): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (2): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=64, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
  )
)
) 

Head 1:
 Bilinear 

Head 2:
 Sequential(
  (0): MLP(
    (model): Sequential(
      (0): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=32768, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (1): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (2): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (3): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (4): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (5): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (6): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1024, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
    )
  )
  (1): FillDiagonalsFromArray()
) 

Namespace(GNN_mode=True, transforms=[], pre_transforms=['constant', 'ContactDistance', 'GeneticDistance_norm', 'AdjPCs_8'], sparsify_threshold=None, sparsify_threshold_upper=None, top_k=None, use_node_features=False, use_edge_weights=False, use_edge_attr=True, keep_zero_edges=False, data_folder=['/project/depablo/erschultz/dataset_11_18_22', '/project/depablo/erschultz/dataset_11_21_22'], scratch='/scratch/midway2/erschultz', root_name='ContactGNNEnergy3', delete_root=False, toxx=False, toxx_mode='mean', y_preprocessing='sweeprand_log_inf', y_zero_diag_count=0, log_preprocessing=None, output_preprocesing=None, kr=False, mean_filt=None, rescale=2, gated=False, preprocessing_norm='mean', min_subtraction=True, x_reshape=True, ydtype=torch.float32, y_reshape=True, crop=None, classes=10, use_scratch=False, use_scratch_parallel=True, split_percents=[0.9, 0.1, 0.0], split_sizes=None, random_split=True, shuffle=True, batch_size=1, num_workers=4, start_epoch=1, n_epochs=100, save_mod=5, print_mod=2, lr=0.0001, weight_decay=0.0, gpus=1, milestones=[50], gamma=0.1, loss='mse', w_reg=None, reg_lambda=0.1, autoencoder_mode=False, verbose=False, print_params=True, output_mode='energy_sym_diag', model_type='ContactGNNEnergy', id=338, pretrained=False, resume_training=False, k=8, m=1024, seed=42, act='prelu', inner_act='prelu', out_act='prelu', training_norm=None, dropout=0.0, parameter_sharing=False, use_bias=True, use_sign_net=False, use_sign_plus=True, message_passing='weighted_GAT', head_architecture='bilinear_triu', head_architecture_2='fc-fill', head_hidden_sizes_list=[1000, 1000, 1000, 1000, 1000, 1000, 1024], encoder_hidden_sizes_list=[64], inner_hidden_sizes_list=None, edge_encoder_hidden_sizes_list=None, update_hidden_sizes_list=[1000, 1000, 64], head_act='prelu', num_heads=8, concat_heads=True, max_diagonal=None, mlp_model_id=None, kernel_w_list=None, hidden_sizes_list=[8, 8, 8, 8], nf=None, dilation_list=None, dilation_list_trunk=None, bottleneck=None, dilation_list_head=None, down_sampling=None, plot=True, plot_predictions=True, ofile_folder='/home/erschultz/sequences_to_contact_maps/results/ContactGNNEnergy/338', log_file_path='/home/erschultz/sequences_to_contact_maps/results/ContactGNNEnergy/338/out.log', log_file=<_io.TextIOWrapper name='/home/erschultz/sequences_to_contact_maps/results/ContactGNNEnergy/338/out.log' mode='a' encoding='UTF-8'>, param_file=<_io.TextIOWrapper name='/home/erschultz/sequences_to_contact_maps/results/ContactGNNEnergy/338/params.log' mode='a' encoding='UTF-8'>, split_neg_pos_edges=False, criterion=<function mse_loss at 0x7f510e21d280>, channels=1, node_feature_size=1, input_m=512, edge_transforms=["'ContactDistance'", 'ContactDistance(norm=False)', 'GeneticDistance(norm=True)'], node_transforms=['AdjPCs(k=8, normalize=False, sign_net=True)', 'Constant(value=1.0)'], edge_dim=2, transforms_processed=None, diag=True, pre_transforms_processed=Compose([
  Constant(value=1.0),
  ContactDistance(norm=False),
  GeneticDistance(norm=True),
  AdjPCs(k=8, normalize=False, sign_net=True)
]), cuda=True, use_parallel=False, device=device(type='cuda'))

Dataset construction time: 21.878 minutes
Average num edges per graph:  222294.36527196653
Mean degree: [362.66 512.   449.47 ... 329.12 511.93 511.26] +- [70.54  0.   53.89 ... 80.99  0.29  1.69]

split sizes: train=4302, val=478, test=0, N=4780
Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0.0
)
#### TRAINING/VALIDATION ####
Epoch 2, loss = 11.0970
Mean test/val loss: 13.2273
[25, 50, 75] quantiles test/val loss: [ 7.615  11.2178 16.1578]

Epoch 4, loss = 9.7324
Mean test/val loss: 10.2230
[25, 50, 75] quantiles test/val loss: [ 6.0087  8.6596 12.2368]

Epoch 6, loss = 8.9290
Mean test/val loss: 9.8811
[25, 50, 75] quantiles test/val loss: [ 6.5436  8.536  11.2718]

Epoch 8, loss = 8.3962
Mean test/val loss: 8.5639
[25, 50, 75] quantiles test/val loss: [ 5.015   7.4058 10.7462]

Epoch 10, loss = 8.2266
Mean test/val loss: 9.3133
[25, 50, 75] quantiles test/val loss: [ 6.662   8.5705 10.5903]

Epoch 12, loss = 7.9878
Mean test/val loss: 8.1430
[25, 50, 75] quantiles test/val loss: [4.6958 6.858  9.7678]

Epoch 14, loss = 7.6340
Mean test/val loss: 7.8282
[25, 50, 75] quantiles test/val loss: [4.6172 6.6685 9.3644]

Epoch 16, loss = 7.5683
Mean test/val loss: 7.8587
[25, 50, 75] quantiles test/val loss: [4.5057 6.5719 9.5137]

Epoch 18, loss = 7.5214
Mean test/val loss: 7.6531
[25, 50, 75] quantiles test/val loss: [4.4579 6.7156 9.424 ]

Epoch 20, loss = 7.4266
Mean test/val loss: 7.9126
[25, 50, 75] quantiles test/val loss: [4.5708 6.5391 9.2563]

Epoch 22, loss = 7.4087
Mean test/val loss: 8.2599
[25, 50, 75] quantiles test/val loss: [4.7791 6.6845 9.6432]

Epoch 24, loss = 7.4295
Mean test/val loss: 9.0707
[25, 50, 75] quantiles test/val loss: [4.4847 6.6425 9.9077]

Epoch 26, loss = 7.1196
Mean test/val loss: 8.1119
[25, 50, 75] quantiles test/val loss: [5.0285 6.9946 9.8848]

Epoch 28, loss = 6.9268
Mean test/val loss: 7.7606
[25, 50, 75] quantiles test/val loss: [4.1982 6.2148 8.9794]

Epoch 30, loss = 6.7583
Mean test/val loss: 7.2987
[25, 50, 75] quantiles test/val loss: [4.2087 6.1678 8.9187]

Epoch 32, loss = 6.6581
Mean test/val loss: 6.8835
[25, 50, 75] quantiles test/val loss: [3.9153 5.87   8.5897]

Epoch 34, loss = 6.5588
Mean test/val loss: 6.8801
[25, 50, 75] quantiles test/val loss: [3.8272 5.8158 8.4426]

Epoch 36, loss = 217.0952
Mean test/val loss: 7.6634
[25, 50, 75] quantiles test/val loss: [4.583  6.5126 9.3301]

Epoch 38, loss = 6.2422
Mean test/val loss: 6.7506
[25, 50, 75] quantiles test/val loss: [3.7775 5.575  8.1982]

Epoch 40, loss = 6.2981
Mean test/val loss: 7.6095
[25, 50, 75] quantiles test/val loss: [4.7037 6.6669 9.    ]

Epoch 42, loss = 6.4037
Mean test/val loss: 6.6475
[25, 50, 75] quantiles test/val loss: [3.621  5.4624 8.14  ]

Epoch 44, loss = 6.2307
Mean test/val loss: 7.0345
[25, 50, 75] quantiles test/val loss: [3.7327 5.648  8.2063]

Epoch 46, loss = 6.1911
Mean test/val loss: 6.5551
[25, 50, 75] quantiles test/val loss: [3.6434 5.4038 7.9806]

Epoch 48, loss = 6.0358
Mean test/val loss: 6.5037
[25, 50, 75] quantiles test/val loss: [3.5651 5.3156 7.8535]

Epoch 50, loss = 6.0901
Mean test/val loss: 6.7905
[25, 50, 75] quantiles test/val loss: [3.7367 5.6009 8.1777]

Epoch 52, loss = 5.7278
Mean test/val loss: 6.3937
[25, 50, 75] quantiles test/val loss: [3.4667 5.1979 7.7514]

Epoch 54, loss = 5.6115
Mean test/val loss: 6.3030
[25, 50, 75] quantiles test/val loss: [3.3884 5.087  7.651 ]

Epoch 56, loss = 5.5222
Mean test/val loss: 6.2249
[25, 50, 75] quantiles test/val loss: [3.3097 5.0905 7.5563]

Epoch 58, loss = 5.4665
Mean test/val loss: 6.1593
[25, 50, 75] quantiles test/val loss: [3.3643 5.0189 7.4573]

Epoch 60, loss = 5.4134
Mean test/val loss: 6.1154
[25, 50, 75] quantiles test/val loss: [3.2933 4.9603 7.4776]

Epoch 62, loss = 5.3715
Mean test/val loss: 6.0811
[25, 50, 75] quantiles test/val loss: [3.2897 4.9782 7.4266]

Epoch 64, loss = 5.3422
Mean test/val loss: 6.0839
[25, 50, 75] quantiles test/val loss: [3.2657 4.9074 7.3894]

Epoch 66, loss = 5.3121
Mean test/val loss: 6.0477
[25, 50, 75] quantiles test/val loss: [3.2174 4.8772 7.3559]

Epoch 68, loss = 5.2847
Mean test/val loss: 6.0503
[25, 50, 75] quantiles test/val loss: [3.2387 4.934  7.3222]

Epoch 70, loss = 5.2612
Mean test/val loss: 6.0243
[25, 50, 75] quantiles test/val loss: [3.2244 4.8332 7.3071]

Epoch 72, loss = 5.2393
Mean test/val loss: 6.0054
[25, 50, 75] quantiles test/val loss: [3.1816 4.8586 7.2794]

Epoch 74, loss = 5.2165
Mean test/val loss: 5.9887
[25, 50, 75] quantiles test/val loss: [3.2061 4.8215 7.2412]

Epoch 76, loss = 5.1973
Mean test/val loss: 5.9921
[25, 50, 75] quantiles test/val loss: [3.1796 4.8124 7.2385]

Epoch 78, loss = 5.1762
Mean test/val loss: 5.9825
[25, 50, 75] quantiles test/val loss: [3.1516 4.7712 7.1692]

Epoch 80, loss = 5.1586
Mean test/val loss: 5.9625
[25, 50, 75] quantiles test/val loss: [3.1491 4.8026 7.187 ]

Epoch 82, loss = 5.1400
Mean test/val loss: 5.9899
[25, 50, 75] quantiles test/val loss: [3.1841 4.797  7.1673]

Epoch 84, loss = 5.1234
Mean test/val loss: 5.9520
[25, 50, 75] quantiles test/val loss: [3.1665 4.7487 7.1111]

Epoch 86, loss = 5.1080
Mean test/val loss: 5.9471
[25, 50, 75] quantiles test/val loss: [3.1974 4.7787 7.1168]

Epoch 88, loss = 5.0916
Mean test/val loss: 5.9392
[25, 50, 75] quantiles test/val loss: [3.213  4.7493 7.1846]

Epoch 90, loss = 5.0758
Mean test/val loss: 5.9430
[25, 50, 75] quantiles test/val loss: [3.1356 4.7041 7.0229]

Epoch 92, loss = 5.0610
Mean test/val loss: 5.9066
[25, 50, 75] quantiles test/val loss: [3.1554 4.6889 7.0386]

Epoch 94, loss = 5.0469
Mean test/val loss: 5.8953
[25, 50, 75] quantiles test/val loss: [3.1084 4.7204 7.1183]

Epoch 96, loss = 5.0324
Mean test/val loss: 5.9046
[25, 50, 75] quantiles test/val loss: [3.0864 4.6957 7.0338]

Epoch 98, loss = 5.0194
Mean test/val loss: 5.8779
[25, 50, 75] quantiles test/val loss: [3.1181 4.6552 6.9993]

Epoch 100, loss = 5.0046
Mean test/val loss: 5.8936
[25, 50, 75] quantiles test/val loss: [3.0413 4.6516 6.9934]


Total parameters: 43360468
Total training + validation time: 17.0 hours, 13.0 mins, and 32.099999999998545 secs
Final val loss: 5.893648171321992

split sizes: train=4302, val=478, test=0, N=4780
#### Plotting Script ####
Prediction Results:
dataset_11_21_22 sample939: 6.423974990844727
