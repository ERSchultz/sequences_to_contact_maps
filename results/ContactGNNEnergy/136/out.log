Took 708.0 seconds to move data to scratch
#### ARCHITECTURE ####
Sequential(
  (0): Linear(3, 100, bias=True)
  (1): PReLU(num_parameters=1)
  (2): Linear(100, 100, bias=True)
  (3): PReLU(num_parameters=1)
  (4): Linear(100, 16, bias=True)
  (5): PReLU(num_parameters=1)
)
Sequential(
  (0): WeightedSignedConv(16, 8, first_aggr=True,edge_dim=0)
  (1): Linear(16, 100, bias=True)
  (2): PReLU(num_parameters=1)
  (3): Linear(100, 100, bias=True)
  (4): PReLU(num_parameters=1)
  (5): Linear(100, 16, bias=True)
  (6): PReLU(num_parameters=1)
  (7): WeightedSignedConv(8, 8, first_aggr=False,edge_dim=0)
  (8): Linear(16, 100, bias=True)
  (9): PReLU(num_parameters=1)
  (10): Linear(100, 100, bias=True)
  (11): PReLU(num_parameters=1)
  (12): Linear(100, 16, bias=True)
  (13): PReLU(num_parameters=1)
  (14): WeightedSignedConv(8, 8, first_aggr=False,edge_dim=0)
  (15): Linear(16, 100, bias=True)
  (16): PReLU(num_parameters=1)
  (17): Linear(100, 100, bias=True)
  (18): PReLU(num_parameters=1)
  (19): Linear(100, 16, bias=True)
  (20): PReLU(num_parameters=1)
)
None 

Namespace(GNN_mode=True, transforms=[], pre_transforms=['degree'], sparsify_threshold=0.176, sparsify_threshold_upper=None, top_k=None, use_node_features=False, use_edge_weights=False, use_edge_attr=False, relabel_11_to_00=False, split_edges_for_feature_augmentation=True, data_folder='/scratch/midway2/erschultz/dataset_01_17_22', scratch='/scratch/midway2/erschultz', root_name='ContactGNNEnergy8', delete_root=False, toxx=False, toxx_mode='mean', y_preprocessing='diag', y_log_transform=True, y_norm=None, min_subtraction=True, x_reshape=True, ydtype=torch.float32, y_reshape=True, crop=None, classes=10, use_scratch=True, use_scratch_parallel=False, split_percents=None, split_sizes=[1000, 200, 0], random_split=False, shuffle=True, batch_size=2, num_workers=4, start_epoch=1, n_epochs=100, save_mod=5, print_mod=2, lr=0.001, gpus=1, milestones=[50], gamma=0.1, loss='mse', autoencoder_mode=False, verbose=False, print_params=True, output_mode='energy', model_type='ContactGNNEnergy', id=136, pretrained=False, resume_training=False, k=None, m=1024, seed=42, act='prelu', inner_act='prelu', out_act='prelu', training_norm=None, parameter_sharing=False, use_bias=True, message_passing='SignedConv', head_architecture='bilinear', head_hidden_sizes_list=None, encoder_hidden_sizes_list=[100, 100, 16], update_hidden_sizes_list=[100, 100, 16], head_act='prelu', num_heads=1, concat_heads=True, kernel_w_list=None, hidden_sizes_list=[8, 8, 8], nf=None, dilation_list=None, dilation_list_trunk=None, bottleneck=None, dilation_list_head=None, down_sampling=None, plot=True, plot_predictions=True, ofile_folder='/home/erschultz/sequences_to_contact_maps/results/ContactGNNEnergy/136', log_file_path='/home/erschultz/sequences_to_contact_maps/results/ContactGNNEnergy/136/out.log', log_file=<_io.TextIOWrapper name='/home/erschultz/sequences_to_contact_maps/results/ContactGNNEnergy/136/out.log' mode='a' encoding='UTF-8'>, param_file=<_io.TextIOWrapper name='/home/erschultz/sequences_to_contact_maps/results/ContactGNNEnergy/136/params.log' mode='a' encoding='UTF-8'>, split_neg_pos_edges=True, criterion=<function mse_loss at 0x7f5ece0fa670>, channels=1, node_feature_size=3, edge_transforms=[], node_transforms=['Degree'], edge_dim=0, transforms_processed=None, pre_transforms_processed=Compose([
  Degree (norm=True, max=None, weighted=False, split_edges=True, split_val=0)
]), cuda=True, use_parallel=False, device=device(type='cuda'))

Dataset construction time: 14.77 minutes
Mean degree: [790.61 345.01 627.84 ... 184.89 253.38 591.5 ] +- [132.49  95.2  140.36 ...  42.29  79.75 157.08]

#### TRAINING/VALIDATION ####
Epoch 2, loss = 1.6305
Mean test/val loss: 1.4681

Epoch 4, loss = 1.4786
Mean test/val loss: 1.4632

Epoch 6, loss = 1.4215
Mean test/val loss: 1.3272

Epoch 8, loss = 1.3644
Mean test/val loss: 1.6173

Epoch 10, loss = 1.3266
Mean test/val loss: 1.3020

Epoch 12, loss = 1.3141
Mean test/val loss: 1.2765

Epoch 14, loss = 1.2924
Mean test/val loss: 1.2531

Epoch 16, loss = 1.2823
Mean test/val loss: 1.2734

Epoch 18, loss = 1.2736
Mean test/val loss: 1.2959

Epoch 20, loss = 1.2626
Mean test/val loss: 1.2345

Epoch 22, loss = 1.2451
Mean test/val loss: 1.2602

Epoch 24, loss = 1.2763
Mean test/val loss: 1.2189

Epoch 26, loss = 1.2580
Mean test/val loss: 1.2174

Epoch 28, loss = 1.2299
Mean test/val loss: 1.2440

Epoch 30, loss = 1.2263
Mean test/val loss: 1.1990

Epoch 32, loss = 1.2347
Mean test/val loss: 1.1954

Epoch 34, loss = 1.2149
Mean test/val loss: 1.1856

Epoch 36, loss = 1.2064
Mean test/val loss: 1.1922

Epoch 38, loss = 1.2052
Mean test/val loss: 1.3014

Epoch 40, loss = 1.2117
Mean test/val loss: 1.2130

Epoch 42, loss = 1.2008
Mean test/val loss: 1.1909

Epoch 44, loss = 1.1984
Mean test/val loss: 1.2001

Epoch 46, loss = 1.1859
Mean test/val loss: 1.2112

Epoch 48, loss = 1.1831
Mean test/val loss: 1.1749

Epoch 50, loss = 1.1897
Mean test/val loss: 1.1897

Epoch 52, loss = 1.1422
Mean test/val loss: 1.1527

Epoch 54, loss = 1.1402
Mean test/val loss: 1.1482

Epoch 56, loss = 1.1376
Mean test/val loss: 1.1450

Epoch 58, loss = 1.1368
Mean test/val loss: 1.1417

Epoch 60, loss = 1.1346
Mean test/val loss: 1.1423

Epoch 62, loss = 1.1334
Mean test/val loss: 1.1451

Epoch 64, loss = 1.1319
Mean test/val loss: 1.1442

Epoch 66, loss = 1.1310
Mean test/val loss: 1.1417

Epoch 68, loss = 1.1304
Mean test/val loss: 1.1401

Epoch 70, loss = 1.1293
Mean test/val loss: 1.1369

Epoch 72, loss = 1.1282
Mean test/val loss: 1.1397

Epoch 74, loss = 1.1275
Mean test/val loss: 1.1421

Epoch 76, loss = 1.1267
Mean test/val loss: 1.1359

Epoch 78, loss = 1.1260
Mean test/val loss: 1.1359

Epoch 80, loss = 1.1251
Mean test/val loss: 1.1356

Epoch 82, loss = 1.1243
Mean test/val loss: 1.1374

Epoch 84, loss = 1.1238
Mean test/val loss: 1.1369

Epoch 86, loss = 1.1224
Mean test/val loss: 1.1362

Epoch 88, loss = 1.1217
Mean test/val loss: 1.1355

Epoch 90, loss = 1.1211
Mean test/val loss: 1.1356

Epoch 92, loss = 1.1205
Mean test/val loss: 1.1349

Epoch 94, loss = 1.1189
Mean test/val loss: 1.1353

Epoch 96, loss = 1.1192
Mean test/val loss: 1.1361

Epoch 98, loss = 1.1183
Mean test/val loss: 1.1357

Epoch 100, loss = 1.1174
