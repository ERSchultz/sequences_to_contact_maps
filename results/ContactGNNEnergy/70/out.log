Took 858.0 seconds to move data to scratch
#### ARCHITECTURE ####
Sequential(
  (0): Linear(3, 100, bias=True)
  (1): PReLU(num_parameters=1)
  (2): Linear(100, 100, bias=True)
  (3): PReLU(num_parameters=1)
  (4): Linear(100, 16, bias=True)
  (5): PReLU(num_parameters=1)
)
Sequential(
  (0): SignedConv(16, 8, first_aggr=True)
  (1): Linear(16, 100, bias=True)
  (2): PReLU(num_parameters=1)
  (3): Linear(100, 100, bias=True)
  (4): PReLU(num_parameters=1)
  (5): Linear(100, 16, bias=True)
  (6): PReLU(num_parameters=1)
  (7): SignedConv(8, 8, first_aggr=False)
  (8): Linear(16, 100, bias=True)
  (9): PReLU(num_parameters=1)
  (10): Linear(100, 100, bias=True)
  (11): PReLU(num_parameters=1)
  (12): Linear(100, 16, bias=True)
  (13): PReLU(num_parameters=1)
  (14): SignedConv(8, 8, first_aggr=False)
  (15): Linear(16, 100, bias=True)
  (16): PReLU(num_parameters=1)
  (17): Linear(100, 100, bias=True)
  (18): PReLU(num_parameters=1)
  (19): Linear(100, 16, bias=True)
  (20): PReLU(num_parameters=1)
)
Sequential(
  (0): LinearBlock(
    (model): Sequential(
      (0): Linear(in_features=32, out_features=100, bias=True)
      (1): PReLU(num_parameters=1)
    )
  )
  (1): LinearBlock(
    (model): Sequential(
      (0): Linear(in_features=100, out_features=100, bias=True)
      (1): PReLU(num_parameters=1)
    )
  )
  (2): LinearBlock(
    (model): Sequential(
      (0): Linear(in_features=100, out_features=1, bias=True)
      (1): PReLU(num_parameters=1)
    )
  )
) 

Namespace(GNN_mode=True, act='prelu', autoencoder_mode=False, batch_size=1, bottleneck=None, channels=1, classes=10, criterion=<function mse_loss at 0x7f20c911fdc0>, crop=None, cuda=True, data_folder='/scratch/midway2/erschultz/dataset_01_15_22', degree=True, delete_root=False, device=device(type='cuda'), dilation_list=None, dilation_list_head=None, dilation_list_trunk=None, down_sampling=None, encoder_hidden_sizes_list=[100, 100, 16], gamma=0.1, gpus=1, head_act='prelu', head_architecture='concat', head_hidden_sizes_list=[100, 100, 1], hidden_sizes_list=[8, 8, 8], id=70, ifile=None, ifile_folder=None, inner_act='prelu', k=2, kernel_w_list=None, log_file=<_io.TextIOWrapper name='results/ContactGNNEnergy/70/out.log' mode='a' encoding='UTF-8'>, loss='mse', lr=0.0001, m=1024, message_passing='SignedConv', milestones=None, min_subtraction=True, model_type='ContactGNNEnergy', n_epochs=100, nf=None, node_feature_size=3, num_workers=4, ofile_folder='results/ContactGNNEnergy/70', out_act='prelu', output_mode='energy', param_file=<_io.TextIOWrapper name='results/ContactGNNEnergy/70/params.log' mode='a' encoding='UTF-8'>, parameter_sharing=False, plot=True, plot_predictions=True, pre_transforms=['degree'], pre_transforms_processed=None, pretrained=False, print_mod=2, print_params=True, relabel_11_to_00=False, resume_training=False, root_name='ContactGNNEnergy4', save_mod=5, scratch='/scratch/midway2/erschultz', seed=42, shuffle=True, sparsify_threshold=0.176, sparsify_threshold_upper=None, split=[0.8, 0.1, 0.1], split_neg_pos_edges=True, split_neg_pos_edges_for_feature_augmentation=True, start_epoch=1, top_k=None, toxx=False, toxx_mode='mean', training_norm=None, transforms=None, transforms_processed=None, update_hidden_sizes_list=[100, 100, 16], use_bias=True, use_edge_weights=False, use_node_features=False, use_parallel=False, use_scratch=True, verbose=False, weighted_LDP=False, weighted_degree=False, x_reshape=True, y_log_transform=True, y_norm=None, y_preprocessing='diag', y_reshape=True, ydtype=torch.float32)

Dataset construction time: 8.944 minutes
Mean degree: [453.59 400.27 259.98 ... 375.28 280.86 356.31] +- [147.64 114.97  74.33 ... 115.79  78.68 101.27]

#### TRAINING/VALIDATION ####
Epoch 2, loss = 2.0465
Mean test/val loss: 1.9191

Epoch 4, loss = 1.8831
Mean test/val loss: 1.8076

Epoch 6, loss = 1.7535
Mean test/val loss: 1.7409

Epoch 8, loss = 1.6886
Mean test/val loss: 1.6667

Epoch 10, loss = 1.6413
Mean test/val loss: 1.6516

Epoch 12, loss = 1.6041
Mean test/val loss: 1.5865

Epoch 14, loss = 1.5663
Mean test/val loss: 1.5773

Epoch 16, loss = 1.5270
Mean test/val loss: 1.5209

Epoch 18, loss = 1.4927
Mean test/val loss: 1.4566

Epoch 20, loss = 1.4580
Mean test/val loss: 1.4227

Epoch 22, loss = 1.4340
Mean test/val loss: 1.3842

Epoch 24, loss = 1.4052
Mean test/val loss: 1.3699

Epoch 26, loss = 1.3819
Mean test/val loss: 1.3601

Epoch 28, loss = 1.3630
Mean test/val loss: 1.3508

Epoch 30, loss = 1.3480
Mean test/val loss: 1.3173

Epoch 32, loss = 1.3293
Mean test/val loss: 1.2900

Epoch 34, loss = 1.3147
Mean test/val loss: 1.2899

Epoch 36, loss = 1.2984
Mean test/val loss: 1.2916

Epoch 38, loss = 1.2814
Mean test/val loss: 1.2815

Epoch 40, loss = 1.2667
Mean test/val loss: 1.2445

Epoch 42, loss = 1.2545
Mean test/val loss: 1.2719

Epoch 44, loss = 1.2396
Mean test/val loss: 1.2224

Epoch 46, loss = 1.2272
Mean test/val loss: 1.2450

Epoch 48, loss = 1.2139
Mean test/val loss: 1.2267

Epoch 50, loss = 1.2041
Mean test/val loss: 1.2144

Epoch 52, loss = 1.1971
Mean test/val loss: 1.1880

Epoch 54, loss = 1.1864
Mean test/val loss: 1.1881

Epoch 56, loss = 1.1771
Mean test/val loss: 1.1716

Epoch 58, loss = 1.1731
Mean test/val loss: 1.1622

Epoch 60, loss = 1.1647
Mean test/val loss: 1.1357

Epoch 62, loss = 1.1573
Mean test/val loss: 1.1432

Epoch 64, loss = 1.1521
Mean test/val loss: 1.1260

Epoch 66, loss = 1.1461
Mean test/val loss: 1.1359

Epoch 68, loss = 1.1381
Mean test/val loss: 1.1257

Epoch 70, loss = 1.1341
Mean test/val loss: 1.1136

Epoch 72, loss = 1.1270
Mean test/val loss: 1.1290

Epoch 74, loss = 1.1219
Mean test/val loss: 1.1279

Epoch 76, loss = 1.1185
Mean test/val loss: 1.1191

Epoch 78, loss = 1.1121
Mean test/val loss: 1.1367

Epoch 80, loss = 1.1079
Mean test/val loss: 1.0910

Epoch 82, loss = 1.1028
Mean test/val loss: 1.0843

Epoch 84, loss = 1.0983
Mean test/val loss: 1.0812

Epoch 86, loss = 1.0935
Mean test/val loss: 1.0700

Epoch 88, loss = 1.0892
Mean test/val loss: 1.0587

Epoch 90, loss = 1.0871
Mean test/val loss: 1.0687

Epoch 92, loss = 1.0791
Mean test/val loss: 1.0719

Epoch 94, loss = 1.0758
Mean test/val loss: 1.0555

Epoch 96, loss = 1.0715
Mean test/val loss: 1.0573

Epoch 98, loss = 1.0671
Mean test/val loss: 1.0587

Epoch 100, loss = 1.0621
Mean test/val loss: 1.0379


Total parameters: 67,197
Total training + validation time: 12.0 hours
Final val loss: 1.0379340544342994

#### Plotting Script ####
Prediction Results:
results/ContactGNNEnergy/70/sample1230: 1.0286401510238647
results/ContactGNNEnergy/70/sample1761: 0.9860392808914185
results/ContactGNNEnergy/70/sample40: 0.9139540195465088
results/ContactGNNEnergy/70/sample1751: 1.171934962272644
results/ContactGNNEnergy/70/sample1718: 1.9506323337554932
Loss: 1.2102401494979858 +- 0.3796519283789652

<<<<<<< HEAD
Dataset construction time: 0.001 minutes
Mean degree: [555.23] +- [68.75]

Dataset construction time: 0.0 minutes
Dataset construction time: 0.0 minutes
Dataset construction time: 0.0 minutes
Dataset construction time: 0.0 minutes
Dataset construction time: 0.0 minutes
Dataset construction time: 0.0 minutes
Dataset construction time: 0.0 minutes
Dataset construction time: 0.035 minutes
Mean degree: [555.23] +- [68.75]

Dataset construction time: 0.035 minutes
Mean degree: [555.23] +- [68.75]

Dataset construction time: 0.036 minutes
Mean degree: [555.23] +- [68.75]
=======
>>>>>>> 43263875078b2689bd7f38468c69a61a5a9d0ed5

Dataset construction time: 0.032 minutes
Mean degree: [ 506.63  468.63 1021.71  727.75  587.82 1021.75  366.64  471.37 1021.5
  483.18  542.32 1021.73 1021.72  439.98  649.48  792.87  472.84] +- [ 46.21  40.89   0.7   84.24 175.15   0.69 119.79  44.68   1.    60.19
  69.88   0.72   0.69 165.96 154.01  94.48  40.97]

Dataset construction time: 0.033 minutes
Mean degree: [ 506.63  468.63 1021.71  727.75  587.82 1021.75  366.64  471.37 1021.5
  483.18  542.32 1021.73 1021.72  439.98  649.48  792.87  472.84] +- [ 46.21  40.89   0.7   84.24 175.15   0.69 119.79  44.68   1.    60.19
  69.88   0.72   0.69 165.96 154.01  94.48  40.97]

Dataset construction time: 0.035 minutes
Mean degree: [ 506.63  468.63 1021.71  727.75  587.82 1021.75  366.64  471.37 1021.5
  483.18  542.32 1021.73 1021.72  439.98  649.48  792.87  472.84] +- [ 46.21  40.89   0.7   84.24 175.15   0.69 119.79  44.68   1.    60.19
  69.88   0.72   0.69 165.96 154.01  94.48  40.97]

