#### INITIAL PARAMETERS ####
W torch.Size([16, 16])
Parameter containing:
tensor([[-2.9030, -1.3579, -0.1159,  0.6306,  0.1300, -2.0060, -0.5259,  0.0074,
         -0.8698, -0.9468, -0.9541,  0.0940,  0.4819,  0.7846,  0.4334, -1.2424],
        [ 0.8018,  0.7559,  0.7693, -0.6304, -1.1795, -0.0864, -0.8206,  0.0708,
         -0.5680,  0.9971,  0.5890, -0.8121, -0.3789,  0.5135, -1.3799, -1.6639],
        [-0.2866,  0.1065, -1.0833, -0.7696,  0.6752,  0.8069,  1.1710, -0.5095,
         -0.3309,  0.7878,  0.8283, -1.5125,  0.5991, -0.5411,  1.1640, -1.1697],
        [ 0.5824,  1.1285, -1.0953,  0.1999,  0.1104, -0.9235, -0.5145, -0.8941,
          0.2107,  1.5064,  0.2353,  0.1846,  1.1384,  0.2889,  1.1886,  1.0991],
        [-0.9328,  0.8778,  0.2807,  1.1763, -1.1221, -0.6303, -0.1815,  0.1342,
         -1.4652,  0.0737, -0.1560,  2.0684,  0.9626,  0.1764, -0.2683,  0.7926],
        [-0.0382, -0.0583,  0.9805,  1.3132, -0.8019,  0.5312, -0.1664, -0.4602,
          0.1676,  0.5409, -0.1002, -0.2722, -1.5354, -0.5798, -0.9458,  0.4174],
        [-1.8368,  1.2562,  1.1416,  1.6526,  0.7174, -0.1810,  1.0058,  0.9529,
          0.6099, -1.9630, -1.2737,  0.6503,  0.6676, -1.9199,  0.5291,  0.4017],
        [ 1.3191, -0.3491,  0.9271,  0.5373,  0.3893,  0.2476, -0.1176, -0.9187,
         -0.9466,  0.0155,  0.6303, -0.3638,  0.2470, -0.5868, -1.6604,  0.8154],
        [ 0.7568,  0.1241,  1.5317,  0.1860,  0.6051,  0.1545, -0.1196, -1.5353,
         -0.6335,  0.7355,  0.5219, -1.4974,  0.5495, -1.3961,  1.8326,  0.9296],
        [-0.4544, -0.7360, -0.3806, -0.2054,  0.2423, -1.3052,  0.5308, -0.7286,
         -0.0297, -1.3221, -0.2901, -0.8714, -0.4782,  0.8444,  0.1544,  1.2638],
        [-0.7017,  1.5306, -1.7392,  1.0725, -0.5102,  1.0391,  0.7741,  0.2248,
          0.6945, -0.4179, -1.5379, -1.8115,  1.6606,  0.0544, -1.1488, -1.3390],
        [ 0.5765, -0.1867, -0.8161, -0.7172,  1.2449,  1.5637, -0.4089, -1.6876,
         -2.0568,  2.2363,  0.5779,  1.6964, -0.6225, -0.1058, -0.8135,  0.6324],
        [ 0.4187,  2.6675,  0.7713, -0.5579,  0.4857,  0.1564,  2.6497, -0.9922,
         -1.7294, -0.2725,  0.2445, -1.2760, -0.8332, -0.5540, -1.0637, -0.7299],
        [ 0.0141, -0.5861,  0.3658, -0.6748, -1.8440,  0.3974,  0.9730, -0.8759,
         -2.5093,  0.7201,  0.3596,  1.4158, -0.7084, -0.9162,  0.0694,  0.6746],
        [-2.3963,  1.0455,  0.5908, -0.5827,  0.8389, -1.3454,  0.4896,  0.0333,
          0.5407, -1.0664,  0.5261, -0.1867, -0.2688, -1.3501,  0.8523, -1.4681],
        [-0.5407,  0.8778, -0.2026,  0.5888, -0.9038, -0.8866, -0.6129,  0.8308,
          1.3488,  0.1773, -2.5409, -1.1862,  0.3562, -0.2256, -0.3424,  0.8276]],
       device='cuda:0', requires_grad=True) 

act.weight torch.Size([1])
Parameter containing:
tensor([0.2500], device='cuda:0', requires_grad=True) 

inner_act.weight torch.Size([1])
Parameter containing:
tensor([0.2500], device='cuda:0', requires_grad=True) 

out_act.weight torch.Size([1])
Parameter containing:
tensor([0.2500], device='cuda:0', requires_grad=True) 

encoder.module_0.weight torch.Size([100, 3])
Parameter containing:
tensor([[ 0.4414,  0.4792, -0.1353],
        [ 0.5304, -0.1265,  0.1165],
        [-0.2811,  0.3391,  0.5090],
        [-0.4236,  0.5018,  0.1081],
        [ 0.4266,  0.0782,  0.2784],
        [-0.0815,  0.4451,  0.0853],
        [-0.2695,  0.1472, -0.2660],
        [-0.0677, -0.2345,  0.3830],
        [-0.4557, -0.2662, -0.1630],
        [-0.3471,  0.0545, -0.5702],
        [ 0.5214, -0.4904,  0.4457],
        [ 0.0961, -0.1875,  0.3568],
        [ 0.0900,  0.4665,  0.0631],
        [-0.1821,  0.1551, -0.1566],
        [ 0.2430,  0.5155,  0.3337],
        [-0.2524,  0.3333,  0.1033],
        [ 0.2932, -0.3519, -0.5715],
        [-0.2231, -0.4428,  0.4737],
        [ 0.1663,  0.2391,  0.1826],
        [-0.0100,  0.4518, -0.4102],
        [ 0.0364, -0.3941,  0.1780],
        [-0.1988,  0.1769, -0.1203],
        [ 0.4788, -0.3422, -0.3443],
        [-0.3444,  0.5193,  0.1924],
        [ 0.5556, -0.4765, -0.5727],
        [-0.4517, -0.3884,  0.2339],
        [ 0.2067,  0.4797, -0.2982],
        [-0.3936,  0.3063, -0.2334],
        [ 0.3504, -0.1370,  0.3303],
        [-0.4486, -0.2914,  0.1760],
        [ 0.1221, -0.1472,  0.3441],
        [ 0.3925, -0.4187, -0.3082],
        [ 0.5287, -0.1948, -0.2047],
        [-0.5586, -0.3306,  0.1442],
        [-0.0762, -0.4191,  0.0135],
        [-0.3944, -0.4898, -0.3179],
        [-0.5053, -0.3676,  0.5771],
        [ 0.1090,  0.1779, -0.5385],
        [-0.3792, -0.1922,  0.0903],
        [-0.5080, -0.2488, -0.3456],
        [ 0.0016, -0.2148, -0.0400],
        [-0.3912, -0.3963, -0.3368],
        [-0.1976, -0.4557,  0.4841],
        [-0.1146,  0.4968,  0.1799],
        [-0.4889,  0.3995, -0.1589],
        [-0.2213, -0.4792, -0.5740],
        [ 0.1652, -0.1261,  0.2248],
        [-0.4738,  0.4286, -0.4238],
        [-0.0997,  0.1206,  0.2981],
        [ 0.4661,  0.5259, -0.4578],
        [ 0.1453, -0.2483, -0.0633],
        [-0.4321,  0.5259, -0.4237],
        [ 0.3086,  0.2029,  0.1876],
        [-0.3121,  0.5248,  0.1269],
        [ 0.0743, -0.5088,  0.2424],
        [-0.0866, -0.2645,  0.4959],
        [ 0.1287, -0.3194, -0.2922],
        [-0.0276,  0.3224, -0.1475],
        [-0.3294, -0.1977, -0.4313],
        [ 0.2059,  0.4469, -0.5435],
        [ 0.1341,  0.2983,  0.1047],
        [-0.2056,  0.3013,  0.3034],
        [ 0.2159, -0.1015, -0.1529],
        [ 0.0618, -0.1020, -0.1721],
        [ 0.3690,  0.4962, -0.0572],
        [-0.1293,  0.0084, -0.0345],
        [ 0.1388,  0.1618, -0.5244],
        [-0.2131,  0.4862,  0.2249],
        [-0.0287, -0.3481, -0.3532],
        [-0.5172, -0.1882,  0.1950],
        [ 0.3681,  0.2666, -0.5103],
        [-0.3472, -0.0911,  0.5585],
        [ 0.0835, -0.1495,  0.2389],
        [-0.2199, -0.3737,  0.4214],
        [-0.2625, -0.1157, -0.5744],
        [ 0.3864,  0.4374,  0.2104],
        [-0.4026, -0.5698, -0.4689],
        [ 0.4305,  0.2772,  0.4858],
        [ 0.3025,  0.1461, -0.0057],
        [-0.4391, -0.4947, -0.5400],
        [ 0.2363, -0.2835, -0.1162],
        [-0.3323, -0.1052, -0.4064],
        [-0.3772,  0.1915, -0.1716],
        [ 0.3564, -0.1852, -0.4235],
        [-0.1019, -0.2799, -0.1766],
        [-0.5496,  0.3230, -0.4020],
        [ 0.2902,  0.2620,  0.4125],
        [-0.4429,  0.4152, -0.2729],
        [ 0.2142,  0.5422, -0.0814],
        [-0.0045, -0.1329, -0.4821],
        [ 0.2771, -0.5731,  0.3584],
        [ 0.4320,  0.5460, -0.1362],
        [-0.4744,  0.1298,  0.3189],
        [-0.5746, -0.1310, -0.3461],
        [-0.0505, -0.2842, -0.2360],
        [-0.1833, -0.5487,  0.4737],
        [ 0.4840, -0.0906, -0.0657],
        [-0.2356, -0.5214, -0.5618],
        [ 0.2146, -0.3170, -0.3712],
        [-0.0450, -0.1923, -0.1868]], device='cuda:0', requires_grad=True) 

encoder.module_0.bias torch.Size([100])
Parameter containing:
tensor([ 0.0186, -0.1225, -0.1988, -0.2764, -0.4699,  0.4841, -0.2310,  0.1530,
        -0.2003,  0.0469,  0.5383,  0.2660, -0.5003,  0.2292,  0.5480,  0.1519,
         0.3871,  0.5692, -0.0885,  0.1198, -0.4013, -0.1190,  0.4276,  0.2960,
        -0.3653, -0.4630, -0.3945, -0.5698, -0.4455, -0.1428,  0.3896,  0.0966,
        -0.4391, -0.4632,  0.2872, -0.4295, -0.0711,  0.2770, -0.2672, -0.0630,
        -0.0503, -0.1366, -0.2927, -0.5147, -0.4667, -0.3091,  0.5576, -0.2789,
        -0.3877,  0.1399,  0.1591,  0.3163,  0.4389,  0.3215, -0.5724,  0.0512,
         0.3497, -0.0534, -0.3402,  0.5504, -0.2159, -0.3287, -0.5205,  0.0258,
         0.2558,  0.1278,  0.1142, -0.4379, -0.5392,  0.0102,  0.5264,  0.3331,
        -0.3362, -0.0749, -0.4256, -0.2785,  0.1046,  0.3144,  0.4783, -0.5301,
         0.3860, -0.4072,  0.2162,  0.4886,  0.0081,  0.5253, -0.4919, -0.2205,
         0.3367, -0.1258, -0.1182, -0.2406,  0.3980,  0.2832,  0.1850, -0.3244,
        -0.4687,  0.0624,  0.1711, -0.2666], device='cuda:0',
       requires_grad=True) 

encoder.module_2.weight torch.Size([100, 100])
Parameter containing:
tensor([[-0.0280,  0.0675,  0.0080,  ...,  0.0930, -0.0259, -0.0423],
        [-0.0242, -0.0483,  0.0170,  ..., -0.0880,  0.0698,  0.0116],
        [-0.0539,  0.0523, -0.0946,  ...,  0.0836, -0.0560,  0.0919],
        ...,
        [ 0.0149, -0.0668, -0.0751,  ...,  0.0350, -0.0560,  0.0757],
        [ 0.0928, -0.0271,  0.0738,  ...,  0.0034,  0.0219,  0.0902],
        [-0.0438, -0.0375,  0.0319,  ..., -0.0964, -0.0224, -0.0814]],
       device='cuda:0', requires_grad=True) 

encoder.module_2.bias torch.Size([100])
Parameter containing:
tensor([ 0.0899, -0.0268,  0.0269, -0.0717,  0.0102, -0.0940,  0.0450,  0.0472,
        -0.0972,  0.0436,  0.0127, -0.0625, -0.0458,  0.0983, -0.0520,  0.0875,
         0.0201,  0.0458, -0.0503,  0.0325,  0.0215,  0.0798, -0.0989, -0.0364,
         0.0025, -0.0718,  0.0170,  0.0645,  0.0603, -0.0516,  0.0404, -0.0728,
         0.0622, -0.0817, -0.0907,  0.0347, -0.0094,  0.0246,  0.0930,  0.0958,
         0.0564, -0.0907, -0.0154,  0.0109, -0.0974,  0.0836,  0.0656, -0.0462,
        -0.0209, -0.0580,  0.0385,  0.0172, -0.0347,  0.0535, -0.0216,  0.0107,
         0.0692, -0.0470, -0.0922,  0.0540, -0.0654,  0.0378, -0.0301,  0.0671,
        -0.0038, -0.0455, -0.0705,  0.0672, -0.0626,  0.0283,  0.0765, -0.0803,
        -0.0839,  0.0102, -0.0393, -0.0734,  0.0661, -0.0930, -0.0456, -0.0831,
         0.0264,  0.0086,  0.0417,  0.0419, -0.0004, -0.0300,  0.0884,  0.0042,
         0.0238, -0.0823, -0.0980, -0.0488, -0.0524,  0.0290, -0.0444,  0.0104,
         0.0927, -0.0295,  0.0255,  0.0012], device='cuda:0',
       requires_grad=True) 

encoder.module_4.weight torch.Size([16, 100])
Parameter containing:
tensor([[ 0.0658,  0.0437, -0.0046,  ..., -0.0985,  0.0879, -0.0389],
        [ 0.0153,  0.0711,  0.0383,  ..., -0.0571, -0.0231,  0.0334],
        [ 0.0378,  0.0129,  0.0656,  ...,  0.0582,  0.0935,  0.0702],
        ...,
        [ 0.0351, -0.0812,  0.0986,  ...,  0.0367, -0.0375, -0.0136],
        [ 0.0063,  0.0894,  0.0749,  ...,  0.0603, -0.0517,  0.0448],
        [ 0.0908, -0.0299, -0.0738,  ..., -0.0864,  0.0315, -0.0033]],
       device='cuda:0', requires_grad=True) 

encoder.module_4.bias torch.Size([16])
Parameter containing:
tensor([-0.0079,  0.0081, -0.0457,  0.0668, -0.0727, -0.0015,  0.0826,  0.0211,
        -0.0720,  0.0993, -0.0593, -0.0425,  0.0614, -0.0601,  0.0040,  0.0369],
       device='cuda:0', requires_grad=True) 

model.module_0.att torch.Size([1, 8, 8])
Parameter containing:
tensor([[[ 0.3807, -0.1864, -0.2529,  0.4075,  0.4911,  0.3227,  0.5292,
          -0.5522],
         [-0.5461,  0.1017, -0.5138, -0.1709,  0.3728,  0.5950, -0.1231,
          -0.5401],
         [-0.3022,  0.5951, -0.1134, -0.5695, -0.3242, -0.3820, -0.1598,
          -0.4434],
         [-0.2073,  0.5614, -0.2900, -0.1316,  0.3438, -0.5722, -0.3216,
          -0.0520],
         [ 0.4815,  0.0852, -0.0030, -0.2032,  0.2836, -0.2632, -0.0958,
          -0.2639],
         [ 0.2289,  0.6108, -0.4620, -0.0431, -0.0434, -0.3066, -0.4366,
          -0.2612],
         [-0.4786,  0.4650, -0.4341,  0.3736,  0.5064, -0.1125,  0.5698,
           0.4425],
         [-0.4270,  0.3496,  0.1574,  0.4818, -0.4645,  0.4766,  0.0869,
           0.2994]]], device='cuda:0', requires_grad=True) 

model.module_0.bias torch.Size([64])
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True) 

model.module_0.lin_l.weight torch.Size([64, 16])
Parameter containing:
tensor([[ 0.2609, -0.1931, -0.2305,  ...,  0.1726,  0.2172, -0.2519],
        [-0.1593,  0.0470,  0.0694,  ...,  0.2443, -0.1650, -0.0972],
        [-0.2300,  0.2131, -0.2607,  ...,  0.0754,  0.2146,  0.0932],
        ...,
        [ 0.1873,  0.2695, -0.1507,  ..., -0.0364, -0.1815,  0.1879],
        [-0.1619, -0.2310, -0.1372,  ..., -0.0149,  0.0015, -0.0409],
        [-0.0051, -0.2007,  0.1358,  ..., -0.1621, -0.0752,  0.0819]],
       device='cuda:0', requires_grad=True) 

model.module_0.lin_l.bias torch.Size([64])
Parameter containing:
tensor([-1.9503e-01, -2.2735e-01, -3.8395e-02,  4.2000e-02,  1.6028e-01,
         2.0225e-01, -2.0255e-01,  3.6468e-03, -1.8165e-01,  2.3623e-02,
        -2.0413e-01, -2.2545e-01,  1.1706e-02,  1.6636e-01, -2.3938e-01,
        -2.2492e-01,  2.8662e-02,  2.4158e-01, -7.1447e-02,  2.3023e-01,
         2.3385e-01, -1.0906e-01,  6.8762e-02,  1.5449e-01, -2.1914e-01,
         1.0020e-01,  1.0355e-01, -6.7002e-04, -2.4693e-01, -3.8965e-02,
        -1.9005e-01,  6.2287e-06,  6.6182e-03, -1.3655e-01,  2.1827e-01,
        -1.6347e-01,  1.6237e-01,  1.5953e-01,  2.3038e-01,  7.3280e-02,
         1.6611e-01, -8.6891e-02,  8.9629e-02,  2.2673e-01, -2.4356e-01,
        -1.7686e-01,  7.4080e-02, -5.7046e-02, -4.2415e-02, -1.4279e-01,
         2.2051e-04, -7.8424e-02,  2.3815e-01,  1.5309e-01,  6.5440e-02,
         1.0256e-01, -1.0525e-01, -6.7248e-02,  1.4803e-01,  2.1481e-01,
        -5.1377e-02, -2.1918e-01, -6.3038e-03,  1.4872e-01], device='cuda:0',
       requires_grad=True) 

model.module_0.lin_r.weight torch.Size([64, 16])
Parameter containing:
tensor([[ 0.0128, -0.2640,  0.0345,  ..., -0.1278,  0.0114,  0.1511],
        [-0.2417,  0.1371,  0.2619,  ...,  0.1213,  0.2468, -0.2216],
        [ 0.2029,  0.0934, -0.2298,  ...,  0.0242,  0.0010,  0.2506],
        ...,
        [-0.0968, -0.0335, -0.1311,  ...,  0.1054, -0.1574, -0.2593],
        [ 0.1068, -0.0511,  0.0811,  ...,  0.1557, -0.2386, -0.2366],
        [ 0.1183,  0.1582, -0.2349,  ...,  0.1247, -0.1119, -0.1041]],
       device='cuda:0', requires_grad=True) 

model.module_0.lin_r.bias torch.Size([64])
Parameter containing:
tensor([ 0.1795, -0.0522,  0.0383,  0.2499, -0.2485, -0.0571, -0.0988, -0.0405,
         0.0103, -0.0542,  0.2427,  0.1014, -0.1571,  0.2422, -0.0694,  0.0360,
         0.0233, -0.1838,  0.1272, -0.1519, -0.2227,  0.2099,  0.1497, -0.1046,
        -0.1789,  0.2258,  0.0884, -0.1659, -0.2077, -0.1034, -0.1389,  0.0177,
         0.0239, -0.2446,  0.1254,  0.1274, -0.2265,  0.2010, -0.0996,  0.1287,
         0.2452,  0.1481,  0.1679,  0.0088,  0.0366, -0.2127, -0.1814,  0.2014,
        -0.2285,  0.1077, -0.1473,  0.2167,  0.1845, -0.0565, -0.0320, -0.1734,
         0.2332, -0.1507,  0.0892,  0.1752, -0.2407, -0.2459, -0.1755, -0.0375],
       device='cuda:0', requires_grad=True) 

Can't access the shape of an uninitialized parameter or buffer. This error usually happens in `load_state_dict` when trying to load an uninitialized parameter into an initialized one. Call `forward` to initialize the parameters before accessing their attributes.
model.module_0.lin_edge.weight
<UninitializedParameter>
model.module_1.weight torch.Size([100, 8])
Parameter containing:
tensor([[-0.1963,  0.2328, -0.0254,  0.2915,  0.0302,  0.2195, -0.2234, -0.3133],
        [ 0.1228,  0.1873,  0.0979,  0.1111,  0.2329, -0.1819,  0.1892,  0.1695],
        [-0.1501, -0.2103, -0.3511,  0.2600, -0.0055,  0.0808, -0.2502, -0.2658],
        [ 0.2901,  0.2491,  0.1912,  0.2827,  0.2847,  0.2088,  0.2554, -0.0454],
        [ 0.0921, -0.3039, -0.1267, -0.0330, -0.0172,  0.3487,  0.2885,  0.1113],
        [ 0.3297, -0.1802,  0.0687, -0.2805, -0.3007,  0.1890,  0.1135, -0.0190],
        [-0.2288,  0.2549, -0.0271,  0.2414, -0.2443,  0.2717, -0.0167, -0.1105],
        [-0.0292, -0.2727, -0.0646,  0.0207, -0.2618,  0.1909,  0.3340,  0.1823],
        [-0.1737, -0.2197, -0.2802, -0.2180,  0.2110,  0.1629, -0.1322,  0.0058],
        [-0.0557,  0.0051, -0.3516,  0.2988, -0.1241, -0.3046, -0.0064,  0.1057],
        [ 0.1619, -0.1786, -0.0258, -0.2648, -0.2665, -0.2334, -0.2221,  0.2220],
        [-0.3189, -0.0734, -0.1466,  0.2325, -0.0228, -0.1946,  0.2871, -0.2681],
        [-0.1339, -0.3226, -0.0282,  0.1006,  0.3089, -0.2563, -0.2360,  0.3533],
        [ 0.0622, -0.2396,  0.1524,  0.3406,  0.0130, -0.1621, -0.1890, -0.1272],
        [ 0.1802, -0.2502,  0.3216, -0.2188,  0.0991,  0.3517, -0.1563, -0.2364],
        [-0.1833, -0.1483, -0.2171, -0.3484, -0.1817, -0.1229,  0.3063, -0.0603],
        [ 0.0412, -0.1109,  0.0551,  0.1684,  0.0618, -0.0435,  0.0298,  0.3336],
        [-0.2355,  0.3469, -0.0785,  0.2521,  0.2738,  0.0123,  0.0080,  0.2451],
        [-0.0491,  0.0973,  0.3383, -0.0700, -0.1245,  0.2742,  0.3176,  0.3390],
        [-0.3336,  0.2328,  0.1740, -0.2256,  0.2309, -0.1899,  0.1870,  0.0302],
        [ 0.1115, -0.2174, -0.0684, -0.1922,  0.1923, -0.2809,  0.0248,  0.1366],
        [-0.0082, -0.1064,  0.0806,  0.0279, -0.1949, -0.0960, -0.0773, -0.1993],
        [ 0.0088, -0.1098, -0.1999, -0.2864, -0.2305, -0.0770,  0.2828, -0.0416],
        [-0.1898, -0.2696,  0.0603,  0.3006, -0.3186,  0.2726,  0.3158,  0.0800],
        [ 0.1764,  0.1050, -0.3091, -0.1410, -0.0828, -0.2108, -0.0111,  0.0746],
        [ 0.0121,  0.1929, -0.2084,  0.2974, -0.1438,  0.2835,  0.3207, -0.2888],
        [-0.3388,  0.2624,  0.2691, -0.0379,  0.0733,  0.0521, -0.1080,  0.1860],
        [ 0.1193, -0.3411, -0.0975, -0.2100,  0.1518, -0.3530, -0.3038, -0.3117],
        [-0.1214, -0.2727, -0.0705, -0.2012,  0.2273,  0.3102,  0.1368, -0.1372],
        [ 0.1443, -0.3036, -0.1768, -0.0741, -0.3306,  0.3315,  0.2317,  0.1638],
        [ 0.1662,  0.3479, -0.1344,  0.0848, -0.1189, -0.0600, -0.2898,  0.2114],
        [-0.2547, -0.1580,  0.3152, -0.2075,  0.0454, -0.3245, -0.2053,  0.3208],
        [ 0.1901,  0.0150, -0.1763,  0.2696,  0.3328,  0.0709, -0.2279,  0.0122],
        [ 0.1722,  0.1032,  0.3181,  0.3184,  0.1791, -0.2459, -0.0191,  0.0685],
        [-0.1179,  0.1058,  0.2308, -0.1910, -0.1078, -0.2689,  0.0549,  0.2116],
        [ 0.3198,  0.0185,  0.2302, -0.3414, -0.0077, -0.0667, -0.1628,  0.3273],
        [ 0.2208, -0.2946, -0.3211, -0.1656,  0.1200,  0.1677,  0.1830,  0.2733],
        [-0.2263,  0.0298, -0.1576, -0.1715,  0.0034, -0.0311, -0.2865, -0.0051],
        [ 0.1700,  0.3171, -0.0955,  0.2391, -0.0791, -0.2601,  0.3058, -0.0836],
        [-0.0821, -0.0598,  0.3187, -0.1510, -0.0939,  0.0206, -0.2315,  0.1379],
        [ 0.2977,  0.1278, -0.0850, -0.0676, -0.3465, -0.2787, -0.0969,  0.1960],
        [ 0.3289, -0.0248, -0.0339,  0.2419, -0.0741, -0.2412,  0.2002, -0.1529],
        [-0.2518,  0.0030, -0.2352, -0.2757, -0.1949, -0.0307,  0.1177, -0.1465],
        [-0.3381, -0.1203,  0.3289, -0.0816, -0.2581, -0.1323,  0.3184,  0.0564],
        [-0.0496,  0.3459,  0.0365,  0.0538,  0.3484, -0.0363, -0.0623,  0.2829],
        [ 0.1121, -0.2480,  0.2263, -0.1482, -0.3013,  0.3125, -0.2612, -0.1505],
        [-0.0564, -0.0021, -0.0404,  0.2631,  0.2645, -0.3146, -0.3280, -0.3460],
        [ 0.2942, -0.0429, -0.1750, -0.2332,  0.0462,  0.1012,  0.2029,  0.2854],
        [ 0.3143, -0.2357, -0.0187, -0.2248, -0.1276,  0.0102,  0.1983, -0.1587],
        [-0.1993, -0.0937,  0.1462,  0.0098,  0.2649, -0.3182,  0.1285,  0.2678],
        [ 0.2227, -0.3077, -0.2360,  0.0052,  0.0017,  0.2106,  0.2884, -0.0233],
        [-0.1039,  0.1928, -0.2292, -0.1613,  0.0435, -0.0595,  0.2137, -0.0701],
        [ 0.0376,  0.1004, -0.0302, -0.1137,  0.2201,  0.1970, -0.0540, -0.2264],
        [-0.1159,  0.2446,  0.2069, -0.1153,  0.1933, -0.0417, -0.0207, -0.1663],
        [ 0.0103, -0.0739,  0.0893, -0.3363, -0.0211, -0.2765, -0.3285, -0.3505],
        [-0.0568,  0.0661,  0.0369,  0.0832, -0.1519, -0.2342, -0.0765, -0.2654],
        [-0.0441,  0.0742,  0.0493, -0.3211,  0.3357,  0.3046, -0.1584, -0.3011],
        [ 0.0516,  0.0798, -0.0945, -0.1624,  0.0267, -0.2152, -0.1853,  0.1131],
        [ 0.0400,  0.1427,  0.2990, -0.0034,  0.1203,  0.2524, -0.2213,  0.2355],
        [ 0.2423, -0.1350,  0.2763, -0.1621, -0.1687,  0.2535,  0.1640, -0.0255],
        [-0.2793,  0.2017,  0.0868, -0.2111, -0.0131,  0.2554,  0.1493, -0.1626],
        [-0.0038, -0.1250,  0.2334,  0.1659, -0.0524, -0.0414, -0.2575, -0.0782],
        [ 0.1733, -0.3391,  0.0451,  0.3364,  0.1522,  0.1756, -0.0957, -0.0298],
        [-0.3019, -0.1711,  0.0888,  0.1207,  0.2085, -0.1992,  0.1015, -0.1952],
        [-0.2607, -0.1356, -0.2405,  0.0670,  0.0557,  0.2269,  0.1846,  0.0689],
        [ 0.1364,  0.1083, -0.3343, -0.1634, -0.2886, -0.2050,  0.0708,  0.3334],
        [ 0.2448, -0.1439,  0.0822,  0.2433,  0.0737, -0.2891,  0.0433,  0.1440],
        [ 0.0579,  0.3229,  0.2950, -0.1976,  0.2688, -0.3164, -0.2007,  0.1434],
        [ 0.0107,  0.3053,  0.2104, -0.0753,  0.3381, -0.2537,  0.0659,  0.1128],
        [ 0.2764,  0.3459,  0.1724, -0.1576,  0.2843,  0.2434, -0.0070,  0.2538],
        [ 0.0563, -0.0307,  0.1092,  0.0010, -0.0996,  0.0513, -0.0024, -0.2429],
        [-0.2591,  0.1162, -0.1377,  0.0086,  0.3122,  0.2608,  0.1964,  0.1883],
        [ 0.2394, -0.2371,  0.3290, -0.2607, -0.2291, -0.1448,  0.3456,  0.1979],
        [ 0.0991,  0.2050, -0.3377, -0.3372,  0.1102, -0.0967,  0.1692, -0.2509],
        [ 0.3347, -0.3084,  0.1802,  0.1863, -0.1896,  0.3124,  0.1842, -0.1614],
        [-0.1599,  0.2706, -0.0871,  0.0232,  0.1621,  0.1650, -0.1770,  0.2661],
        [ 0.1072,  0.1138, -0.1643,  0.1594, -0.0439,  0.2507,  0.1813,  0.2558],
        [-0.0480,  0.2298,  0.2720,  0.0784, -0.0223, -0.2310, -0.1825,  0.2036],
        [-0.2167, -0.1739,  0.1803, -0.1797, -0.2735,  0.0786,  0.1526, -0.3205],
        [ 0.0452, -0.2726,  0.1492,  0.1934,  0.1190,  0.3453, -0.0686, -0.1406],
        [ 0.0957,  0.1061, -0.1580, -0.2327, -0.1076,  0.1467,  0.1494, -0.1566],
        [ 0.2990, -0.2628, -0.0410,  0.0863, -0.2547,  0.3218, -0.1738,  0.2094],
        [ 0.0142, -0.1985,  0.3305, -0.0393,  0.0629,  0.3325, -0.3329,  0.2845],
        [-0.1745, -0.2870,  0.1966, -0.1082, -0.2822, -0.1733,  0.2285,  0.1549],
        [-0.0174,  0.1816,  0.0702, -0.2647,  0.0679,  0.2518, -0.0331, -0.0404],
        [ 0.3472, -0.1011,  0.3296,  0.3096,  0.2858,  0.0939,  0.0970,  0.0910],
        [-0.3241, -0.1399, -0.1980, -0.0521, -0.1768, -0.1139, -0.2407, -0.2571],
        [ 0.1143,  0.3451, -0.1434, -0.2336, -0.1189, -0.1335, -0.3443,  0.3105],
        [ 0.2401,  0.2622, -0.1827, -0.2338,  0.3389, -0.2139, -0.2971, -0.2945],
        [ 0.1076, -0.0472, -0.2157,  0.3011, -0.1960,  0.0230,  0.1762,  0.1692],
        [ 0.2959, -0.0438,  0.1822, -0.0281,  0.0467, -0.2076,  0.2188,  0.0805],
        [ 0.0709, -0.0271,  0.1431, -0.3415,  0.2889,  0.2774,  0.1894, -0.0129],
        [-0.3299, -0.0958, -0.2840, -0.1694,  0.2781, -0.0408,  0.3522, -0.1782],
        [-0.1647, -0.2834,  0.1347,  0.1309,  0.1785,  0.3058,  0.2126, -0.3242],
        [ 0.0101,  0.3084,  0.2467,  0.1368, -0.1660, -0.2983,  0.3408, -0.1158],
        [ 0.1209, -0.0956, -0.1194,  0.1259,  0.3271, -0.0840,  0.1532,  0.1423],
        [-0.0565,  0.0703,  0.1175,  0.3156, -0.1335,  0.2719, -0.0228,  0.1395],
        [-0.2867,  0.2376,  0.2980,  0.0824,  0.0109,  0.0923, -0.2792, -0.3048],
        [ 0.2633, -0.3175, -0.0792, -0.2165, -0.3300, -0.1500, -0.0029, -0.2837],
        [-0.2710, -0.2852, -0.0128, -0.0942,  0.2598, -0.3385, -0.3088, -0.1078]],
       device='cuda:0', requires_grad=True) 

model.module_1.bias torch.Size([100])
Parameter containing:
tensor([ 0.3535,  0.3220, -0.0457, -0.0964,  0.0489,  0.0149,  0.0332, -0.1528,
        -0.0172,  0.0680,  0.2113, -0.2230, -0.0719,  0.1775,  0.2327,  0.1915,
        -0.0858,  0.0806, -0.1210,  0.3003,  0.2008, -0.3190,  0.1717,  0.3471,
         0.0945, -0.2804,  0.2274,  0.0189, -0.0949,  0.1543,  0.2745,  0.1604,
        -0.2788,  0.1476, -0.2182, -0.3302, -0.1754,  0.1860,  0.2718,  0.1044,
         0.2173, -0.3373, -0.1722, -0.2017, -0.3004, -0.2208, -0.2524, -0.2954,
         0.0516, -0.1648,  0.0146, -0.0566,  0.0243, -0.3211, -0.2900, -0.3197,
         0.2460,  0.0117, -0.3009, -0.1361, -0.0036, -0.2340,  0.2860,  0.2537,
         0.0850, -0.0609,  0.2056,  0.0926,  0.0301,  0.0436, -0.3077,  0.1001,
         0.1291,  0.2181,  0.1677,  0.3009, -0.1879,  0.0290, -0.2604,  0.3035,
        -0.0165, -0.3055,  0.2292,  0.0417, -0.1019,  0.3163, -0.1862, -0.1754,
        -0.1214,  0.0405, -0.2747,  0.0765, -0.3421, -0.1906, -0.2068, -0.2569,
         0.0810,  0.1553, -0.0864, -0.1062], device='cuda:0',
       requires_grad=True) 

model.module_3.weight torch.Size([100, 100])
Parameter containing:
tensor([[ 0.0021, -0.0811, -0.0949,  ..., -0.0725,  0.0704,  0.0900],
        [ 0.0484,  0.0464,  0.0117,  ..., -0.0322,  0.0169, -0.0740],
        [-0.0875, -0.0173, -0.0092,  ..., -0.0512,  0.0958, -0.0666],
        ...,
        [-0.0894,  0.0921,  0.0412,  ...,  0.0142, -0.0679,  0.0197],
        [-0.0752,  0.0008,  0.0438,  ...,  0.0488, -0.0308,  0.0717],
        [ 0.0223, -0.0737,  0.0873,  ..., -0.0218, -0.0701, -0.0689]],
       device='cuda:0', requires_grad=True) 

model.module_3.bias torch.Size([100])
Parameter containing:
tensor([ 0.0128,  0.0304, -0.0027,  0.0805, -0.0071, -0.0700,  0.0351, -0.0668,
         0.0844,  0.0144,  0.0032,  0.0354,  0.0492, -0.0744, -0.0399, -0.0333,
        -0.0020,  0.0463, -0.0473,  0.0334,  0.0118, -0.0789,  0.0522,  0.0025,
        -0.0650,  0.0220, -0.0465,  0.0027, -0.0805,  0.0798, -0.0116,  0.0937,
         0.0742, -0.0895, -0.0994,  0.0621, -0.0307,  0.0605,  0.0213,  0.0414,
         0.0677,  0.0397,  0.0870, -0.0102,  0.0785, -0.0830, -0.0961, -0.0715,
         0.0391, -0.0956,  0.0405,  0.0356, -0.0707, -0.0811, -0.0252, -0.0550,
        -0.0395, -0.0154,  0.0252, -0.0551, -0.0679, -0.0866,  0.0354, -0.0411,
         0.0798, -0.0071,  0.0014, -0.0223, -0.0664,  0.0567,  0.0774, -0.0689,
         0.0436, -0.0088, -0.0913, -0.0078, -0.0310, -0.0320, -0.0725, -0.0122,
         0.0989,  0.0113, -0.0983,  0.0052, -0.0819,  0.0411,  0.0889,  0.0467,
        -0.0007, -0.0454, -0.0379, -0.0898, -0.0411, -0.0499, -0.0406, -0.0628,
        -0.0217, -0.0543, -0.0063, -0.0234], device='cuda:0',
       requires_grad=True) 

model.module_5.weight torch.Size([16, 100])
Parameter containing:
tensor([[-0.0667, -0.0962,  0.0332,  ...,  0.0733, -0.0823, -0.0458],
        [-0.0393, -0.0396, -0.0971,  ...,  0.0624, -0.0102, -0.0783],
        [-0.0040,  0.0307,  0.0643,  ..., -0.0358, -0.0758, -0.0695],
        ...,
        [ 0.0863, -0.0604,  0.0888,  ...,  0.0976, -0.0689, -0.0836],
        [ 0.0729, -0.0521,  0.0849,  ...,  0.0770,  0.0095,  0.0062],
        [ 0.0395, -0.0174,  0.0340,  ...,  0.0762, -0.0666, -0.0944]],
       device='cuda:0', requires_grad=True) 

model.module_5.bias torch.Size([16])
Parameter containing:
tensor([-0.0663, -0.0940,  0.0314, -0.0325, -0.0087, -0.0434, -0.0306, -0.0974,
        -0.0871,  0.0982, -0.0997, -0.0501, -0.0436, -0.0922, -0.0594, -0.0100],
       device='cuda:0', requires_grad=True) 

model.module_7.att torch.Size([1, 8, 8])
Parameter containing:
tensor([[[ 0.4634,  0.0852,  0.2109, -0.0922, -0.0387,  0.2463, -0.3935,
          -0.2763],
         [ 0.0829, -0.1475, -0.2862, -0.4209, -0.1527,  0.5915, -0.3211,
           0.1952],
         [ 0.5284,  0.4613,  0.0025, -0.1559,  0.5342,  0.1739, -0.5354,
          -0.3123],
         [-0.1937, -0.0897, -0.2985,  0.1239,  0.2612, -0.4703, -0.4213,
           0.2745],
         [-0.6031,  0.2076,  0.1413, -0.5636, -0.0221,  0.4695, -0.0351,
          -0.1065],
         [-0.4534,  0.0010, -0.6040,  0.4501, -0.5196, -0.1404,  0.2451,
          -0.0504],
         [ 0.1229,  0.0728, -0.1056, -0.1996,  0.5122,  0.1316,  0.0268,
           0.1904],
         [ 0.2084,  0.2485, -0.0135, -0.0250, -0.0737, -0.4491, -0.0468,
          -0.4681]]], device='cuda:0', requires_grad=True) 

model.module_7.bias torch.Size([64])
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True) 

model.module_7.lin_l.weight torch.Size([64, 16])
Parameter containing:
tensor([[-0.0477,  0.0717,  0.2436,  ..., -0.0925, -0.1753,  0.0175],
        [ 0.1573, -0.0959, -0.0725,  ..., -0.0770, -0.1749, -0.2479],
        [-0.1683, -0.1131, -0.2245,  ...,  0.1830,  0.2619,  0.2329],
        ...,
        [ 0.1714, -0.2274,  0.1701,  ..., -0.0520, -0.1646,  0.1446],
        [ 0.1058, -0.1957,  0.0826,  ..., -0.1337,  0.0356,  0.2649],
        [ 0.2276,  0.1130,  0.2026,  ...,  0.1146, -0.1264,  0.2053]],
       device='cuda:0', requires_grad=True) 

model.module_7.lin_l.bias torch.Size([64])
Parameter containing:
tensor([-0.0930,  0.2281, -0.1311,  0.0221, -0.1026, -0.1546, -0.0581,  0.0683,
        -0.0814,  0.1839, -0.0960, -0.0945, -0.1629, -0.1142,  0.1748,  0.2391,
        -0.1933,  0.0899,  0.1450, -0.1287, -0.2233,  0.0631, -0.2281,  0.0534,
         0.2157,  0.0139,  0.1709,  0.1645,  0.1396, -0.0830, -0.1023, -0.2280,
        -0.1465,  0.1346,  0.1094, -0.1421,  0.0361, -0.0370,  0.1260, -0.1364,
        -0.0101,  0.0594,  0.0316, -0.0672,  0.1442, -0.1677, -0.0552, -0.2178,
         0.1122, -0.1235,  0.1252, -0.0313, -0.1002,  0.1909, -0.1757,  0.1404,
         0.2142, -0.2330,  0.1010, -0.2195,  0.1333, -0.1578, -0.0737, -0.1237],
       device='cuda:0', requires_grad=True) 

model.module_7.lin_r.weight torch.Size([64, 16])
Parameter containing:
tensor([[ 0.1261, -0.0211, -0.2196,  ..., -0.2049,  0.0439, -0.2028],
        [-0.0208, -0.2171, -0.1333,  ..., -0.2361, -0.2372, -0.1916],
        [-0.1802,  0.2531, -0.0743,  ..., -0.1926,  0.0504, -0.2573],
        ...,
        [-0.0817, -0.2499,  0.2385,  ...,  0.1613, -0.1533,  0.1105],
        [-0.2312,  0.0799,  0.0866,  ...,  0.0489, -0.1652, -0.2408],
        [ 0.1627,  0.2097, -0.2413,  ..., -0.2383,  0.0066,  0.0044]],
       device='cuda:0', requires_grad=True) 

model.module_7.lin_r.bias torch.Size([64])
Parameter containing:
tensor([-0.1489, -0.1644,  0.2378,  0.0293, -0.0366, -0.1083,  0.1492,  0.0259,
         0.1915, -0.0589, -0.1467,  0.1655, -0.0985,  0.2116, -0.0038,  0.0365,
        -0.1304, -0.2443, -0.1401,  0.0876, -0.2090, -0.0407, -0.0159,  0.0480,
        -0.2480, -0.0672, -0.0189,  0.0303, -0.0698,  0.2422, -0.0406,  0.2258,
         0.2253,  0.1133,  0.1239,  0.2264, -0.1963, -0.1801,  0.0707, -0.0650,
        -0.1196, -0.2053,  0.0659,  0.1898, -0.0921,  0.1162,  0.1008,  0.0097,
        -0.1386,  0.1512,  0.1113, -0.0946,  0.1856,  0.1835, -0.1299, -0.2146,
         0.0864,  0.1499, -0.0231,  0.2207, -0.0332, -0.0041,  0.1706, -0.0287],
       device='cuda:0', requires_grad=True) 

Can't access the shape of an uninitialized parameter or buffer. This error usually happens in `load_state_dict` when trying to load an uninitialized parameter into an initialized one. Call `forward` to initialize the parameters before accessing their attributes.
model.module_7.lin_edge.weight
<UninitializedParameter>
model.module_8.weight torch.Size([100, 8])
Parameter containing:
tensor([[ 2.3449e-01,  1.8045e-01,  7.1496e-02, -3.4591e-01,  3.1430e-02,
          5.2825e-02,  1.0014e-01,  3.4851e-01],
        [ 2.7006e-01,  1.1649e-01,  3.2212e-01, -2.2966e-02,  2.2866e-01,
          2.2575e-02,  7.1117e-02,  9.0269e-02],
        [-6.7368e-02,  3.3597e-01,  2.8517e-01, -1.6724e-01, -2.6558e-01,
          3.1497e-01,  1.4191e-01, -1.4662e-01],
        [-3.4882e-01, -2.6735e-02,  1.2189e-01,  3.1814e-01,  1.5214e-01,
          2.8323e-02,  3.1294e-01,  1.0768e-01],
        [ 1.1778e-01, -5.5938e-02,  3.1924e-01,  1.4804e-01, -4.4783e-02,
          1.2232e-01, -2.1948e-01, -3.0662e-01],
        [ 3.5057e-01,  1.9343e-01,  9.5104e-02, -2.4594e-01,  3.1611e-01,
         -3.6452e-02,  8.2861e-02, -2.1500e-01],
        [-6.9241e-02,  7.7760e-02, -1.5141e-01,  4.0364e-02, -2.3884e-01,
         -1.6730e-01, -2.8917e-01,  3.0095e-01],
        [ 2.5795e-02,  5.2766e-02,  2.1175e-01,  3.3200e-01,  2.8306e-01,
          2.1228e-01, -2.0771e-02,  1.3625e-01],
        [ 1.6783e-01,  1.9376e-01, -2.6869e-01, -2.7568e-01, -3.4559e-01,
          2.4108e-01,  3.0716e-01,  2.5820e-01],
        [ 7.0344e-02,  2.2805e-02, -1.6504e-01,  3.2189e-01,  2.4701e-01,
          2.8555e-02, -3.3594e-01,  2.8966e-01],
        [-1.9360e-01, -1.2560e-01,  2.8019e-01, -1.8978e-01,  2.2286e-01,
          1.7773e-01, -1.6826e-01, -3.3895e-01],
        [-9.7025e-02, -2.5847e-01, -3.3772e-01,  3.3220e-01, -1.2796e-01,
         -1.0899e-01,  2.1168e-01, -6.8240e-02],
        [-7.0499e-02, -2.8112e-01, -5.6107e-02, -2.7363e-01,  2.0114e-01,
         -3.4100e-01,  1.1077e-01, -3.3413e-01],
        [ 2.9199e-01,  2.0800e-01,  5.8963e-02, -1.7438e-03,  1.2269e-01,
          3.2443e-01, -2.8522e-01,  2.0073e-01],
        [ 2.2054e-02, -1.1669e-01,  3.3163e-01,  1.0467e-01, -3.5040e-01,
         -1.2241e-01,  1.5258e-01, -2.1353e-01],
        [-2.7884e-01, -2.6467e-01,  1.4926e-01, -8.3799e-02, -2.4141e-01,
          1.8310e-01,  1.6158e-01,  2.5157e-01],
        [-4.4409e-02, -2.2865e-01,  7.0431e-02, -1.5777e-03,  2.8150e-01,
          1.6746e-01, -1.7958e-01,  2.1852e-01],
        [ 1.0374e-01, -1.1852e-01, -8.2552e-02, -1.3793e-01,  2.8148e-01,
          1.6067e-01, -5.4410e-02,  3.3220e-01],
        [ 2.0818e-01,  2.5980e-01, -4.1388e-02, -2.6355e-02, -3.4542e-01,
         -2.0879e-01,  2.0996e-01, -5.4244e-02],
        [ 1.5170e-01, -2.5531e-01,  2.5828e-01,  3.1246e-01, -2.1874e-01,
          3.4374e-01,  2.7968e-01, -1.1653e-01],
        [ 1.7667e-01, -3.4176e-01, -4.2526e-02,  4.0138e-02,  3.2553e-01,
          2.9882e-01, -2.5832e-01, -2.0131e-02],
        [-2.3567e-01,  3.9492e-02, -3.1488e-01,  8.1377e-02,  1.5034e-01,
          1.2153e-01,  1.5923e-01,  3.2630e-01],
        [ 7.2081e-02,  1.8823e-01,  1.3407e-01, -2.7968e-01, -2.9550e-01,
          9.4698e-02, -8.5978e-02,  6.2771e-02],
        [-8.7289e-02,  1.1670e-01, -3.1584e-01, -3.8804e-02, -7.7011e-02,
          3.5177e-01,  1.7637e-01,  5.1882e-02],
        [-2.1962e-01, -2.6003e-01, -2.9585e-01, -1.8954e-01, -3.1937e-01,
          2.5121e-01,  3.4558e-01, -8.7874e-02],
        [-6.5112e-02,  2.7220e-01, -1.4426e-01,  1.3582e-01, -2.8917e-01,
          3.1715e-01, -7.7537e-02,  7.2390e-02],
        [-3.4383e-01,  9.4985e-03, -6.4139e-02, -2.1927e-01, -3.4267e-01,
         -1.6177e-01,  2.8514e-01, -8.3681e-02],
        [ 4.5075e-03,  2.9538e-01,  2.4314e-01,  3.4782e-01,  3.3279e-01,
          2.2875e-02, -2.3081e-03,  3.4190e-01],
        [-1.7301e-01, -9.4239e-03, -5.0701e-02,  8.4089e-02,  7.6260e-02,
         -1.6226e-01,  1.3173e-01,  2.1648e-02],
        [-2.9799e-02,  2.4529e-01,  2.2829e-01,  2.8898e-02, -2.8355e-01,
         -3.3428e-01, -2.0713e-01, -3.4193e-01],
        [ 3.2988e-01,  2.2557e-02, -2.8809e-01, -6.5620e-02,  1.0603e-01,
         -9.4423e-02, -6.5540e-02, -1.9446e-01],
        [ 3.0290e-01,  7.4566e-02,  3.0362e-01, -2.6180e-03,  2.0449e-01,
         -2.8558e-01,  1.5212e-01, -2.2759e-01],
        [ 2.8855e-01, -2.5362e-01, -2.5998e-01, -2.5053e-01, -3.0233e-01,
         -3.2924e-01, -2.7373e-01,  2.7984e-01],
        [-5.7170e-02,  3.6769e-02,  3.4065e-01, -1.9288e-01, -9.9282e-02,
         -2.6566e-01,  2.5599e-01, -3.1725e-01],
        [-2.4038e-01, -5.8733e-02,  5.7584e-02,  2.4027e-01, -9.5462e-02,
         -4.3862e-02,  1.3557e-01,  1.6631e-01],
        [ 3.4421e-01,  2.7335e-01, -2.7210e-01,  1.8383e-01,  2.2447e-01,
         -3.4675e-01,  1.7338e-01, -2.2176e-01],
        [ 1.7419e-01,  9.2997e-02, -4.3547e-02, -6.8123e-02, -2.2243e-01,
         -2.5354e-01,  9.5774e-02,  4.0177e-02],
        [-2.3306e-01, -2.9264e-01,  8.8189e-02,  1.7372e-01,  3.0851e-01,
         -2.0913e-01, -1.5710e-01, -1.7950e-01],
        [ 1.3213e-01, -3.4608e-01, -3.2465e-01, -7.5416e-02, -1.5747e-01,
          2.1190e-01, -1.4182e-01,  2.6397e-02],
        [-2.4922e-01,  2.8758e-01,  2.6534e-01,  8.7424e-02,  1.3329e-01,
         -3.2520e-01, -2.5314e-01, -1.4155e-02],
        [-1.2857e-02,  1.2586e-01,  1.1834e-01, -1.9314e-02,  1.5748e-01,
         -6.5560e-02,  1.9024e-01, -2.7200e-01],
        [ 2.1055e-01,  3.0859e-01, -3.8910e-03,  1.9343e-01, -2.3015e-01,
         -9.7728e-03, -1.0692e-01, -3.8550e-03],
        [-1.2649e-01,  2.5361e-01, -1.4892e-01,  1.1380e-01,  1.9247e-01,
          3.0644e-01,  2.7483e-01, -2.0889e-01],
        [-5.2850e-02,  2.4125e-01,  9.4167e-02,  1.7685e-01,  1.1580e-01,
         -3.3373e-01,  1.6517e-01,  2.8377e-01],
        [ 1.6826e-01, -1.5415e-01, -7.7181e-02, -1.2591e-01,  8.2865e-02,
          1.6147e-01, -2.5374e-01,  1.5845e-02],
        [-5.4872e-02,  2.3337e-01, -5.2093e-02, -2.1398e-01,  2.0004e-01,
         -1.5870e-01, -2.5389e-02, -9.9111e-02],
        [ 1.4467e-01, -7.5788e-02,  1.1372e-01, -2.3899e-01, -4.5703e-02,
          3.3434e-01, -2.1207e-01, -5.1707e-02],
        [-2.3566e-02,  4.2533e-02, -1.4266e-01,  1.2685e-01, -9.9927e-02,
          4.0564e-02,  3.2451e-01,  3.1121e-01],
        [-1.0390e-01, -1.9694e-01, -2.1747e-01, -5.7284e-02,  1.7892e-02,
          1.0551e-01, -1.2923e-01,  1.7027e-01],
        [-1.9905e-02,  8.9495e-02,  3.3759e-01,  1.4310e-02,  1.2689e-02,
         -9.6439e-02,  2.5383e-01, -3.0623e-01],
        [-1.7017e-01,  5.2115e-02,  2.0298e-01, -3.4947e-01, -5.4954e-02,
          6.9509e-02,  3.0442e-01,  1.6302e-01],
        [ 3.1412e-01, -6.6276e-02, -3.4261e-01,  3.7976e-02,  2.3401e-01,
          7.8109e-03,  5.8706e-02,  3.4296e-01],
        [-3.3048e-01,  5.9212e-02,  1.0711e-01,  6.8014e-02, -8.9808e-02,
         -1.5108e-01,  2.0180e-01,  8.8269e-02],
        [ 9.8580e-02, -1.0861e-02, -2.3115e-01,  1.5186e-01, -1.9062e-01,
         -2.2442e-01, -1.6626e-01, -2.8856e-01],
        [-2.8837e-01, -3.8466e-02, -3.4699e-01,  2.5856e-01, -2.7251e-01,
          1.3014e-01,  1.4620e-01, -1.3991e-01],
        [ 7.2959e-02,  3.3140e-01, -3.3232e-01,  6.7997e-03,  2.3743e-01,
          3.3100e-01,  9.7705e-02,  3.1083e-01],
        [-1.8895e-01,  3.2107e-01,  5.1398e-02, -2.2707e-01,  2.1204e-01,
          2.8632e-01, -2.9545e-01,  1.8448e-01],
        [ 9.2496e-02,  3.0109e-01, -3.3549e-01,  1.8016e-01,  3.3951e-01,
         -1.0935e-01,  1.1194e-01, -3.1424e-01],
        [ 8.4182e-02, -2.4996e-01, -3.3683e-01,  1.5845e-01,  1.9237e-01,
          1.9625e-01,  2.6392e-01, -9.3869e-02],
        [-2.5904e-01,  2.0380e-01,  2.8644e-01, -2.6879e-01, -2.9898e-01,
         -2.1763e-01, -2.0929e-01,  2.2889e-01],
        [-3.2767e-01,  1.2179e-01,  5.5134e-02, -2.5131e-01, -3.0640e-01,
          2.4871e-01, -2.3000e-01, -2.3662e-01],
        [ 4.4798e-02, -3.2523e-01, -1.8818e-02, -8.8604e-02, -1.2804e-01,
         -3.3612e-01, -2.3478e-01,  2.6187e-01],
        [-2.2837e-01,  1.7193e-01,  2.6608e-01, -1.7332e-03,  1.7944e-01,
         -2.4128e-01, -2.6666e-01,  2.5708e-01],
        [ 1.9644e-01,  4.6419e-02, -2.2831e-01, -1.4111e-01,  3.0825e-01,
          3.4879e-01, -2.8118e-01,  1.4852e-01],
        [-2.1638e-01, -5.9528e-02, -2.2114e-02, -2.7801e-01,  7.3340e-04,
         -9.3262e-02,  3.0261e-01, -1.6648e-01],
        [-2.7662e-01,  4.8917e-02,  3.4522e-01,  3.0836e-01,  1.4414e-01,
         -3.3219e-01,  1.9153e-01, -7.7659e-02],
        [-2.7095e-01, -1.3450e-01, -3.3076e-01, -1.5439e-01,  1.4340e-01,
          3.1256e-01, -2.9129e-01,  1.3120e-01],
        [ 8.8271e-02, -2.2193e-01, -8.4940e-02, -2.4143e-01, -2.3248e-01,
          2.9458e-01,  3.2795e-01,  1.2623e-01],
        [-6.8478e-02, -1.5429e-01,  9.9241e-02, -3.1824e-01, -2.3691e-01,
          1.7425e-01,  2.3612e-01, -2.6792e-02],
        [-1.7047e-01,  1.5998e-01,  2.2443e-01, -3.4654e-01, -7.2328e-02,
         -3.2932e-01, -2.9996e-02,  2.7730e-01],
        [-3.2267e-01, -2.4491e-01, -5.1508e-02,  1.0872e-01,  3.4471e-01,
          1.4504e-01,  2.2935e-01,  3.0770e-01],
        [ 2.1169e-01, -1.9910e-01, -2.1201e-01,  2.3105e-01, -7.2817e-02,
         -3.0417e-01,  9.2229e-02, -2.7696e-01],
        [ 8.6879e-02,  3.4358e-01,  2.9500e-01, -6.0581e-03,  3.1008e-02,
          7.0718e-02, -1.2340e-01, -2.0873e-01],
        [ 3.4190e-01, -3.2865e-01,  5.9240e-02, -1.3950e-01,  1.4437e-01,
         -1.8304e-01,  1.6715e-01, -2.2466e-01],
        [-1.7102e-01,  2.8527e-01,  3.1050e-02,  3.1314e-01, -2.2995e-01,
          2.3798e-01,  4.4451e-02,  2.8046e-01],
        [-8.6616e-02, -4.3829e-03,  3.0209e-02, -2.1500e-01,  4.4321e-02,
         -2.0835e-01,  1.8851e-01,  3.1395e-01],
        [-8.7010e-02, -5.7409e-02,  1.1244e-01, -3.5019e-03, -2.2161e-02,
         -1.0054e-01,  2.7069e-01, -7.2954e-02],
        [ 3.1568e-01,  1.9687e-01,  2.5511e-01,  6.0018e-02, -1.1342e-01,
         -2.0277e-01,  3.1660e-01, -2.5013e-01],
        [ 2.1685e-02, -2.7518e-01, -3.4200e-01,  2.6371e-02, -1.7390e-01,
         -2.4957e-01, -1.9796e-01,  3.3806e-01],
        [ 2.9581e-03,  7.4741e-02, -2.3534e-01, -1.1603e-01,  2.4892e-01,
          2.8131e-01,  3.2870e-04, -3.2049e-01],
        [ 3.3506e-01, -2.3012e-01, -1.7881e-01,  2.5396e-01,  1.1570e-01,
         -1.0685e-02,  1.9993e-01,  1.4686e-01],
        [ 3.4698e-01,  3.0820e-01,  8.9270e-02,  3.0100e-01, -3.4210e-01,
         -2.1883e-01, -2.5038e-01, -7.8647e-02],
        [ 3.2837e-01,  5.8573e-02, -3.1169e-02, -2.7761e-02, -1.3902e-01,
         -1.1919e-01,  3.1256e-01, -2.1710e-01],
        [ 1.0462e-01,  2.2103e-02,  3.3868e-01, -1.7745e-01, -9.3239e-02,
         -2.0265e-01, -5.6254e-02, -2.2506e-01],
        [-2.1634e-01, -3.4382e-02,  4.4304e-02,  1.5791e-01,  9.6343e-02,
         -2.0324e-01, -3.4454e-01, -3.2629e-01],
        [-1.8380e-02,  2.8783e-02,  2.8636e-01,  3.2871e-01,  3.5231e-01,
          3.0092e-01,  2.8911e-01, -6.0913e-03],
        [ 1.6272e-01, -2.4163e-02, -2.8882e-01,  1.1966e-01,  3.4150e-01,
         -1.6320e-01,  2.8460e-01, -2.0664e-01],
        [-2.3982e-01,  2.5729e-01, -2.9540e-01,  1.1459e-01,  1.7101e-01,
          2.4841e-01, -1.2386e-01, -3.0261e-01],
        [ 3.4738e-01, -4.4551e-02,  1.7233e-01,  1.5788e-03, -2.1512e-01,
         -3.4425e-01,  2.9632e-01, -3.4997e-01],
        [-2.3605e-01,  2.5123e-01,  5.3453e-02, -1.1125e-02, -2.4314e-01,
         -3.4515e-01, -7.7905e-02,  1.7224e-01],
        [ 1.3630e-01,  1.5150e-01,  2.7823e-01,  4.3926e-02, -6.1659e-02,
         -2.3038e-01, -5.6127e-03,  1.6074e-01],
        [ 9.2456e-02,  2.7882e-01,  3.1621e-01,  1.3784e-01, -3.2221e-01,
         -1.5136e-01,  1.4789e-01,  2.9342e-01],
        [-1.7004e-02, -2.1234e-01,  1.1753e-01,  1.3927e-03, -1.6003e-01,
          3.1163e-01, -2.1628e-02,  3.5340e-01],
        [-1.2030e-01,  2.3222e-01,  5.4841e-03, -1.3671e-01,  2.6559e-01,
         -3.0380e-01, -2.7046e-02,  7.5555e-02],
        [-4.9471e-02, -1.9471e-01, -8.8898e-02,  2.3331e-01, -3.4345e-02,
         -1.4118e-01, -1.3190e-01, -2.6151e-01],
        [ 2.1796e-01, -3.2562e-01, -1.4878e-01,  6.1518e-02,  3.2372e-01,
         -2.8964e-01,  5.0854e-02, -2.4873e-01],
        [ 2.5567e-01,  2.6594e-01,  2.1922e-01, -1.5497e-01, -2.7268e-01,
         -2.0344e-01, -1.1807e-01,  1.6013e-01],
        [-5.4071e-02,  3.0651e-01, -3.3343e-01,  1.6224e-02, -3.0089e-01,
         -3.3472e-01, -2.5422e-01,  3.9879e-02],
        [-3.2562e-01, -4.4025e-02, -1.5652e-01, -1.9338e-01,  3.0885e-01,
         -1.3362e-01, -3.4003e-01,  2.4340e-01],
        [-3.5317e-01, -2.0873e-02, -2.7109e-01,  1.7762e-01,  1.2493e-01,
         -4.1732e-02, -1.0219e-01, -2.5237e-01]], device='cuda:0',
       requires_grad=True) 

model.module_8.bias torch.Size([100])
Parameter containing:
tensor([-0.2465,  0.1356, -0.2491, -0.1260,  0.2924, -0.3231,  0.0064, -0.1132,
         0.1785, -0.1147,  0.2809,  0.1029, -0.1990,  0.0669, -0.0108, -0.0114,
        -0.1919,  0.1796,  0.0584, -0.0111, -0.1866,  0.3431, -0.3120,  0.1528,
        -0.3198, -0.2705,  0.1267,  0.0896, -0.0572,  0.1494, -0.0289,  0.1204,
        -0.3025,  0.1729, -0.0918,  0.2269, -0.3182,  0.0358, -0.1161,  0.1381,
        -0.2544, -0.2779,  0.1144, -0.0192, -0.2535, -0.2610, -0.2779,  0.0716,
         0.1503, -0.1702,  0.2363,  0.2223,  0.0514, -0.2205, -0.2679, -0.1269,
        -0.2106,  0.0895,  0.2331,  0.3261, -0.1787, -0.1699,  0.0443, -0.0724,
         0.2286, -0.3278,  0.1773,  0.0098,  0.2968, -0.1112,  0.1308,  0.0777,
        -0.2219, -0.2488,  0.2189, -0.1596,  0.2589,  0.0549,  0.0364,  0.2166,
        -0.1090, -0.1033,  0.1225,  0.0742,  0.1715,  0.2367,  0.3051,  0.2079,
        -0.0098, -0.2988, -0.1273, -0.0365,  0.0272, -0.2072, -0.1322,  0.2522,
        -0.1988,  0.0227,  0.0742,  0.0251], device='cuda:0',
       requires_grad=True) 

model.module_10.weight torch.Size([100, 100])
Parameter containing:
tensor([[ 0.0099,  0.0910,  0.0740,  ...,  0.0216, -0.0476, -0.0698],
        [-0.0425, -0.0441, -0.0717,  ...,  0.0460, -0.0548, -0.0608],
        [ 0.0038,  0.0580,  0.0429,  ...,  0.0068, -0.0380,  0.0113],
        ...,
        [ 0.0971,  0.0434, -0.0285,  ...,  0.0544, -0.0282,  0.0816],
        [ 0.0474,  0.0400, -0.0641,  ..., -0.0596,  0.0041,  0.0382],
        [-0.0722, -0.0200,  0.0781,  ...,  0.0751, -0.0270, -0.0087]],
       device='cuda:0', requires_grad=True) 

model.module_10.bias torch.Size([100])
Parameter containing:
tensor([ 0.0759, -0.0522,  0.0973,  0.0510,  0.0821,  0.0085,  0.0499,  0.0955,
         0.0753, -0.0662, -0.0072,  0.0874, -0.0861,  0.0266, -0.0816,  0.0519,
        -0.0475, -0.0252,  0.0601, -0.0208, -0.0732,  0.0481,  0.0052,  0.0914,
        -0.0943, -0.0714,  0.0537, -0.0769, -0.0624,  0.0836, -0.0575, -0.0637,
         0.0134,  0.0312, -0.0341,  0.0356, -0.0150, -0.0029, -0.0811, -0.0053,
        -0.0366,  0.0753,  0.0014, -0.0007, -0.0039,  0.0132, -0.0418,  0.0783,
         0.0875,  0.0581, -0.0049,  0.0790, -0.0158, -0.0668, -0.0514,  0.0546,
        -0.0229,  0.0805, -0.0258, -0.0102, -0.0447, -0.0788,  0.0794,  0.0524,
        -0.0189, -0.0640,  0.0672,  0.0709,  0.0208, -0.0712, -0.0649,  0.0853,
        -0.0675,  0.0194,  0.0752,  0.0784,  0.0271, -0.0356,  0.0174, -0.0638,
         0.0031, -0.0497,  0.0907, -0.0802, -0.0029,  0.0451, -0.0088,  0.0848,
        -0.0623,  0.0512, -0.0478,  0.0687, -0.0422, -0.0467,  0.0120,  0.0813,
        -0.0364,  0.0654, -0.0093,  0.0431], device='cuda:0',
       requires_grad=True) 

model.module_12.weight torch.Size([16, 100])
Parameter containing:
tensor([[-0.0188,  0.0718,  0.0040,  ..., -0.0547,  0.0174,  0.0222],
        [ 0.0596, -0.0500, -0.0479,  ...,  0.0039,  0.0301,  0.0338],
        [ 0.0672,  0.0868,  0.0826,  ..., -0.0271, -0.0653, -0.0540],
        ...,
        [-0.0494, -0.0702, -0.0197,  ..., -0.0925,  0.0914,  0.0787],
        [-0.0290, -0.0961, -0.0604,  ..., -0.0275,  0.0241,  0.0086],
        [-0.0030, -0.0625, -0.0918,  ...,  0.0973, -0.0920, -0.0296]],
       device='cuda:0', requires_grad=True) 

model.module_12.bias torch.Size([16])
Parameter containing:
tensor([ 0.0932, -0.0856, -0.0121, -0.0565,  0.0484,  0.0935, -0.0828,  0.0812,
         0.0708,  0.0539, -0.0031, -0.0408,  0.0642, -0.0471, -0.0143, -0.0685],
       device='cuda:0', requires_grad=True) 

model.module_14.att torch.Size([1, 8, 8])
Parameter containing:
tensor([[[-0.3003, -0.0149,  0.1686,  0.5373,  0.5301, -0.5581,  0.0228,
           0.5333],
         [ 0.1013, -0.5856,  0.4474, -0.4449, -0.4168, -0.4383,  0.5806,
           0.5053],
         [-0.1829,  0.2073,  0.1781, -0.6035, -0.3552, -0.2546,  0.0157,
          -0.1733],
         [ 0.4494, -0.3866, -0.3175,  0.0555, -0.0516, -0.0956, -0.3038,
           0.3626],
         [ 0.3378,  0.0669, -0.0057,  0.5678,  0.0443, -0.2186, -0.3601,
           0.3442],
         [-0.3473,  0.2165, -0.1635,  0.2689,  0.2841,  0.4369, -0.0502,
          -0.2657],
         [ 0.0804, -0.2323,  0.2304, -0.0302, -0.0243,  0.5789, -0.4501,
           0.4991],
         [ 0.2171, -0.3686,  0.4962, -0.1926, -0.4883, -0.1538,  0.0313,
          -0.5283]]], device='cuda:0', requires_grad=True) 

model.module_14.bias torch.Size([64])
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True) 

model.module_14.lin_l.weight torch.Size([64, 16])
Parameter containing:
tensor([[ 0.2036,  0.0110,  0.1759,  ...,  0.2234, -0.0096, -0.1133],
        [-0.0443,  0.1893, -0.0839,  ...,  0.1737, -0.0610,  0.1528],
        [-0.0728,  0.0643,  0.0926,  ..., -0.1706, -0.1813, -0.1708],
        ...,
        [-0.2728, -0.1817,  0.0224,  ..., -0.0053, -0.0546,  0.0884],
        [ 0.0955,  0.0791, -0.0357,  ...,  0.0619,  0.1968,  0.1449],
        [-0.1324, -0.2713,  0.0361,  ...,  0.0024,  0.2295, -0.0350]],
       device='cuda:0', requires_grad=True) 

model.module_14.lin_l.bias torch.Size([64])
Parameter containing:
tensor([ 0.0605,  0.0675,  0.2130,  0.2493,  0.0059,  0.1660,  0.2050, -0.0629,
        -0.1727,  0.1330,  0.0537, -0.0800,  0.2153, -0.1832,  0.1894, -0.0736,
        -0.0382, -0.0059,  0.1130,  0.1769,  0.0355, -0.1077,  0.1574,  0.1483,
         0.1834, -0.0132, -0.0181,  0.0107,  0.0072,  0.0820,  0.2380, -0.1210,
        -0.2460,  0.0178, -0.0552, -0.1109,  0.0818,  0.2383,  0.1736, -0.0456,
         0.1918, -0.0738, -0.2297, -0.1103, -0.1893,  0.0728,  0.1756,  0.0766,
        -0.0426,  0.0827, -0.0209, -0.1878,  0.2472,  0.0457, -0.2235,  0.0316,
         0.2185, -0.0199,  0.0045, -0.0901, -0.1100, -0.1186,  0.0558, -0.2406],
       device='cuda:0', requires_grad=True) 

model.module_14.lin_r.weight torch.Size([64, 16])
Parameter containing:
tensor([[-0.1353,  0.2266,  0.0555,  ..., -0.2295, -0.0138,  0.1319],
        [-0.2662, -0.1531, -0.1868,  ..., -0.1535, -0.0544, -0.1729],
        [ 0.1328, -0.2372,  0.0959,  ..., -0.0659, -0.2035,  0.2377],
        ...,
        [-0.0033, -0.1488,  0.1982,  ..., -0.2423, -0.0129,  0.0992],
        [-0.1004,  0.0677, -0.0787,  ...,  0.0949,  0.1227,  0.0329],
        [-0.1265, -0.1206, -0.0959,  ...,  0.0689,  0.1873,  0.2278]],
       device='cuda:0', requires_grad=True) 

model.module_14.lin_r.bias torch.Size([64])
Parameter containing:
tensor([ 0.0347,  0.2270, -0.1547, -0.2372,  0.0632,  0.1447,  0.1428, -0.1885,
         0.2193, -0.2117,  0.2080, -0.0865,  0.2382,  0.1191,  0.1647,  0.0101,
        -0.2437, -0.2155, -0.2134, -0.0903, -0.0780,  0.0704, -0.2258, -0.1947,
        -0.0248, -0.0570, -0.0477, -0.2157, -0.2374, -0.2199, -0.1220, -0.0373,
        -0.1878, -0.2278, -0.0319,  0.2456,  0.1725, -0.0340, -0.1870,  0.0471,
        -0.2157, -0.1746,  0.0279,  0.0574, -0.1185, -0.0036, -0.0074, -0.0520,
         0.0673,  0.1131, -0.1412, -0.0328, -0.2156, -0.1477, -0.2464, -0.1933,
        -0.1590, -0.0062, -0.0068,  0.0310,  0.1802,  0.0493, -0.0039,  0.0872],
       device='cuda:0', requires_grad=True) 

Can't access the shape of an uninitialized parameter or buffer. This error usually happens in `load_state_dict` when trying to load an uninitialized parameter into an initialized one. Call `forward` to initialize the parameters before accessing their attributes.
model.module_14.lin_edge.weight
<UninitializedParameter>
model.module_15.weight torch.Size([100, 8])
Parameter containing:
tensor([[ 3.1380e-02,  1.7911e-01, -2.9994e-01,  1.8411e-01,  1.0237e-01,
          3.3509e-01,  3.4137e-01, -4.2196e-02],
        [-1.1758e-01, -3.5150e-01, -1.7969e-01,  7.1697e-02,  2.1460e-01,
          2.7138e-01,  1.5581e-01, -3.5331e-02],
        [ 9.0413e-02,  1.9247e-01,  2.6844e-01,  2.3234e-01, -1.9597e-01,
         -1.6343e-01, -5.6996e-02, -1.8696e-02],
        [-3.3892e-01,  2.7683e-01,  2.2823e-01,  3.0324e-01,  1.3554e-01,
          3.4722e-01, -1.4635e-02, -6.7791e-02],
        [ 3.5975e-02,  9.9776e-02,  1.0415e-01, -1.9106e-02, -1.9174e-03,
         -2.3500e-01, -5.4841e-03,  1.8631e-01],
        [-4.2627e-04, -3.3261e-01, -5.0288e-02, -1.0996e-01,  2.4353e-01,
          2.3183e-01, -2.1808e-01,  1.4283e-01],
        [ 8.2783e-02, -2.3022e-01, -1.2574e-01,  3.4990e-02,  1.7113e-01,
         -1.3206e-01,  9.7178e-02,  1.1387e-01],
        [-2.8954e-01, -8.3429e-02,  9.8263e-02,  1.6897e-01,  5.6529e-02,
         -1.3388e-01, -1.2270e-01,  1.4919e-01],
        [ 1.5067e-01, -5.9320e-02,  3.5129e-01,  6.3900e-02, -2.9130e-01,
         -8.9372e-04, -4.3560e-02, -7.0754e-02],
        [ 1.3690e-02, -1.0642e-01, -1.7013e-01,  2.2259e-01,  2.2479e-01,
         -2.1638e-01,  4.7673e-02,  1.1318e-01],
        [-2.0426e-02, -5.7206e-04, -7.2438e-02, -2.8520e-02, -2.8400e-01,
          3.4012e-01,  3.4232e-01,  1.9542e-01],
        [-1.3874e-01, -1.0695e-01,  8.2373e-02,  1.8293e-01, -5.2676e-02,
         -6.7488e-02, -3.1652e-01, -2.6638e-01],
        [ 2.4467e-02, -1.5701e-02, -3.2100e-02,  3.1038e-01,  1.1559e-01,
         -2.6483e-01, -2.4948e-01,  8.3823e-02],
        [ 2.1621e-01, -3.2622e-01,  1.2965e-01, -1.5779e-01,  1.3387e-01,
         -1.8416e-01,  2.9730e-01,  3.3076e-01],
        [-2.4642e-01, -3.1270e-01,  1.0207e-01,  2.8210e-01,  1.5510e-01,
         -1.7237e-01,  3.0006e-01, -3.4992e-01],
        [ 2.9490e-01, -1.2738e-01, -2.4447e-02, -6.2892e-04, -9.7741e-02,
         -8.0860e-02,  8.7091e-02,  6.8227e-02],
        [ 4.0969e-02,  3.4707e-01, -3.4777e-02, -2.1889e-01, -3.0975e-01,
          2.6802e-01, -2.5582e-01,  1.7838e-01],
        [-2.0608e-01, -2.3604e-01,  5.5773e-02,  2.6865e-01, -3.1838e-01,
          2.3232e-01, -3.5182e-01, -7.1217e-02],
        [-1.0023e-01, -2.5082e-01, -2.7396e-01, -1.4739e-01, -2.8301e-01,
          3.6072e-03, -2.3633e-01, -1.6451e-01],
        [-1.7635e-01,  1.0397e-01, -1.0146e-01,  2.0530e-01,  2.1548e-01,
         -2.6002e-01, -6.0018e-02, -3.5244e-01],
        [ 1.6924e-01, -7.7040e-02,  9.1081e-02,  1.1648e-01,  3.1345e-01,
         -3.5254e-02, -1.0459e-01,  3.3364e-01],
        [-2.7827e-02,  6.4765e-02,  3.4938e-02, -1.8639e-01,  2.0384e-01,
         -2.9234e-01,  1.7679e-01,  1.7217e-01],
        [ 2.3087e-01, -2.4984e-01,  2.4824e-01,  3.2664e-01, -5.8816e-03,
         -3.3467e-01, -1.6718e-02, -1.9049e-03],
        [-3.0929e-01, -2.1903e-02, -1.1680e-01, -3.2052e-01,  2.2966e-01,
          9.3063e-02, -1.5064e-01,  3.2087e-01],
        [ 1.7964e-02,  1.9528e-02,  4.3530e-02, -2.9210e-01, -1.4922e-01,
          4.7657e-02, -1.3073e-01,  1.8396e-01],
        [ 9.3894e-03,  7.4825e-02,  3.4224e-01, -1.5167e-01, -4.7625e-02,
         -2.0734e-01, -3.0939e-01,  8.0690e-02],
        [ 2.5161e-01,  1.1967e-01, -2.7788e-01,  2.2888e-01, -3.2285e-01,
         -2.4752e-01,  3.2175e-01, -1.6227e-01],
        [-1.8468e-01, -1.4696e-01, -1.6965e-01,  9.9187e-02, -1.8635e-01,
         -3.4819e-01, -1.0694e-01,  3.6180e-02],
        [ 2.2749e-02,  3.4433e-03, -3.5271e-01, -9.9583e-02, -1.9922e-01,
         -8.5082e-02, -1.9837e-01, -1.1007e-02],
        [-6.4165e-02, -3.2354e-01,  1.5774e-01,  1.1030e-01,  9.2325e-02,
          1.3344e-01, -1.7497e-02,  6.7913e-02],
        [-3.4532e-01,  2.2872e-01,  2.7892e-01, -1.4028e-01,  2.9482e-01,
         -1.0794e-01,  1.4272e-01, -1.3591e-01],
        [-1.9780e-01, -1.8271e-01,  2.4039e-01, -1.0919e-01, -2.7732e-01,
          4.2114e-02, -2.1947e-01, -2.2234e-01],
        [-1.9720e-01, -2.1623e-01, -2.2665e-01,  1.3351e-01, -1.2548e-01,
          8.5053e-02,  3.0112e-01,  8.5781e-02],
        [ 3.4766e-01,  2.8976e-01,  2.9626e-02,  1.6552e-01,  3.3910e-01,
         -2.5551e-01, -2.2241e-01,  2.5449e-01],
        [ 1.7919e-01,  1.0575e-01,  5.7206e-02, -2.0066e-01,  1.3292e-01,
         -2.9420e-02,  1.4406e-02,  3.1643e-01],
        [ 1.8201e-01,  3.0428e-03, -8.0463e-02, -2.1800e-01,  1.4886e-01,
          3.3775e-01, -1.8031e-01, -2.4497e-01],
        [ 2.2940e-01, -3.0736e-01,  3.2276e-01,  4.4807e-02,  3.0838e-01,
          2.6980e-02,  1.0411e-01, -3.3130e-01],
        [-5.3911e-02,  2.6352e-01, -1.4003e-01,  2.9150e-01, -5.2382e-02,
         -1.7940e-01, -1.7458e-01, -2.7473e-01],
        [ 1.0499e-01, -1.9504e-01,  1.4644e-01, -9.6862e-02,  1.8606e-01,
          2.5126e-01, -2.6022e-01, -2.1182e-01],
        [ 2.3854e-01, -9.6660e-02, -2.9433e-01, -2.0371e-01, -3.3832e-01,
         -2.2268e-01,  2.1200e-05,  2.5032e-01],
        [ 6.5395e-02,  3.3718e-02, -9.6455e-03, -2.1290e-01,  1.6623e-01,
          2.3972e-01,  1.1597e-01,  2.9385e-01],
        [ 3.4519e-01,  4.0052e-02, -1.9373e-01,  2.5417e-01,  3.5957e-02,
         -7.1056e-02, -4.1584e-02,  4.7493e-02],
        [-1.6799e-01,  2.1422e-01, -2.6861e-01,  2.9379e-02, -3.4817e-01,
          1.3700e-01, -1.6632e-01, -2.7336e-02],
        [ 2.7318e-01,  6.1567e-02,  2.0730e-02,  1.4119e-01,  2.4770e-02,
          2.2840e-01,  2.8818e-02,  2.2496e-01],
        [ 2.3329e-01, -5.3136e-02,  2.9400e-01,  6.3883e-02,  2.5866e-01,
         -3.3297e-01, -3.3214e-01,  9.6785e-02],
        [ 1.1039e-01, -1.2976e-01, -9.1017e-02,  8.6198e-02,  3.1448e-01,
          7.8511e-02,  2.9144e-01,  3.0966e-01],
        [ 1.0645e-01,  3.0379e-02, -9.4788e-02, -2.5410e-01,  1.9653e-01,
          1.1474e-03, -1.4563e-01, -8.8241e-02],
        [-2.0520e-01, -2.9985e-02,  2.0163e-01,  2.8537e-01, -2.2869e-01,
          5.5844e-02, -1.3970e-01, -3.3380e-01],
        [ 1.9747e-01,  2.8291e-01,  2.9818e-01,  3.0349e-01, -1.2440e-01,
          7.6671e-02,  1.1965e-01, -2.4487e-01],
        [ 3.1478e-01, -3.0423e-01,  1.2512e-02, -1.1241e-01,  1.5613e-01,
          2.7202e-01,  3.4727e-02, -1.6311e-01],
        [ 2.7375e-01, -3.4529e-01, -2.6434e-02, -3.1843e-01, -8.9520e-03,
          2.5819e-01,  1.6348e-01, -1.6734e-01],
        [-1.7655e-01,  1.1859e-01, -2.1749e-01, -2.9998e-01,  2.7762e-01,
          1.0275e-01, -2.7003e-01, -3.2514e-01],
        [ 1.2022e-01, -1.0927e-01, -1.8765e-01,  3.5063e-01,  1.7595e-01,
          2.7029e-01,  5.6858e-02, -8.1351e-03],
        [-2.3147e-01,  1.4158e-01, -1.1166e-01, -3.6643e-02,  1.9005e-02,
          2.6631e-01,  1.3169e-01, -3.4733e-01],
        [ 5.3344e-03,  2.5671e-01, -2.0933e-02,  2.9700e-01, -1.5090e-01,
         -2.5944e-01,  3.2107e-01, -1.2587e-01],
        [ 2.6150e-01,  2.6648e-01, -2.3400e-01, -3.1764e-01, -4.1935e-02,
         -1.9761e-01,  2.5858e-01,  1.7410e-01],
        [ 2.9998e-01, -1.6052e-01,  6.7863e-02,  8.1673e-03, -2.5927e-01,
         -1.7271e-01,  1.6311e-01,  2.7980e-02],
        [ 3.1301e-01,  2.7600e-02,  1.0385e-01,  1.2613e-01, -2.3755e-01,
         -2.6538e-01, -2.2332e-01, -1.2664e-01],
        [ 1.1237e-01, -1.8939e-01, -2.8149e-01, -1.0364e-01, -3.0076e-01,
         -2.9391e-02, -3.3284e-01, -4.3052e-02],
        [-2.7321e-01, -3.2370e-01,  2.5381e-01,  3.4392e-01, -2.4764e-01,
          5.5961e-02, -2.9641e-01, -2.4622e-01],
        [-3.1286e-01, -2.5776e-01,  3.4294e-01,  8.9617e-02,  1.5830e-01,
         -2.0628e-01,  1.4595e-01,  1.2534e-01],
        [ 1.0497e-01,  2.5074e-02, -1.7107e-01,  1.3022e-01,  2.7915e-01,
         -6.7714e-02,  2.9545e-01,  6.2602e-02],
        [ 8.2978e-02,  2.4289e-01, -4.7104e-02,  7.6306e-02, -6.7074e-02,
         -2.9196e-01,  2.3975e-02,  9.5748e-02],
        [ 1.8791e-02,  8.1403e-02, -7.9034e-02,  2.8193e-01, -2.9531e-01,
         -1.3280e-03,  1.5910e-02,  1.2755e-01],
        [-1.1450e-01,  6.6377e-02, -2.9171e-01,  1.3069e-01, -1.1233e-01,
          2.4454e-01,  3.0797e-01,  3.1169e-01],
        [-2.0452e-01,  1.6487e-01, -1.6379e-01,  5.6721e-02,  1.3273e-01,
         -3.0408e-01, -3.3362e-01,  2.7759e-02],
        [ 4.1208e-02, -2.2698e-01,  1.7332e-01,  2.2121e-01, -5.8838e-02,
         -3.2061e-01,  9.5351e-02, -1.9296e-01],
        [-3.4272e-02, -7.4934e-02,  1.8541e-01,  9.5108e-02, -2.4674e-01,
         -3.1223e-01, -1.8683e-01, -3.4710e-01],
        [-2.6840e-03, -1.5942e-01, -1.2082e-01, -3.2009e-01,  2.8954e-01,
         -2.1617e-01, -1.2123e-01,  2.4706e-01],
        [-5.7731e-02, -7.6024e-02,  1.7248e-01, -1.4008e-01,  2.5023e-01,
         -2.1675e-02,  1.8294e-01,  2.3414e-02],
        [-1.5988e-01, -1.6447e-01,  1.5069e-02, -9.6061e-02,  2.1828e-01,
          2.1260e-01, -1.8743e-01, -2.0297e-01],
        [-5.8719e-02,  2.8147e-01, -1.4228e-03,  2.8354e-01,  1.0490e-01,
         -1.5352e-03, -2.8600e-01, -1.7168e-02],
        [-2.3329e-01, -1.8130e-01,  1.7945e-01, -2.0686e-01, -1.8412e-01,
          2.8054e-01, -1.4840e-01, -2.6442e-01],
        [-9.3442e-02, -8.0458e-03, -3.0411e-01, -3.3918e-01, -3.0941e-01,
          1.4764e-03, -3.1791e-02, -3.3213e-01],
        [-2.3168e-01,  4.8308e-03, -3.1318e-01, -2.0118e-01, -1.2761e-01,
         -3.4119e-01, -5.5284e-02, -1.1817e-01],
        [ 6.1810e-02,  1.5762e-02,  1.6277e-01, -1.0647e-01, -1.6565e-01,
         -2.1380e-01,  2.9633e-01, -2.6264e-01],
        [-6.1531e-02, -1.6751e-01,  7.5584e-02, -2.5789e-01, -2.4175e-01,
          2.3149e-01, -3.1687e-01,  4.8534e-03],
        [ 1.0513e-01, -3.4787e-01,  1.7253e-01, -2.0042e-01,  2.2897e-02,
         -3.4153e-01,  2.0755e-01,  3.3794e-01],
        [ 3.5293e-01, -1.8683e-01, -5.8097e-02,  2.8850e-01, -6.0147e-03,
          1.8561e-01,  2.6444e-01, -2.8155e-01],
        [-2.5892e-01,  3.0066e-01, -1.3151e-01, -2.2337e-01,  1.1720e-01,
         -1.8218e-02, -1.2259e-01, -2.5439e-01],
        [ 6.2653e-02, -8.3863e-02, -2.6372e-01, -3.5741e-02, -4.2620e-02,
         -3.2924e-01,  2.5198e-01, -3.0722e-01],
        [ 2.5272e-02,  5.5560e-02, -8.7806e-02, -2.2380e-01, -7.4286e-02,
          1.7935e-02,  1.4052e-02, -7.6612e-02],
        [ 1.5945e-01,  1.0415e-01, -3.2407e-01, -2.1166e-01, -1.2588e-01,
          8.3372e-02, -1.8013e-01,  2.7774e-01],
        [ 9.7124e-02,  2.3831e-01,  9.8047e-02,  6.1919e-02,  1.1898e-01,
         -9.5983e-02,  8.2140e-03, -6.8451e-02],
        [ 2.8156e-01,  1.3739e-01, -6.1542e-02,  2.8292e-01, -2.8104e-01,
         -2.4209e-01, -1.0580e-01,  1.6217e-01],
        [ 6.2036e-02, -1.6601e-01,  3.0131e-01,  4.3753e-03,  1.4107e-01,
         -4.9462e-02, -7.9806e-02, -7.8778e-02],
        [-2.6553e-01,  1.8219e-01, -9.5013e-02,  1.9875e-01, -2.0308e-01,
         -2.6744e-01, -2.0147e-01,  7.1741e-02],
        [ 7.2921e-02, -2.1289e-01,  2.5552e-01,  2.3418e-01, -1.0910e-01,
          3.4285e-01, -1.2665e-01,  3.2056e-01],
        [-3.1654e-01,  3.3169e-01,  3.1429e-01,  1.2596e-01,  2.4322e-01,
         -1.8081e-01,  2.9574e-01,  1.2725e-01],
        [ 1.7043e-01,  3.0750e-01, -3.0620e-01,  1.3002e-01,  6.0995e-02,
          2.6609e-01,  7.7173e-02, -3.2282e-02],
        [ 2.6525e-01,  1.9476e-01,  9.2001e-02, -1.1741e-01,  2.4485e-01,
         -1.5694e-01,  6.7555e-03,  3.4314e-01],
        [ 1.6087e-01,  5.7359e-02,  3.5226e-01, -1.0159e-01,  2.3113e-01,
         -1.3617e-01,  2.5697e-01,  1.9590e-01],
        [-7.2001e-02, -3.2079e-01,  2.5945e-01, -3.5074e-01,  1.5812e-02,
         -2.2457e-02, -2.9487e-01,  2.3664e-02],
        [-2.5672e-01, -1.1391e-01,  3.3963e-01,  4.3389e-02,  1.3653e-01,
         -2.0169e-01,  1.5634e-01,  3.1987e-01],
        [-9.0923e-03, -1.6316e-01,  1.6227e-01,  1.3353e-01, -3.4154e-01,
         -1.6237e-01, -7.3346e-02, -1.8967e-01],
        [-3.3385e-01,  2.7694e-02, -1.3628e-01,  5.9999e-02, -1.0999e-01,
          2.8102e-01, -2.7663e-02,  8.2983e-02],
        [-2.7718e-01,  5.1514e-03,  6.0863e-02, -8.9118e-02,  3.1636e-01,
          2.6513e-01,  8.4391e-03, -2.0412e-01],
        [ 2.0098e-01, -2.2858e-01,  3.0387e-01,  1.5550e-01, -3.1816e-01,
          9.2722e-02,  4.1949e-02, -3.4723e-01],
        [-3.2443e-01, -2.5589e-01,  5.5145e-02, -2.3459e-01,  3.4269e-01,
          2.3206e-01,  3.0201e-01,  1.4171e-01],
        [ 9.1992e-02, -1.7379e-02, -8.2138e-03, -1.9951e-01,  2.1091e-01,
          1.5504e-01, -1.3180e-01,  2.1507e-01]], device='cuda:0',
       requires_grad=True) 

model.module_15.bias torch.Size([100])
Parameter containing:
tensor([ 0.0875,  0.2373, -0.1326, -0.2202,  0.2343,  0.0471, -0.2821,  0.0377,
        -0.0294, -0.0393, -0.0905, -0.3075, -0.1574, -0.1349,  0.0742, -0.1496,
         0.2033, -0.2854, -0.1601,  0.0911,  0.2534, -0.2178,  0.2900,  0.3356,
         0.1447,  0.1801, -0.0836,  0.0230,  0.2969,  0.0116, -0.0502,  0.2228,
         0.0322,  0.0423,  0.3464, -0.1513,  0.1718,  0.2476,  0.0320, -0.0508,
         0.1923,  0.1300,  0.0496,  0.0659,  0.2350,  0.2577, -0.1307,  0.0506,
         0.3000,  0.1716, -0.2132,  0.2965, -0.2698, -0.1324,  0.0518, -0.1492,
         0.1294,  0.0266,  0.3270, -0.0470,  0.2820,  0.0576, -0.2980,  0.1886,
         0.2918, -0.3408,  0.0635, -0.1035,  0.2124,  0.1547,  0.1739,  0.2975,
         0.3095, -0.2029,  0.2526, -0.0796,  0.3344, -0.2665, -0.0073, -0.2258,
        -0.3473, -0.3503, -0.2319, -0.0713,  0.2322, -0.3523,  0.1768,  0.2084,
        -0.1145,  0.0267, -0.1267, -0.0814, -0.1482,  0.2152, -0.2049,  0.2619,
         0.1245,  0.2967,  0.0154,  0.2630], device='cuda:0',
       requires_grad=True) 

model.module_17.weight torch.Size([100, 100])
Parameter containing:
tensor([[-0.0746, -0.0143, -0.0211,  ...,  0.0327, -0.0366, -0.0722],
        [-0.0204,  0.0817,  0.0515,  ...,  0.0344,  0.0875,  0.0474],
        [ 0.0837,  0.0965, -0.0289,  ..., -0.0498, -0.0013,  0.0671],
        ...,
        [-0.0952,  0.0597,  0.0692,  ..., -0.0660, -0.0511,  0.0487],
        [ 0.0079, -0.0298, -0.0798,  ...,  0.0604,  0.0705,  0.0737],
        [ 0.0926, -0.0048,  0.0914,  ..., -0.0897,  0.0985, -0.0827]],
       device='cuda:0', requires_grad=True) 

model.module_17.bias torch.Size([100])
Parameter containing:
tensor([-0.0954,  0.0801, -0.0888,  0.0759, -0.0840, -0.0776, -0.0860,  0.0966,
        -0.0047,  0.0424,  0.0556,  0.0610,  0.0738,  0.0866,  0.0550,  0.0259,
         0.0558, -0.0432,  0.0711, -0.0673,  0.0945, -0.0926,  0.0177,  0.0499,
         0.0232, -0.0752, -0.0894, -0.0867,  0.0361,  0.0915, -0.0984,  0.0869,
         0.0742,  0.0598, -0.0775, -0.0323,  0.0301, -0.0311,  0.0374,  0.0587,
         0.0855, -0.0121,  0.0759,  0.0861,  0.0494, -0.0827, -0.0463, -0.0775,
        -0.0404,  0.0073,  0.0432, -0.0613, -0.0822, -0.0792,  0.0805,  0.0153,
        -0.0332,  0.0428, -0.0625,  0.0030, -0.0931,  0.0063, -0.0671, -0.0201,
        -0.0640,  0.0657,  0.0379, -0.0136,  0.0973, -0.0601,  0.0123, -0.0198,
         0.0053,  0.0020, -0.0247,  0.0219,  0.0967,  0.0369, -0.0419,  0.0244,
         0.0435, -0.0209, -0.0437, -0.0738,  0.0248, -0.0599,  0.0316, -0.0919,
         0.0034,  0.0488, -0.0759,  0.0838,  0.0606,  0.0946, -0.0258,  0.0733,
         0.0465, -0.0065,  0.0502, -0.0911], device='cuda:0',
       requires_grad=True) 

model.module_19.weight torch.Size([16, 100])
Parameter containing:
tensor([[ 0.0867, -0.0256,  0.0982,  ..., -0.0692,  0.0983,  0.0032],
        [ 0.0980, -0.0661, -0.0175,  ..., -0.0317,  0.0464,  0.0097],
        [ 0.0679,  0.0307, -0.0378,  ..., -0.0710, -0.0118, -0.0471],
        ...,
        [ 0.0986,  0.0449, -0.0811,  ...,  0.0153, -0.0240, -0.0868],
        [-0.0796,  0.0387,  0.0612,  ..., -0.0523, -0.0002,  0.0027],
        [-0.0323,  0.0649,  0.0532,  ...,  0.0859, -0.0617,  0.0608]],
       device='cuda:0', requires_grad=True) 

model.module_19.bias torch.Size([16])
Parameter containing:
tensor([ 0.0059,  0.0236,  0.0295, -0.0390,  0.0246, -0.0829,  0.0784, -0.0579,
         0.0283, -0.0061, -0.0233,  0.0680,  0.0229,  0.0967,  0.0407, -0.0391],
       device='cuda:0', requires_grad=True) 



