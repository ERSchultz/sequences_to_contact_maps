#### ARCHITECTURE ####
Node Encoder:
 Sequential(
  (0): Linear(1, 1000, bias=True)
  (1): PReLU(num_parameters=1)
  (2): Linear(1000, 1000, bias=True)
  (3): PReLU(num_parameters=1)
  (4): Linear(1000, 64, bias=True)
  (5): PReLU(num_parameters=1)
) 

Edge Encoder:
 None 

Linear:
 Linear(in_features=128, out_features=64, bias=True) 

Model:
 Sequential(
  (0): WeightedGATv2Conv(64, 8, heads=8)
  (1): MLP(
  (model): Sequential(
    (0): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=64, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (1): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (2): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=64, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
  )
)
  (2): WeightedGATv2Conv(64, 8, heads=8)
  (3): MLP(
  (model): Sequential(
    (0): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=64, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (1): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (2): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=64, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
  )
)
  (4): WeightedGATv2Conv(64, 8, heads=8)
  (5): MLP(
  (model): Sequential(
    (0): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=64, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (1): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (2): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=64, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
  )
)
  (6): WeightedGATv2Conv(64, 8, heads=8)
  (7): MLP(
  (model): Sequential(
    (0): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=64, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (1): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (2): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=64, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
  )
)
) 

Head 1:
 Bilinear 

Head 2:
 Sequential(
  (0): MLP(
    (model): Sequential(
      (0): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=32768, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (1): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (2): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (3): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (4): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (5): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (6): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1024, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
    )
  )
  (1): FillDiagonalsFromArray()
) 

#### ARCHITECTURE ####
Sign Net:
 SignNet(
  (phi): GNN3d(
    (convs): ModuleList(
      (0): MaskedGINConv(
        (nn): MaskedMLP(
          (layers): ModuleList(
            (0): Linear(in_features=1, out_features=64, bias=False)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (norms): ModuleList(
            (0): MaskedBN(
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): MaskedBN(
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
        (layer): GINConv(nn=Identity())
      )
      (1): MaskedGINConv(
        (nn): MaskedMLP(
          (layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=False)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (norms): ModuleList(
            (0): MaskedBN(
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): MaskedBN(
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
        (layer): GINConv(nn=Identity())
      )
      (2): MaskedGINConv(
        (nn): MaskedMLP(
          (layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=False)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (norms): ModuleList(
            (0): MaskedBN(
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): MaskedBN(
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
        (layer): GINConv(nn=Identity())
      )
      (3): MaskedGINConv(
        (nn): MaskedMLP(
          (layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=False)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (norms): ModuleList(
            (0): MaskedBN(
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): MaskedBN(
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
        (layer): GINConv(nn=Identity())
      )
      (4): MaskedGINConv(
        (nn): MaskedMLP(
          (layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=False)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (norms): ModuleList(
            (0): MaskedBN(
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): MaskedBN(
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
        (layer): GINConv(nn=Identity())
      )
      (5): MaskedGINConv(
        (nn): MaskedMLP(
          (layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=False)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (norms): ModuleList(
            (0): MaskedBN(
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): MaskedBN(
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
        (layer): GINConv(nn=Identity())
      )
      (6): MaskedGINConv(
        (nn): MaskedMLP(
          (layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=False)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (norms): ModuleList(
            (0): MaskedBN(
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): MaskedBN(
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
        (layer): GINConv(nn=Identity())
      )
      (7): MaskedGINConv(
        (nn): MaskedMLP(
          (layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=False)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (norms): ModuleList(
            (0): MaskedBN(
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): MaskedBN(
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
        (layer): GINConv(nn=Identity())
      )
    )
    (norms): ModuleList(
      (0): MaskedBN(
        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): MaskedBN(
        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): MaskedBN(
        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): MaskedBN(
        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): MaskedBN(
        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (5): MaskedBN(
        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (6): MaskedBN(
        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (7): MaskedBN(
        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (rho): SetTransformer(
    (transformer_layers): ModuleList(
      (0): TransformerEncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=64, out_features=64, bias=False)
          (w_ks): Linear(in_features=64, out_features=64, bias=False)
          (w_vs): Linear(in_features=64, out_features=64, bias=False)
          (fc): Linear(in_features=64, out_features=64, bias=False)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0, inplace=False)
          (norm): MaskedLN(
            (ln): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          )
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=64, out_features=64, bias=True)
          (w_2): Linear(in_features=64, out_features=64, bias=True)
          (norm): MaskedLN(
            (ln): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          )
          (dropout): Dropout(p=0, inplace=False)
        )
      )
      (1): TransformerEncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=64, out_features=64, bias=False)
          (w_ks): Linear(in_features=64, out_features=64, bias=False)
          (w_vs): Linear(in_features=64, out_features=64, bias=False)
          (fc): Linear(in_features=64, out_features=64, bias=False)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0, inplace=False)
          (norm): MaskedLN(
            (ln): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          )
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=64, out_features=64, bias=True)
          (w_2): Linear(in_features=64, out_features=64, bias=True)
          (norm): MaskedLN(
            (ln): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          )
          (dropout): Dropout(p=0, inplace=False)
        )
      )
      (2): TransformerEncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=64, out_features=64, bias=False)
          (w_ks): Linear(in_features=64, out_features=64, bias=False)
          (w_vs): Linear(in_features=64, out_features=64, bias=False)
          (fc): Linear(in_features=64, out_features=64, bias=False)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0, inplace=False)
          (norm): MaskedLN(
            (ln): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          )
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=64, out_features=64, bias=True)
          (w_2): Linear(in_features=64, out_features=64, bias=True)
          (norm): MaskedLN(
            (ln): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          )
          (dropout): Dropout(p=0, inplace=False)
        )
      )
      (3): TransformerEncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=64, out_features=64, bias=False)
          (w_ks): Linear(in_features=64, out_features=64, bias=False)
          (w_vs): Linear(in_features=64, out_features=64, bias=False)
          (fc): Linear(in_features=64, out_features=64, bias=False)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0, inplace=False)
          (norm): MaskedLN(
            (ln): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          )
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=64, out_features=64, bias=True)
          (w_2): Linear(in_features=64, out_features=64, bias=True)
          (norm): MaskedLN(
            (ln): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          )
          (dropout): Dropout(p=0, inplace=False)
        )
      )
      (4): TransformerEncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=64, out_features=64, bias=False)
          (w_ks): Linear(in_features=64, out_features=64, bias=False)
          (w_vs): Linear(in_features=64, out_features=64, bias=False)
          (fc): Linear(in_features=64, out_features=64, bias=False)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0, inplace=False)
          (norm): MaskedLN(
            (ln): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          )
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=64, out_features=64, bias=True)
          (w_2): Linear(in_features=64, out_features=64, bias=True)
          (norm): MaskedLN(
            (ln): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          )
          (dropout): Dropout(p=0, inplace=False)
        )
      )
      (5): TransformerEncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=64, out_features=64, bias=False)
          (w_ks): Linear(in_features=64, out_features=64, bias=False)
          (w_vs): Linear(in_features=64, out_features=64, bias=False)
          (fc): Linear(in_features=64, out_features=64, bias=False)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0, inplace=False)
          (norm): MaskedLN(
            (ln): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          )
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=64, out_features=64, bias=True)
          (w_2): Linear(in_features=64, out_features=64, bias=True)
          (norm): MaskedLN(
            (ln): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          )
          (dropout): Dropout(p=0, inplace=False)
        )
      )
      (6): TransformerEncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=64, out_features=64, bias=False)
          (w_ks): Linear(in_features=64, out_features=64, bias=False)
          (w_vs): Linear(in_features=64, out_features=64, bias=False)
          (fc): Linear(in_features=64, out_features=64, bias=False)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0, inplace=False)
          (norm): MaskedLN(
            (ln): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          )
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=64, out_features=64, bias=True)
          (w_2): Linear(in_features=64, out_features=64, bias=True)
          (norm): MaskedLN(
            (ln): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          )
          (dropout): Dropout(p=0, inplace=False)
        )
      )
      (7): TransformerEncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=64, out_features=64, bias=False)
          (w_ks): Linear(in_features=64, out_features=64, bias=False)
          (w_vs): Linear(in_features=64, out_features=64, bias=False)
          (fc): Linear(in_features=64, out_features=64, bias=False)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0, inplace=False)
          (norm): MaskedLN(
            (ln): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          )
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=64, out_features=64, bias=True)
          (w_2): Linear(in_features=64, out_features=64, bias=True)
          (norm): MaskedLN(
            (ln): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          )
          (dropout): Dropout(p=0, inplace=False)
        )
      )
    )
    (out): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=False)
      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (eigen_encoder): MaskedMLP(
    (layers): ModuleList(
      (0): Linear(in_features=1, out_features=1, bias=False)
      (1): Linear(in_features=1, out_features=64, bias=False)
    )
    (norms): ModuleList(
      (0): MaskedBN(
        (bn): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): MaskedBN(
        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
) 

Namespace(GNN_mode=True, transforms=[], pre_transforms=['constant', 'ContactDistance', 'GeneticDistance_norm', 'AdjPCs_8'], sparsify_threshold=None, sparsify_threshold_upper=None, top_k=None, use_node_features=False, use_edge_weights=False, use_edge_attr=True, keep_zero_edges=False, data_folder=['/project/depablo/erschultz/dataset_11_18_22', '/project/depablo/erschultz/dataset_11_21_22'], scratch='/scratch/midway2/erschultz', root_name='ContactGNNEnergy1', delete_root=False, toxx=False, toxx_mode='mean', y_preprocessing='sweeprand_log_inf', y_zero_diag_count=0, log_preprocessing=None, output_preprocesing=None, kr=False, mean_filt=None, rescale=2, gated=False, preprocessing_norm='mean', min_subtraction=True, x_reshape=True, ydtype=torch.float32, y_reshape=True, crop=None, classes=10, use_scratch=False, use_scratch_parallel=True, split_percents=[0.9, 0.1, 0.0], split_sizes=None, random_split=True, shuffle=True, batch_size=2, num_workers=4, start_epoch=1, n_epochs=100, save_mod=5, print_mod=2, lr=1e-05, weight_decay=0.0, gpus=1, milestones=[50], gamma=0.1, loss='mse', w_reg=None, reg_lambda=0.1, autoencoder_mode=False, verbose=False, print_params=True, output_mode='energy_sym_diag', model_type='ContactGNNEnergy', id=337, pretrained=False, resume_training=False, k=8, m=1024, seed=42, act='prelu', inner_act='prelu', out_act='prelu', training_norm=None, dropout=0.0, parameter_sharing=False, use_bias=True, use_sign_net=True, message_passing='weighted_GAT', head_architecture='bilinear_triu', head_architecture_2='fc-fill', head_hidden_sizes_list=[1000, 1000, 1000, 1000, 1000, 1000, 1024], encoder_hidden_sizes_list=[1000, 1000, 64], inner_hidden_sizes_list=None, edge_encoder_hidden_sizes_list=None, update_hidden_sizes_list=[1000, 1000, 64], head_act='prelu', num_heads=8, concat_heads=True, max_diagonal=None, mlp_model_id=None, kernel_w_list=None, hidden_sizes_list=[8, 8, 8, 8], nf=None, dilation_list=None, dilation_list_trunk=None, bottleneck=None, dilation_list_head=None, down_sampling=None, plot=True, plot_predictions=True, ofile_folder='/home/erschultz/sequences_to_contact_maps/results/ContactGNNEnergy/337', log_file_path='/home/erschultz/sequences_to_contact_maps/results/ContactGNNEnergy/337/out.log', log_file=<_io.TextIOWrapper name='/home/erschultz/sequences_to_contact_maps/results/ContactGNNEnergy/337/out.log' mode='a' encoding='UTF-8'>, param_file=<_io.TextIOWrapper name='/home/erschultz/sequences_to_contact_maps/results/ContactGNNEnergy/337/params.log' mode='a' encoding='UTF-8'>, split_neg_pos_edges=False, criterion=<function mse_loss at 0x7ff5b17b6280>, channels=1, node_feature_size=1, input_m=512, edge_transforms=["'ContactDistance'", 'ContactDistance(norm=False)', 'GeneticDistance(norm=True)'], node_transforms=['AdjPCs(k=8, normalize=False, sign_net=True)', 'Constant(value=1.0)'], edge_dim=2, transforms_processed=None, diag=True, pre_transforms_processed=Compose([
  Constant(value=1.0),
  ContactDistance(norm=False),
  GeneticDistance(norm=True),
  AdjPCs(k=8, normalize=False, sign_net=True)
]), cuda=True, use_parallel=False, device=device(type='cuda'))

Dataset construction time: 46.545 minutes
Average num edges per graph:  222294.36527196653
Mean degree: [362.66 512.   449.47 ... 329.12 511.93 511.26] +- [70.54  0.   53.89 ... 80.99  0.29  1.69]

split sizes: train=4302, val=478, test=0, N=4780
Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0.0
)
#### TRAINING/VALIDATION ####
Epoch 2, loss = 12.0394
Mean test/val loss: 12.2118
[25, 50, 75] quantiles test/val loss: [ 8.1118 10.5534 12.9328]

Epoch 4, loss = 11.3297
Mean test/val loss: 11.9230
[25, 50, 75] quantiles test/val loss: [ 8.1126 10.1568 12.814 ]

Epoch 6, loss = 11.0215
Mean test/val loss: 12.5652
[25, 50, 75] quantiles test/val loss: [ 9.287  11.5084 13.7028]

Epoch 8, loss = 10.6500
Mean test/val loss: 11.1956
[25, 50, 75] quantiles test/val loss: [ 7.9952 10.0468 12.6154]

Epoch 10, loss = 10.3206
Mean test/val loss: 10.7994
[25, 50, 75] quantiles test/val loss: [ 7.4723  9.8445 12.8918]

Epoch 12, loss = 10.1558
Mean test/val loss: 11.6538
[25, 50, 75] quantiles test/val loss: [ 8.6375 10.6392 13.252 ]

Epoch 14, loss = 9.8589
Mean test/val loss: 10.6743
[25, 50, 75] quantiles test/val loss: [ 7.4184  9.1098 11.8738]

Epoch 16, loss = 9.3889
Mean test/val loss: 9.7013
[25, 50, 75] quantiles test/val loss: [ 6.523   8.5694 11.0595]

Epoch 18, loss = 9.0990
Mean test/val loss: 11.4022
[25, 50, 75] quantiles test/val loss: [ 7.7078  9.7764 12.7928]

Epoch 20, loss = 8.9240
Mean test/val loss: 9.8019
[25, 50, 75] quantiles test/val loss: [ 6.6981  9.0576 11.3881]

Epoch 22, loss = 8.8171
Mean test/val loss: 9.2199
[25, 50, 75] quantiles test/val loss: [ 6.2301  8.3552 10.4284]

Epoch 24, loss = 8.6989
Mean test/val loss: 8.9039
[25, 50, 75] quantiles test/val loss: [ 6.2311  8.0747 10.2893]

Epoch 26, loss = 8.5341
Mean test/val loss: 10.2580
[25, 50, 75] quantiles test/val loss: [ 7.9016  9.4336 11.0382]

Epoch 28, loss = 8.4874
Mean test/val loss: 9.4360
[25, 50, 75] quantiles test/val loss: [ 6.2528  8.0572 10.1339]

Epoch 30, loss = 8.4256
Mean test/val loss: 9.3381
[25, 50, 75] quantiles test/val loss: [6.1542 8.1312 9.9264]

Epoch 32, loss = 8.3123
Mean test/val loss: 8.7818
[25, 50, 75] quantiles test/val loss: [6.2136 7.7653 9.6436]

Epoch 34, loss = 8.2582
Mean test/val loss: 9.7508
[25, 50, 75] quantiles test/val loss: [ 6.9032  8.5681 10.8278]

Epoch 36, loss = 8.2007
Mean test/val loss: 9.2391
[25, 50, 75] quantiles test/val loss: [6.3117 7.884  9.8691]

Epoch 38, loss = 8.0875
Mean test/val loss: 9.3974
[25, 50, 75] quantiles test/val loss: [ 6.8411  8.338  10.0131]

Epoch 40, loss = 8.0303
Mean test/val loss: 9.2667
[25, 50, 75] quantiles test/val loss: [6.4127 7.8226 9.56  ]

Epoch 42, loss = 7.9654
Mean test/val loss: 8.3367
[25, 50, 75] quantiles test/val loss: [5.8458 7.2492 9.5551]

Epoch 44, loss = 7.9809
Mean test/val loss: 8.6371
[25, 50, 75] quantiles test/val loss: [5.9742 7.3203 9.01  ]

Epoch 46, loss = 7.8224
Mean test/val loss: 8.2977
[25, 50, 75] quantiles test/val loss: [5.7098 7.2706 9.0058]

Epoch 48, loss = 7.6753
Mean test/val loss: 8.7445
[25, 50, 75] quantiles test/val loss: [5.7576 7.3913 9.4506]

Epoch 50, loss = 7.7074
Mean test/val loss: 8.1978
[25, 50, 75] quantiles test/val loss: [5.644  7.2178 8.9308]

Epoch 52, loss = 7.3131
Mean test/val loss: 8.2117
[25, 50, 75] quantiles test/val loss: [5.2871 7.1294 9.1686]

Epoch 54, loss = 7.3008
Mean test/val loss: 8.1153
[25, 50, 75] quantiles test/val loss: [5.3771 7.0282 9.0289]

Epoch 56, loss = 7.2899
Mean test/val loss: 8.3058
[25, 50, 75] quantiles test/val loss: [5.5492 7.0093 8.6772]

Epoch 58, loss = 7.2727
Mean test/val loss: 8.3170
[25, 50, 75] quantiles test/val loss: [5.3927 7.2443 9.0191]

Epoch 60, loss = 7.2635
Mean test/val loss: 8.4475
[25, 50, 75] quantiles test/val loss: [5.726  7.1912 8.7352]

Epoch 62, loss = 7.2601
Mean test/val loss: 8.1717
[25, 50, 75] quantiles test/val loss: [5.3852 7.1651 8.8052]

Epoch 64, loss = 7.2440
Mean test/val loss: 8.2950
[25, 50, 75] quantiles test/val loss: [5.8143 7.2398 8.9071]

Epoch 66, loss = 7.2357
Mean test/val loss: 8.3164
[25, 50, 75] quantiles test/val loss: [5.3734 7.1335 8.9663]

Epoch 68, loss = 7.2238
Mean test/val loss: 8.2657
[25, 50, 75] quantiles test/val loss: [5.3385 7.0231 8.7927]

Epoch 70, loss = 7.2189
Mean test/val loss: 8.2329
[25, 50, 75] quantiles test/val loss: [5.4482 7.2069 8.8766]

Epoch 72, loss = 7.2116
Mean test/val loss: 8.1974
[25, 50, 75] quantiles test/val loss: [5.1795 7.1431 8.8719]

Epoch 74, loss = 7.2020
Mean test/val loss: 8.2809
[25, 50, 75] quantiles test/val loss: [5.4469 7.0809 8.8331]

Epoch 76, loss = 7.1945
Mean test/val loss: 8.2140
[25, 50, 75] quantiles test/val loss: [5.1506 6.9849 8.9421]

Epoch 78, loss = 7.1886
Mean test/val loss: 8.3036
[25, 50, 75] quantiles test/val loss: [5.4695 7.1691 8.9876]

Epoch 80, loss = 7.1788
Mean test/val loss: 8.1004
[25, 50, 75] quantiles test/val loss: [5.4364 7.0153 8.692 ]

Epoch 82, loss = 7.1711
Mean test/val loss: 8.2759
[25, 50, 75] quantiles test/val loss: [5.3721 6.9875 8.6613]

Epoch 84, loss = 7.1702
Mean test/val loss: 8.1561
[25, 50, 75] quantiles test/val loss: [5.2462 7.0742 8.9265]

Epoch 86, loss = 7.1621
Mean test/val loss: 8.0554
[25, 50, 75] quantiles test/val loss: [5.2373 6.8941 9.0298]

Epoch 88, loss = 7.1577
Mean test/val loss: 8.1456
[25, 50, 75] quantiles test/val loss: [5.1837 7.1102 9.3238]

Epoch 90, loss = 7.1437
Mean test/val loss: 8.2159
[25, 50, 75] quantiles test/val loss: [5.3925 6.8379 8.7655]

Epoch 92, loss = 7.1445
Mean test/val loss: 8.2217
[25, 50, 75] quantiles test/val loss: [5.4022 6.9684 8.8076]

Epoch 94, loss = 7.1360
Mean test/val loss: 8.0019
[25, 50, 75] quantiles test/val loss: [5.1793 6.9469 8.6676]

Epoch 96, loss = 7.1314
Mean test/val loss: 8.1595
[25, 50, 75] quantiles test/val loss: [5.3207 6.9398 8.946 ]

Epoch 98, loss = 7.1375
Mean test/val loss: 8.0602
[25, 50, 75] quantiles test/val loss: [5.2324 6.9463 8.8499]

Epoch 100, loss = 7.1226
Mean test/val loss: 7.9930
[25, 50, 75] quantiles test/val loss: [5.3617 6.9715 8.6525]


Total parameters: 44700183
Total training + validation time: 38.0 hours, 56.0 mins, and 51.89999999999418 secs
Final val loss: 7.992993945357191

split sizes: train=4302, val=478, test=0, N=4780
#### Plotting Script ####
Prediction Results:
dataset_11_21_22 sample939: 9.85620403289795
dataset_11_18_22 sample203: 7.802265644073486
dataset_11_21_22 sample743: 9.596567153930664
dataset_11_21_22 sample45: 3.3351423740386963
dataset_11_18_22 sample559: 4.423516750335693
Loss: 7.003 +- 2.669

Downsampling (40%) Results:
dataset_11_18_22 sample203-downsampling: 8.191811561584473
dataset_11_18_22 sample45-downsampling: 20.985807418823242
dataset_11_18_22 sample559-downsampling: 4.423516273498535
dataset_11_18_22 sample743-downsampling: 12.815378189086914
dataset_11_18_22 sample939-downsampling: 5.313344478607178
Loss: 7.951 +- 5.581

Removing /project/depablo/erschultz/dataset_11_18_22/ContactGNNEnergy1downsample
Original sampling (100%) Results:
dataset_11_18_22 sample203-regular: 7.651617050170898
dataset_11_18_22 sample45-regular: 19.741558074951172
dataset_11_18_22 sample559-regular: 4.418680191040039
dataset_11_18_22 sample743-regular: 12.256841659545898
dataset_11_18_22 sample939-regular: 4.946961402893066
Loss: 7.561 +- 5.312

Removing /project/depablo/erschultz/dataset_11_18_22/ContactGNNEnergy1regsample
Upsampling (200%) Results:
dataset_11_18_22 sample203-upsampling: 8.088245391845703
dataset_11_18_22 sample45-upsampling: 20.344127655029297
dataset_11_18_22 sample559-upsampling: 4.615455150604248
dataset_11_18_22 sample743-upsampling: 12.187749862670898
dataset_11_18_22 sample939-upsampling: 4.869392395019531
Loss: 7.643 +- 5.449

Removing /project/depablo/erschultz/dataset_11_18_22/ContactGNNEnergy1upsample
