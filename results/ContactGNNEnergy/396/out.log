#### ARCHITECTURE ####
Node Encoder:
 Sequential(
  (0): Linear(1, 64, bias=True)
  (1): PReLU(num_parameters=1)
) 

Edge Encoder:
 None 

Linear:
 Linear(in_features=72, out_features=64, bias=True) 

Model:
 Sequential(
  (0): WeightedGATv2Conv(64, 8, heads=8)
  (1): MLP(
  (model): Sequential(
    (0): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=64, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (1): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (2): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=64, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
  )
)
  (2): WeightedGATv2Conv(64, 8, heads=8)
  (3): MLP(
  (model): Sequential(
    (0): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=64, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (1): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (2): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=64, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
  )
)
  (4): WeightedGATv2Conv(64, 8, heads=8)
  (5): MLP(
  (model): Sequential(
    (0): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=64, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (1): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (2): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=64, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
  )
)
  (6): WeightedGATv2Conv(64, 8, heads=8)
  (7): MLP(
  (model): Sequential(
    (0): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=64, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (1): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (2): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=64, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
  )
)
) 

Head L:
 None 
 Bilinear 

Head D:
 None 
 Sequential(
  (0): MLP(
    (model): Sequential(
      (0): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=16384, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (1): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (2): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (3): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (4): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (5): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (6): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=512, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
    )
  )
  (1): FillDiagonalsFromArray()
) 

Namespace(GNN_mode=True, transforms=[], pre_transforms=['constant', 'ContactDistance', 'GeneticDistance_norm', 'AdjPCs_8'], sparsify_threshold=None, sparsify_threshold_upper=None, top_k=None, use_node_features=False, use_edge_weights=False, use_edge_attr=True, keep_zero_edges=False, data_folder=['/project2/depablo/erschultz/dataset_03_21_23_max_ent'], scratch='/scratch/midway3/erschultz', root_name='ContactGNNEnergy4', delete_root=False, toxx=False, toxx_mode='mean', y_preprocessing='log_inf', y_zero_diag_count=0, log_preprocessing=None, output_preprocesing='log', kr=False, mean_filt=None, rescale=2, gated=False, preprocessing_norm='mean', min_subtraction=True, x_reshape=True, ydtype=torch.float32, y_reshape=True, crop=None, classes=10, move_data_to_scratch=False, use_scratch_parallel=False, plaid_score_cutoff=None, split_percents=[0.9, 0.1, 0.0], split_sizes=None, random_split=True, shuffle=True, batch_size=1, num_workers=4, start_epoch=1, n_epochs=40, save_mod=5, print_mod=2, lr=0.0001, min_lr=1e-06, weight_decay=0.0, gpus=1, scheduler='MultiStepLR', milestones=[25], gamma=0.1, patience=10, loss='mse', w_reg=None, reg_lambda=0.1, autoencoder_mode=False, verbose=False, print_params=True, output_mode='energy_sym_diag', model_type='ContactGNNEnergy', id=396, pretrain_id=394, resume_training=False, k=8, m=512, seed=42, act='prelu', inner_act='prelu', out_act='prelu', training_norm=None, dropout=0.0, parameter_sharing=False, use_bias=True, use_sign_net=False, use_sign_plus=True, message_passing='weighted_GAT', head_architecture='bilinear', head_architecture_2='fc-fill_512', head_hidden_sizes_list=[1000, 1000, 1000, 1000, 1000, 1000], encoder_hidden_sizes_list=[64], inner_hidden_sizes_list=None, edge_encoder_hidden_sizes_list=None, update_hidden_sizes_list=[1000, 1000, 64], head_act='prelu', num_heads=8, concat_heads=True, max_diagonal=None, mlp_model_id=None, kernel_w_list=None, hidden_sizes_list=[8, 8, 8, 8], nf=None, dilation_list=None, dilation_list_trunk=None, bottleneck=None, dilation_list_head=None, down_sampling=None, plot=True, plot_predictions=True, ofile_folder='/home/erschultz/sequences_to_contact_maps/results/ContactGNNEnergy/396', log_file_path='/home/erschultz/sequences_to_contact_maps/results/ContactGNNEnergy/396/out.log', log_file=<_io.TextIOWrapper name='/home/erschultz/sequences_to_contact_maps/results/ContactGNNEnergy/396/out.log' mode='a' encoding='UTF-8'>, param_file=<_io.TextIOWrapper name='/home/erschultz/sequences_to_contact_maps/results/ContactGNNEnergy/396/params.log' mode='a' encoding='UTF-8'>, split_neg_pos_edges=False, criterion=<function mse_loss at 0x7fb142a323a0>, channels=1, node_feature_size=1, input_m=256, edge_transforms=["'ContactDistance'", 'ContactDistance(norm=False)', 'GeneticDistance(norm=True)'], node_transforms=['AdjPCs(k=8, normalize=False, sign_net=True)', 'Constant(value=1.0)'], edge_dim=2, transforms_processed=None, diag=True, pre_transforms_processed=Compose([
  Constant(value=1.0),
  ContactDistance(norm=False),
  GeneticDistance(norm=True),
  AdjPCs(k=8, normalize=False, sign_net=True)
]), cuda=True, use_parallel=False, device=device(type='cuda'))

Dataset construction time: 2.011 minutes
Number of samples: 713
Average num edges per graph:  55984.23562412342
Mean degree: [255.16 253.27 254.95 255.48 254.77 253.63 254.03 253.68 250.85 253.49
 255.37 254.14 252.5  254.71 253.77 254.76 253.6  255.52 255.8  252.97
 254.12 255.64 252.95 255.61 254.58 254.34 254.4  254.14 254.48 254.98
 251.56 253.48 254.85 255.1  255.21 251.44 254.43 251.87 254.48 255.45
 254.4  254.2  255.6  255.34 253.37 254.41 251.25 253.11 255.86 254.64
 251.67 254.68 255.73 252.98 252.35 255.57 255.04 255.29 252.3  255.05
 255.28 251.77 252.77 255.64 253.94 249.36 255.1  254.52 253.34 253.47
 253.32 254.76 246.72  42.36 142.23 246.59 246.03 241.2  241.23  73.59
 253.08 231.23 237.66  60.04  41.23 241.89  53.71 208.29  43.66  67.88
 243.33  53.12 227.99 120.75  23.88  68.95 246.78 151.17  46.91 127.52
  36.84 114.78 137.15 236.9  245.05 166.99 231.    47.45 246.38  94.05
  99.78 249.23  19.84 197.77 179.94 247.12 247.75  60.98 230.83 249.48
 232.71 248.81 253.94  31.93  50.12 125.14 103.75  68.45  40.39  94.4
  50.86  53.7   79.87  50.12  80.03  88.69  28.06  49.55  39.07  97.02
  87.94  30.99  42.26 117.46  66.52  80.08  85.34  41.98  84.54  35.98
  54.05  29.59  24.16  29.18  75.28  41.77 153.61  86.41  61.37  34.3
 219.43 136.85  66.34  46.53  88.22 255.12 255.93 253.62 255.92 255.85
 253.12 251.2  255.09 252.99 252.73 255.   254.58 254.62 254.27 254.8
 255.41 255.52 249.47 254.48 255.62 252.5  255.58 254.83 255.62 255.33
 255.65 254.36 235.91 254.04 255.35 255.11 251.06 255.3  254.56 255.69
 255.02 250.2  254.88 255.08 252.41 255.91 253.2  255.44 254.63 255.07
 254.44 255.18 251.66 253.72 255.62 254.77 254.7  255.68 254.51 254.75
 253.58 255.4  255.15 255.18 126.6  252.3  254.59 254.05 254.34 254.96
 254.34 255.7  252.22 254.12 255.71 254.16 255.23 254.74 247.48 255.73
 249.28 248.38 250.71 253.93 253.56 239.09 250.89 245.86 249.7  229.42
 231.74 251.37 254.68 237.34 127.2  249.27 246.89 254.79 248.77 255.29
 250.09 254.76 255.57 253.2  254.72 144.27  32.92 253.14 247.8  252.92
 246.94 243.55 243.68 252.54 254.26  47.95 244.43 159.8  201.36 249.09
 247.62 250.73 246.67 252.35 197.8  248.31 250.83 240.51 245.83 247.41
 255.04 254.5  254.23 246.83 254.04 254.7   87.01 235.78 251.59  85.65
 245.56 249.57 241.59 192.49 247.02 232.86 249.17 254.81 252.66 241.95
 235.81  66.29 249.15 252.5  252.98 254.95 255.66 254.2  254.91 253.84
 253.07 255.26 253.38 254.3  255.2  253.21 253.52 255.34 254.59 253.69
 255.77 232.34 253.02 221.87 255.67 255.02 239.29 255.61 254.7  225.41
 234.11 255.8  255.3  250.66 255.27 252.65 255.78 253.97 254.14 254.26
 254.83 255.16 253.95 254.37 255.26 255.39 255.67 255.36 186.59 254.77
 198.2  255.14 223.66 254.75 253.62 251.62 253.2  255.71 255.48 255.6
 253.74 254.95 254.54 255.15 254.42 255.38 255.68 254.36 254.07 252.22
 254.84 253.81 249.59 255.66 251.86 254.44 252.21 254.83 256.   255.23
 255.02 255.02 253.29 254.3  253.38 255.05 251.8  253.82 254.08 254.5
 254.95 254.73 255.2  254.8  251.38 254.98 255.61 253.23 255.36 255.37
 255.88 255.6  255.23 255.57 254.14 255.47 254.87 255.8  254.4  254.15
 254.66 252.78 255.74 253.84 255.68 253.84 255.33 254.03 254.95 254.32
 255.55 255.84 255.12 255.17 251.64 255.12 255.25 248.8  252.8  254.88
 255.12 252.37 255.75 255.1  255.41 255.59 253.8  255.66 254.88 255.97
 255.39 254.77 252.52 254.02 253.78 254.38 255.51 255.66 253.6  253.9
 254.23 255.02 252.34 255.23 254.34 254.3   32.48 104.11  76.91  50.26
  45.63  30.75  36.03  50.05  41.73  97.97  42.98  63.12  55.03  49.75
  79.39  75.37  29.26  61.45  46.83  35.33  23.    53.61  24.25  37.05
  75.52  77.09  34.03  22.5   93.97  20.91  56.21 104.61  33.33  36.78
  56.95  80.79  61.65  61.1   83.58  26.88 241.84 243.73  66.01  72.26
 156.7   87.95 255.45 255.88 255.95 255.99 255.78 255.77 255.77 255.69
 255.7  255.91 255.8  255.86 255.95 255.96 255.97 255.58 256.   255.91
 255.73 255.71 256.   255.97 255.98 255.96 255.45 255.95 255.99 255.54
 255.99 255.88 255.98 255.98 255.54 255.93 255.48 255.59 255.97 255.82
 255.85 255.98 255.99 255.98 255.94 255.93 255.02 255.99 255.8  255.95
 254.91 255.98 255.74 255.34 255.96 255.97 255.98 255.98 255.98 255.98
 255.98 255.96 255.73 256.   256.   255.66 255.17 255.96 255.91 255.73
 255.88 255.84 254.15 254.6  255.12 253.98 255.8  251.73 250.44 242.54
 254.75 255.75 235.7  253.42 254.04  35.47 244.44 254.02 255.2  254.35
 251.35 239.75 254.3  158.   237.73 251.7   91.3  225.65 255.05 217.05
 253.2  250.05 224.76 254.8  254.23 250.23 249.61 243.27 254.98 254.48
 247.22  48.15 244.8  253.27  30.48 255.04 225.4  249.7  249.73 253.06
 251.01 247.42 252.57 254.28 253.15 254.34 253.17 250.34 252.89 250.14
 247.33 227.94 253.12 244.22 241.74 247.41 246.22 241.13 251.74 250.91
 234.7  241.84 246.21 254.25 254.15 249.46 252.91  92.87 209.84 250.93
 251.39 248.95 250.91 186.37 250.71 126.12 248.3  248.34 252.28 246.44
 249.29 231.16 253.52 254.3  245.78 252.34 252.96 251.6  253.02 251.11
 253.18 252.17 248.6  252.54 252.54 248.97 254.36 239.99 247.77 251.8
 252.43 247.99 252.56 250.25 252.77 244.28 254.83 250.26 253.34 239.98
 253.99 249.63 246.81 230.63 253.1  252.4  244.7  248.27 216.67 253.45
 249.61  55.57 251.79] +- [ 1.31  2.67  1.96  0.86  1.64  3.03  2.64  2.8   4.7   2.83  1.18  2.64
  3.42  1.88  2.49  1.65  2.24  0.98  0.51  3.51  3.12  0.7   5.36  0.83
  1.96  2.55  2.14  2.42  2.89  1.82  3.75  4.6   1.88  1.33  1.43  5.4
  2.15  5.44  2.42  1.33  2.19  2.2   1.04  1.07  3.22  2.91  4.11  3.52
  0.45  1.97  3.43  1.91  0.57  4.76  4.39  0.78  1.75  1.41  6.59  1.19
  1.32  3.77  2.91  0.8   2.7   4.31  1.76  1.94  2.97  2.9   2.69  2.44
  8.99 16.91 43.66 10.63  9.26 10.68 10.83 44.82  3.53 26.51 15.23 26.
 37.31 12.46 18.05 32.81 18.1  44.81  8.77 27.84 17.68 48.94 10.16 27.67
  7.42 38.94 29.79 44.76 14.85 55.86 69.88 13.9  10.31 34.18 18.38 21.28
  9.12 52.63 50.06  5.74  9.6  50.26 69.52 10.65 21.35 45.29 21.4   6.6
 19.9   7.11  4.98 19.61 29.52 60.32 49.49 41.68 26.26 52.97 31.69 35.96
 48.06 30.69 48.34 58.3  18.82 31.7  25.16 47.34 52.58 20.13 25.97 32.77
 37.22 55.11 47.17 31.49 52.47 21.38 34.55 17.74 13.29 22.06 38.22 25.37
 66.84 49.41 37.43 22.37 36.69 54.97 45.76 30.01 51.72  1.98  0.28  2.89
  0.33  0.39  4.42  4.03  1.87  4.3   3.56  1.39  2.1   2.14  3.14  1.64
  1.24  0.91  6.06  2.41  0.77  4.02  0.85  1.62  0.81  1.27  0.77  1.99
 17.52  2.51  1.23  1.36  3.56  1.31  2.03  0.72  1.62  7.82  1.75  1.66
  5.49  0.32  3.19  1.18  2.26  1.55  2.34  1.44  4.42  3.12  0.87  1.66
  1.68  0.73  2.44  1.84  2.91  1.32  1.41  1.35 58.35  4.56  1.95  3.22
  2.16  1.49  2.43  0.71  4.43  2.34  0.61  3.05  1.14  1.77  9.41  0.63
  4.59  7.01  6.49  2.93  2.87 13.25  4.58  8.6   5.91 34.24 39.12  4.63
  2.46 31.27 62.49  6.5  16.42  2.11  6.53  1.13  6.41  1.79  0.84  3.99
  2.18 43.34 17.65  3.23 15.13  3.83  6.42 19.19  9.12  4.61  2.52 21.34
 12.56 60.85 52.43 16.05 12.35  6.1  18.09  4.8  47.81  5.69  4.25 25.98
  8.82  6.89  1.46  1.89  2.17  7.18  3.3   1.89 56.45 18.19  4.94 50.28
  8.74  6.33 23.67 44.67  7.94 12.07  5.29  1.76  4.81 10.25 24.11 19.89
  6.22  4.21  3.32  1.49  0.67  2.02  1.25  2.67  4.54  1.08  2.72  1.9
  1.35  3.43  6.27  1.07  1.83  2.98  0.51 28.84  3.32 39.78  0.66  1.39
 25.01  0.79  1.5  44.64 32.4   0.53  1.15  9.8   1.18  3.64  0.49  2.3
  2.09  2.3   1.59  1.33  2.67  2.75  1.45  1.08  0.65  1.08 60.8   1.65
 41.51  1.66 39.64  1.76  3.03  7.87  2.58  0.59  0.82  0.76  1.97  1.71
  1.78  1.34  2.7   1.03  0.66  2.22  2.32  4.18  2.01  2.6   5.08  0.79
 10.73  1.88  4.33  1.57  0.    1.27  1.55  1.56  3.3   2.58  2.63  1.4
  3.99  2.78  2.03  1.79  1.68  1.6   1.25  1.72  4.47  1.57  0.74  3.37
  1.02  1.1   0.35  0.74  1.31  0.89  2.8   1.21  1.7   0.52  1.65  2.39
  1.84  3.44  0.55  3.46  0.67  2.5   1.2   2.47  1.89  2.05  0.87  0.46
  1.36  1.28  4.05  1.41  1.17  7.16  3.87  1.51  1.31  4.01  0.65  1.45
  1.04  0.94  2.31  0.72  1.77  0.17  1.11  2.07  3.72  2.64  2.4   2.02
  0.97  0.67  2.53  2.98  2.93  1.71  4.08  1.34  1.83  7.19 25.99 62.47
 53.66 41.54 36.84 22.39 21.88 42.46 32.98 59.22 32.39 44.19 33.44 36.95
 55.36 46.28 19.06 45.23 27.56 27.89 17.54 37.64  9.62 25.29 50.49 44.08
 24.19 16.12 58.22 13.15 33.74 58.12 26.34 28.67 39.9  50.26 26.72 38.71
 50.63 10.19 30.12 12.03 44.59 50.23 70.12 55.06  1.37  0.4   0.23  0.09
  0.58  0.56  0.6   0.74  0.62  0.34  0.51  0.45  0.33  0.19  0.17  0.89
  0.    0.43  0.68  0.7   0.    0.17  0.12  0.19  1.22  0.24  0.09  0.77
  0.09  0.33  0.15  0.12  0.84  0.3   0.96  0.96  0.2   0.51  0.49  0.15
  0.09  0.12  0.27  0.27  1.56  0.09  0.59  0.24  1.69  0.12  0.65  1.17
  0.19  0.17  0.2   0.15  0.15  0.15  0.15  0.19  0.68  0.    0.    0.8
  1.33  0.23  0.33  0.62  0.39  0.41  2.37  1.65  1.54  2.95  0.56  5.
  5.56 23.98  1.91  0.55 34.44  2.72  2.79 17.71 28.31  2.39  1.18  3.01
  4.44 27.56  2.37 37.59 31.33  6.05 46.54 41.31  1.44 47.5   3.53  9.16
 34.48  1.78  2.37  8.49  6.88 20.86  1.36  1.71  7.5  35.29 24.38  3.95
 16.01  1.42 42.84  5.93  6.87  4.59  5.09  5.53  2.73  2.18  4.18  1.97
  3.29  5.01  3.96  5.76  9.82 18.16  3.26 11.66 10.94  7.22  7.36 13.83
  3.95  6.14 15.64 13.36  7.46  2.45  2.23  6.69  4.08 58.8  39.35  6.14
  3.92  7.07  7.76 46.57  6.64 68.05  7.1   7.99  3.8   7.25  8.17 21.3
  3.41  2.13 11.46  4.2   3.58  5.06  3.04  4.97  3.77  4.54  7.77  4.33
  4.58  8.42  2.33 11.91  8.35  4.42  3.62  5.47  4.73  6.3   3.35  9.2
  1.55  4.49  2.67 14.36  2.23  6.8   6.74 15.89  3.27  4.17  9.02  6.27
 27.12  2.65  5.49 26.34  4.16]

split sizes: train=642, val=71, test=0, N=713
Pre-trained model is loaded.
Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0.0
)
Scheduler: <torch.optim.lr_scheduler.MultiStepLR object at 0x7fb1041c2040>
#### TRAINING/VALIDATION ####
Epoch 2, loss = 0.4779
Mean test/val loss: 0.3665
[25, 50, 75] percentiles test/val loss: [0.1772 0.247  0.3522]

Epoch 4, loss = 0.4357
Mean test/val loss: 0.3109
[25, 50, 75] percentiles test/val loss: [0.1632 0.2243 0.3426]

Epoch 6, loss = 0.4090
Mean test/val loss: 0.3152
[25, 50, 75] percentiles test/val loss: [0.1711 0.2437 0.3669]

Epoch 8, loss = 0.3999
Mean test/val loss: 0.2975
[25, 50, 75] percentiles test/val loss: [0.1726 0.2384 0.3222]

Epoch 10, loss = 0.3827
Mean test/val loss: 0.3109
[25, 50, 75] percentiles test/val loss: [0.1787 0.2243 0.3029]

Epoch 12, loss = 0.3742
Mean test/val loss: 0.2876
[25, 50, 75] percentiles test/val loss: [0.1708 0.2252 0.2934]

Epoch 14, loss = 0.3786
Mean test/val loss: 0.2978
[25, 50, 75] percentiles test/val loss: [0.1739 0.241  0.3303]

Epoch 16, loss = 0.3542
Mean test/val loss: 0.2844
[25, 50, 75] percentiles test/val loss: [0.1532 0.2189 0.3153]

Epoch 18, loss = 0.3582
Mean test/val loss: 0.2919
[25, 50, 75] percentiles test/val loss: [0.1687 0.2214 0.3112]

Epoch 20, loss = 0.3460
Mean test/val loss: 0.2796
[25, 50, 75] percentiles test/val loss: [0.1643 0.2217 0.2966]

Epoch 22, loss = 0.3396
Mean test/val loss: 0.2778
[25, 50, 75] percentiles test/val loss: [0.1634 0.2131 0.2966]

Epoch 24, loss = 0.3497
Mean test/val loss: 0.2839
[25, 50, 75] percentiles test/val loss: [0.1589 0.2148 0.3051]

New lr: 1e-05
Epoch 26, loss = 0.3172
Mean test/val loss: 0.2728
[25, 50, 75] percentiles test/val loss: [0.1617 0.2151 0.2815]

Epoch 28, loss = 0.3005
Mean test/val loss: 0.2707
[25, 50, 75] percentiles test/val loss: [0.1501 0.2122 0.281 ]

Epoch 30, loss = 0.2952
Mean test/val loss: 0.2731
[25, 50, 75] percentiles test/val loss: [0.1508 0.2097 0.2818]

Epoch 32, loss = 0.2911
Mean test/val loss: 0.2694
[25, 50, 75] percentiles test/val loss: [0.1511 0.2105 0.2827]

Epoch 34, loss = 0.2875
Mean test/val loss: 0.2716
[25, 50, 75] percentiles test/val loss: [0.1498 0.2111 0.2833]

Epoch 36, loss = 0.2840
Mean test/val loss: 0.2732
[25, 50, 75] percentiles test/val loss: [0.15   0.2112 0.2826]

Epoch 38, loss = 0.2812
Mean test/val loss: 0.2743
[25, 50, 75] percentiles test/val loss: [0.1503 0.2103 0.2837]

Epoch 40, loss = 0.2780
Mean test/val loss: 0.2746
[25, 50, 75] percentiles test/val loss: [0.1495 0.2115 0.2845]


Total parameters: 26465972
Total training + validation time: 0.0 hours, 29.0 mins, and 55.90000000000009 secs
Final val loss: 0.2745692147020723

split sizes: train=642, val=71, test=0, N=713
#### Plotting Script ####
Prediction Results:
dataset_03_21_23_max_ent sample1519: 0.22349220514297485
dataset_03_21_23_max_ent sample1850: 0.2533164620399475
dataset_03_21_23_max_ent sample1780: 0.0830078125
dataset_03_21_23_max_ent sample1387: 0.24488171935081482
dataset_03_21_23_max_ent sample1842: 0.20182886719703674
Loss: 0.201 +- 0.062

Downsampling (40%) Results:
