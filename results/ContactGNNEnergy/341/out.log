#### ARCHITECTURE ####
Node Encoder:
 Sequential(
  (0): Linear(1, 64, bias=True)
  (1): PReLU(num_parameters=1)
) 

Edge Encoder:
 None 

Linear:
 Linear(in_features=72, out_features=64, bias=True) 

Model:
 Sequential(
  (0): WeightedGATv2Conv(64, 8, heads=8)
  (1): MLP(
  (model): Sequential(
    (0): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=64, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (1): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (2): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=64, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
  )
)
  (2): WeightedGATv2Conv(64, 8, heads=8)
  (3): MLP(
  (model): Sequential(
    (0): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=64, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (1): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (2): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=64, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
  )
)
  (4): WeightedGATv2Conv(64, 8, heads=8)
  (5): MLP(
  (model): Sequential(
    (0): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=64, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (1): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (2): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=64, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
  )
)
  (6): WeightedGATv2Conv(64, 8, heads=8)
  (7): MLP(
  (model): Sequential(
    (0): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=64, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (1): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=1000, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
    (2): LinearBlock(
      (model): Sequential(
        (0): Linear(in_features=1000, out_features=64, bias=True)
        (1): PReLU(num_parameters=1)
      )
    )
  )
)
) 

Head 1:
 Bilinear 

Head 2:
 Sequential(
  (0): MLP(
    (model): Sequential(
      (0): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=32768, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (1): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (2): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (3): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (4): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (5): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1000, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
      (6): LinearBlock(
        (model): Sequential(
          (0): Linear(in_features=1000, out_features=1024, bias=True)
          (1): PReLU(num_parameters=1)
        )
      )
    )
  )
  (1): FillDiagonalsFromArray()
) 

Namespace(GNN_mode=True, transforms=[], pre_transforms=['constant', 'ContactDistance', 'GeneticDistance_norm', 'AdjPCs_8'], sparsify_threshold=None, sparsify_threshold_upper=None, top_k=None, use_node_features=False, use_edge_weights=False, use_edge_attr=True, keep_zero_edges=False, data_folder=['/project/depablo/erschultz/dataset_11_18_22', '/project/depablo/erschultz/dataset_11_21_22'], scratch='/scratch/midway2/erschultz', root_name='ContactGNNEnergy3', delete_root=False, toxx=False, toxx_mode='mean', y_preprocessing='sweeprand_log_inf', y_zero_diag_count=0, log_preprocessing=None, output_preprocesing='log', kr=False, mean_filt=None, rescale=2, gated=False, preprocessing_norm='mean', min_subtraction=True, x_reshape=True, ydtype=torch.float32, y_reshape=True, crop=None, classes=10, use_scratch=False, use_scratch_parallel=True, split_percents=[0.9, 0.1, 0.0], split_sizes=None, random_split=True, shuffle=True, batch_size=1, num_workers=4, start_epoch=1, n_epochs=100, save_mod=5, print_mod=2, lr=0.0001, weight_decay=0.0, gpus=1, milestones=[50], gamma=0.1, loss='mse', w_reg=None, reg_lambda=0.1, autoencoder_mode=False, verbose=False, print_params=True, output_mode='energy_sym_diag', model_type='ContactGNNEnergy', id=341, pretrained=False, resume_training=False, k=8, m=1024, seed=42, act='prelu', inner_act='prelu', out_act='prelu', training_norm=None, dropout=0.0, parameter_sharing=False, use_bias=True, use_sign_net=False, use_sign_plus=True, message_passing='weighted_GAT', head_architecture='bilinear_triu', head_architecture_2='fc-fill', head_hidden_sizes_list=[1000, 1000, 1000, 1000, 1000, 1000, 1024], encoder_hidden_sizes_list=[64], inner_hidden_sizes_list=None, edge_encoder_hidden_sizes_list=None, update_hidden_sizes_list=[1000, 1000, 64], head_act='prelu', num_heads=8, concat_heads=True, max_diagonal=None, mlp_model_id=None, kernel_w_list=None, hidden_sizes_list=[8, 8, 8, 8], nf=None, dilation_list=None, dilation_list_trunk=None, bottleneck=None, dilation_list_head=None, down_sampling=None, plot=True, plot_predictions=True, ofile_folder='/home/erschultz/sequences_to_contact_maps/results/ContactGNNEnergy/341', log_file_path='/home/erschultz/sequences_to_contact_maps/results/ContactGNNEnergy/341/out.log', log_file=<_io.TextIOWrapper name='/home/erschultz/sequences_to_contact_maps/results/ContactGNNEnergy/341/out.log' mode='a' encoding='UTF-8'>, param_file=<_io.TextIOWrapper name='/home/erschultz/sequences_to_contact_maps/results/ContactGNNEnergy/341/params.log' mode='a' encoding='UTF-8'>, split_neg_pos_edges=False, criterion=<function mse_loss at 0x7f45750c8280>, channels=1, node_feature_size=1, input_m=512, edge_transforms=["'ContactDistance'", 'ContactDistance(norm=False)', 'GeneticDistance(norm=True)'], node_transforms=['AdjPCs(k=8, normalize=False, sign_net=True)', 'Constant(value=1.0)'], edge_dim=2, transforms_processed=None, diag=True, pre_transforms_processed=Compose([
  Constant(value=1.0),
  ContactDistance(norm=False),
  GeneticDistance(norm=True),
  AdjPCs(k=8, normalize=False, sign_net=True)
]), cuda=True, use_parallel=False, device=device(type='cuda'))

Dataset construction time: 27.698 minutes
Average num edges per graph:  222294.36527196653
Mean degree: [362.66 512.   449.47 ... 329.12 511.93 511.26] +- [70.54  0.   53.89 ... 80.99  0.29  1.69]

split sizes: train=4302, val=478, test=0, N=4780
Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0.0
)
#### TRAINING/VALIDATION ####
Epoch 2, loss = 0.7355
Mean test/val loss: 1.4336
[25, 50, 75] quantiles test/val loss: [0.4006 0.7564 1.1081]

Epoch 4, loss = 0.6751
Mean test/val loss: 0.6306
[25, 50, 75] quantiles test/val loss: [0.3386 0.6396 0.9049]

Epoch 6, loss = 0.6408
Mean test/val loss: 0.6064
[25, 50, 75] quantiles test/val loss: [0.2853 0.6012 0.8873]

Epoch 8, loss = 0.5878
Mean test/val loss: 0.5427
[25, 50, 75] quantiles test/val loss: [0.2723 0.5142 0.7921]

Epoch 10, loss = 0.5613
Mean test/val loss: 0.5208
[25, 50, 75] quantiles test/val loss: [0.2437 0.493  0.7727]

Epoch 12, loss = 0.5450
Mean test/val loss: 0.5082
[25, 50, 75] quantiles test/val loss: [0.2411 0.4854 0.7541]

Epoch 14, loss = 0.5294
Mean test/val loss: 0.5146
[25, 50, 75] quantiles test/val loss: [0.2612 0.4772 0.7509]

Epoch 16, loss = 0.5202
Mean test/val loss: 0.4977
[25, 50, 75] quantiles test/val loss: [0.22   0.4747 0.7541]

Epoch 18, loss = 0.5192
Mean test/val loss: 0.5030
[25, 50, 75] quantiles test/val loss: [0.23   0.4902 0.7391]

Epoch 20, loss = 0.5011
Mean test/val loss: 0.5041
[25, 50, 75] quantiles test/val loss: [0.2371 0.4637 0.7453]

Epoch 22, loss = 0.4902
Mean test/val loss: 0.4754
[25, 50, 75] quantiles test/val loss: [0.2124 0.4594 0.7103]

Epoch 24, loss = 0.4812
Mean test/val loss: 0.5087
[25, 50, 75] quantiles test/val loss: [0.2786 0.452  0.7224]

Epoch 26, loss = 0.5138
Mean test/val loss: 0.4731
[25, 50, 75] quantiles test/val loss: [0.2048 0.4461 0.7156]

Epoch 28, loss = 0.4768
Mean test/val loss: 0.4677
[25, 50, 75] quantiles test/val loss: [0.2056 0.4305 0.6995]

Epoch 30, loss = 0.4664
Mean test/val loss: 0.4503
[25, 50, 75] quantiles test/val loss: [0.1873 0.4195 0.6874]

Epoch 32, loss = 0.4584
Mean test/val loss: 0.4616
[25, 50, 75] quantiles test/val loss: [0.2032 0.4323 0.6937]

Epoch 34, loss = 0.4899
Mean test/val loss: 0.4586
[25, 50, 75] quantiles test/val loss: [0.1948 0.4359 0.6982]

Epoch 36, loss = 0.4498
Mean test/val loss: 0.4443
[25, 50, 75] quantiles test/val loss: [0.1878 0.412  0.6714]

Epoch 38, loss = 0.6083
Mean test/val loss: 0.5782
[25, 50, 75] quantiles test/val loss: [0.294  0.5759 0.8401]

Epoch 40, loss = 0.5359
Mean test/val loss: 0.5052
[25, 50, 75] quantiles test/val loss: [0.2437 0.4849 0.7477]

Epoch 42, loss = 0.4713
Mean test/val loss: 0.4527
[25, 50, 75] quantiles test/val loss: [0.1934 0.4275 0.6831]

Epoch 44, loss = 0.4390
Mean test/val loss: 0.4510
[25, 50, 75] quantiles test/val loss: [0.1999 0.4242 0.6829]

Epoch 46, loss = 0.4437
Mean test/val loss: 0.4456
[25, 50, 75] quantiles test/val loss: [0.1993 0.4162 0.6693]

Epoch 48, loss = 0.4460
Mean test/val loss: 0.4418
[25, 50, 75] quantiles test/val loss: [0.1856 0.4147 0.6761]

Epoch 50, loss = 0.4401
Mean test/val loss: 0.4482
[25, 50, 75] quantiles test/val loss: [0.1973 0.4125 0.6726]

Epoch 52, loss = 0.4157
Mean test/val loss: 0.4211
[25, 50, 75] quantiles test/val loss: [0.1704 0.3882 0.6492]

Epoch 54, loss = 0.4124
Mean test/val loss: 0.4193
[25, 50, 75] quantiles test/val loss: [0.1706 0.386  0.6444]

Epoch 56, loss = 0.4100
Mean test/val loss: 0.4178
[25, 50, 75] quantiles test/val loss: [0.1689 0.3849 0.6446]

Epoch 58, loss = 0.4080
Mean test/val loss: 0.4169
[25, 50, 75] quantiles test/val loss: [0.1669 0.3815 0.6432]

Epoch 60, loss = 0.4062
Mean test/val loss: 0.4166
[25, 50, 75] quantiles test/val loss: [0.168  0.3813 0.6412]

Epoch 62, loss = 0.4045
Mean test/val loss: 0.4160
[25, 50, 75] quantiles test/val loss: [0.1666 0.3832 0.6415]

Epoch 64, loss = 0.4029
Mean test/val loss: 0.4161
[25, 50, 75] quantiles test/val loss: [0.164  0.3808 0.6363]

Epoch 66, loss = 0.4015
Mean test/val loss: 0.4147
[25, 50, 75] quantiles test/val loss: [0.1667 0.3793 0.6341]

Epoch 68, loss = 0.4001
Mean test/val loss: 0.4144
[25, 50, 75] quantiles test/val loss: [0.1657 0.3821 0.6344]

Epoch 70, loss = 0.3987
Mean test/val loss: 0.4143
[25, 50, 75] quantiles test/val loss: [0.1645 0.3804 0.6334]

Epoch 72, loss = 0.3974
Mean test/val loss: 0.4135
[25, 50, 75] quantiles test/val loss: [0.1654 0.3806 0.6381]

Epoch 74, loss = 0.3962
Mean test/val loss: 0.4140
[25, 50, 75] quantiles test/val loss: [0.1645 0.3796 0.6356]

Epoch 76, loss = 0.3950
Mean test/val loss: 0.4134
[25, 50, 75] quantiles test/val loss: [0.1641 0.3776 0.6359]

Epoch 78, loss = 0.3943
Mean test/val loss: 0.4136
[25, 50, 75] quantiles test/val loss: [0.1657 0.375  0.6362]

Epoch 80, loss = 0.3931
Mean test/val loss: 0.4135
[25, 50, 75] quantiles test/val loss: [0.1657 0.3751 0.6319]

Epoch 82, loss = 0.3920
Mean test/val loss: 0.4131
[25, 50, 75] quantiles test/val loss: [0.164  0.3744 0.6291]

Epoch 84, loss = 0.3910
Mean test/val loss: 0.4126
[25, 50, 75] quantiles test/val loss: [0.1637 0.3753 0.6337]

Epoch 86, loss = 0.3899
Mean test/val loss: 0.4135
[25, 50, 75] quantiles test/val loss: [0.1643 0.377  0.6313]

Epoch 88, loss = 0.3889
