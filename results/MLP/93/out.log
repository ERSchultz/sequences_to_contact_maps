Took 2.0 seconds to move data to scratch
#### ARCHITECTURE ####
Sequential(
  (0): LinearBlock(
    (model): Sequential(
      (0): Linear(in_features=512, out_features=1000, bias=True)
      (1): PReLU(num_parameters=1)
    )
  )
  (1): LinearBlock(
    (model): Sequential(
      (0): Linear(in_features=1000, out_features=1000, bias=True)
      (1): PReLU(num_parameters=1)
    )
  )
  (2): LinearBlock(
    (model): Sequential(
      (0): Linear(in_features=1000, out_features=1000, bias=True)
      (1): PReLU(num_parameters=1)
    )
  )
  (3): LinearBlock(
    (model): Sequential(
      (0): Linear(in_features=1000, out_features=1000, bias=True)
      (1): PReLU(num_parameters=1)
    )
  )
  (4): LinearBlock(
    (model): Sequential(
      (0): Linear(in_features=1000, out_features=1000, bias=True)
      (1): PReLU(num_parameters=1)
    )
  )
  (5): LinearBlock(
    (model): Sequential(
      (0): Linear(in_features=1000, out_features=1000, bias=True)
      (1): PReLU(num_parameters=1)
    )
  )
  (6): LinearBlock(
    (model): Sequential(
      (0): Linear(in_features=1000, out_features=1024, bias=True)
      (1): PReLU(num_parameters=1)
    )
  )
)
Namespace(GNN_mode=False, transforms=[], pre_transforms=[], sparsify_threshold=None, sparsify_threshold_upper=None, top_k=None, use_node_features=False, use_edge_weights=True, use_edge_attr=False, relabel_11_to_00=False, split_edges_for_feature_augmentation=False, data_folder='/home/erschultz/scratch/MLP1/dataset_09_30_22', scratch='/home/erschultz/scratch/MLP1', root_name=None, delete_root=True, toxx=False, toxx_mode='mean', y_preprocessing='log', y_zero_diag_count=0, log_preprocessing=None, preprocessing_norm='mean', min_subtraction=True, x_reshape=True, ydtype=torch.float32, y_reshape=True, crop=[0, 512], classes=10, use_scratch=True, use_scratch_parallel=False, split_percents=[0.9, 0.1, 0.0], split_sizes=None, random_split=True, shuffle=True, batch_size=32, num_workers=8, start_epoch=1, n_epochs=100, save_mod=5, print_mod=2, lr=0.001, gpus=1, milestones=[50], gamma=0.1, loss='mse', autoencoder_mode=False, verbose=False, print_params=True, output_mode='diag_chi_continuous', model_type='MLP', id=93, pretrained=False, resume_training=False, k=None, m=512, seed=42, act='prelu', inner_act=None, out_act='prelu', training_norm=None, dropout=None, dropout_p=0.2, parameter_sharing=False, use_bias=True, message_passing='GCN', head_architecture=None, head_architecture_2=None, head_hidden_sizes_list=None, encoder_hidden_sizes_list=None, update_hidden_sizes_list=None, head_act='relu', num_heads=1, concat_heads=True, max_diagonal=None, mlp_model_id=None, kernel_w_list=None, hidden_sizes_list=[1000, 1000, 1000, 1000, 1000, 1000, 1024], nf=None, dilation_list=None, dilation_list_trunk=None, bottleneck=None, dilation_list_head=None, down_sampling=None, plot=True, plot_predictions=True, ofile_folder='/home/erschultz/sequences_to_contact_maps/results/MLP/93', log_file_path='/home/erschultz/sequences_to_contact_maps/results/MLP/93/out.log', log_file=<_io.TextIOWrapper name='/home/erschultz/sequences_to_contact_maps/results/MLP/93/out.log' mode='a' encoding='UTF-8'>, param_file=<_io.TextIOWrapper name='/home/erschultz/sequences_to_contact_maps/results/MLP/93/params.log' mode='a' encoding='UTF-8'>, split_neg_pos_edges=False, criterion=<function mse_loss at 0x7f90b5dbce50>, channels=1, edge_transforms=[], node_transforms=[], edge_dim=0, transforms_processed=None, pre_transforms_processed=None, cuda=True, use_parallel=False, device=device(type='cuda'))

split sizes: train=2268, val=252, test=0, N=2520
#### TRAINING/VALIDATION ####
Epoch 2, loss = 5.1697
Mean test/val loss: 8.5601

Epoch 4, loss = 4.6163
Mean test/val loss: 4.2698

Epoch 6, loss = 3.5780
Mean test/val loss: 3.1378

Epoch 8, loss = 3.6823
Mean test/val loss: 3.4241

Epoch 10, loss = 3.4945
Mean test/val loss: 4.1503

Epoch 12, loss = 3.7348
Mean test/val loss: 4.0865

Epoch 14, loss = 3.2335
Mean test/val loss: 2.9567

Epoch 16, loss = 3.4493
Mean test/val loss: 3.1686

Epoch 18, loss = 3.2532
Mean test/val loss: 5.6513

Epoch 20, loss = 3.2350
Mean test/val loss: 3.1129

Epoch 22, loss = 3.1433
Mean test/val loss: 2.9348

Epoch 24, loss = 3.1696
Mean test/val loss: 2.9991

Epoch 26, loss = 2.9758
Mean test/val loss: 3.2668

Epoch 28, loss = 3.2840
Mean test/val loss: 3.0683

Epoch 30, loss = 3.1864
Mean test/val loss: 4.1264

Epoch 32, loss = 2.8117
Mean test/val loss: 2.8315

Epoch 34, loss = 2.8833
Mean test/val loss: 3.2865

Epoch 36, loss = 2.8877
Mean test/val loss: 3.0255

Epoch 38, loss = 3.0990
Mean test/val loss: 3.2662

Epoch 40, loss = 2.9871
Mean test/val loss: 2.8542

Epoch 42, loss = 2.6585
Mean test/val loss: 3.4319

Epoch 44, loss = 2.7076
Mean test/val loss: 2.7349

Epoch 46, loss = 2.8849
Mean test/val loss: 3.1192

Epoch 48, loss = 2.7204
Mean test/val loss: 2.8236

Epoch 50, loss = 2.7866
Mean test/val loss: 3.3732

Epoch 52, loss = 2.2330
Mean test/val loss: 2.7230

Epoch 54, loss = 2.1954
Mean test/val loss: 2.7258

Epoch 56, loss = 2.2067
Mean test/val loss: 2.7071

Epoch 58, loss = 2.1688
Mean test/val loss: 2.7828

Epoch 60, loss = 2.1503
Mean test/val loss: 2.6771

Epoch 62, loss = 2.1488
Mean test/val loss: 2.7363

Epoch 64, loss = 2.1180
Mean test/val loss: 2.6927

Epoch 66, loss = 2.1513
Mean test/val loss: 2.7938

Epoch 68, loss = 2.1232
Mean test/val loss: 2.7205

Epoch 70, loss = 2.0948
Mean test/val loss: 2.6664

Epoch 72, loss = 2.0615
Mean test/val loss: 2.7139

Epoch 74, loss = 2.0458
Mean test/val loss: 2.7188

Epoch 76, loss = 2.0570
Mean test/val loss: 2.7373

Epoch 78, loss = 2.0289
Mean test/val loss: 2.7891

Epoch 80, loss = 2.0414
Mean test/val loss: 2.7351

Epoch 82, loss = 1.9958
Mean test/val loss: 2.6359

Epoch 84, loss = 1.9939
Mean test/val loss: 2.7402

Epoch 86, loss = 1.9570
Mean test/val loss: 2.7302

Epoch 88, loss = 1.9922
Mean test/val loss: 2.7907

Epoch 90, loss = 1.9132
Mean test/val loss: 2.8819

Epoch 92, loss = 1.9316
Mean test/val loss: 2.7598

Epoch 94, loss = 1.9377
Mean test/val loss: 2.8481

Epoch 96, loss = 1.8745
Mean test/val loss: 2.7252

Epoch 98, loss = 1.9050
Mean test/val loss: 2.7678

Epoch 100, loss = 1.8348
Mean test/val loss: 3.0217


Total parameters: 6,543,031
Total training + validation time: 0.0 hours and 2.0 mins
Final val loss: 3.02166385948658

split sizes: train=2268, val=252, test=0, N=2520
#### Plotting Script ####
Prediction Results:
/home/erschultz/sequences_to_contact_maps/results/MLP/93/sample552: <function plot_diag_chi at 0x7f8f7fe52ca0>
/home/erschultz/sequences_to_contact_maps/results/MLP/93/sample1794: <function plot_diag_chi at 0x7f8f7fe52ca0>
/home/erschultz/sequences_to_contact_maps/results/MLP/93/sample1128: <function plot_diag_chi at 0x7f8f7fe52ca0>
/home/erschultz/sequences_to_contact_maps/results/MLP/93/sample1938: <function plot_diag_chi at 0x7f8f7fe52ca0>
/home/erschultz/sequences_to_contact_maps/results/MLP/93/sample1131: <function plot_diag_chi at 0x7f8f7fe52ca0>
Loss: 3.6409492933132115 +- 3.06297407381138

