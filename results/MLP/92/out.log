Took 2.0 seconds to move data to scratch
#### ARCHITECTURE ####
Sequential(
  (0): LinearBlock(
    (model): Sequential(
      (0): Linear(in_features=1024, out_features=100, bias=True)
      (1): PReLU(num_parameters=1)
    )
  )
  (1): LinearBlock(
    (model): Sequential(
      (0): Linear(in_features=100, out_features=100, bias=True)
      (1): PReLU(num_parameters=1)
    )
  )
  (2): LinearBlock(
    (model): Sequential(
      (0): Linear(in_features=100, out_features=100, bias=True)
      (1): PReLU(num_parameters=1)
    )
  )
  (3): LinearBlock(
    (model): Sequential(
      (0): Linear(in_features=100, out_features=100, bias=True)
      (1): PReLU(num_parameters=1)
    )
  )
  (4): LinearBlock(
    (model): Sequential(
      (0): Linear(in_features=100, out_features=100, bias=True)
      (1): PReLU(num_parameters=1)
    )
  )
  (5): LinearBlock(
    (model): Sequential(
      (0): Linear(in_features=100, out_features=100, bias=True)
      (1): PReLU(num_parameters=1)
    )
  )
  (6): LinearBlock(
    (model): Sequential(
      (0): Linear(in_features=100, out_features=1024, bias=True)
      (1): PReLU(num_parameters=1)
    )
  )
)
Namespace(GNN_mode=False, transforms=[], pre_transforms=[], sparsify_threshold=None, sparsify_threshold_upper=None, top_k=None, use_node_features=False, use_edge_weights=True, use_edge_attr=False, relabel_11_to_00=False, split_edges_for_feature_augmentation=False, data_folder='/home/erschultz/scratch/MLP1/dataset_09_30_22', scratch='/home/erschultz/scratch/MLP1', root_name=None, delete_root=True, toxx=False, toxx_mode='mean', y_preprocessing='log', y_zero_diag_count=0, log_preprocessing=None, preprocessing_norm='mean', min_subtraction=True, x_reshape=True, ydtype=torch.float32, y_reshape=True, crop=None, classes=10, use_scratch=True, use_scratch_parallel=False, split_percents=[0.9, 0.1, 0.0], split_sizes=None, random_split=True, shuffle=True, batch_size=32, num_workers=8, start_epoch=1, n_epochs=100, save_mod=5, print_mod=2, lr=0.001, gpus=1, milestones=[50], gamma=0.1, loss='mse', autoencoder_mode=False, verbose=False, print_params=True, output_mode='diag_chi_continuous', model_type='MLP', id=92, pretrained=False, resume_training=False, k=None, m=1024, seed=42, act='prelu', inner_act=None, out_act='prelu', training_norm=None, dropout=None, dropout_p=0.2, parameter_sharing=False, use_bias=True, message_passing='GCN', head_architecture=None, head_architecture_2=None, head_hidden_sizes_list=None, encoder_hidden_sizes_list=None, update_hidden_sizes_list=None, head_act='relu', num_heads=1, concat_heads=True, max_diagonal=None, mlp_model_id=None, kernel_w_list=None, hidden_sizes_list=[100, 100, 100, 100, 100, 100, 1024], nf=None, dilation_list=None, dilation_list_trunk=None, bottleneck=None, dilation_list_head=None, down_sampling=None, plot=True, plot_predictions=True, ofile_folder='/home/erschultz/sequences_to_contact_maps/results/MLP/92', log_file_path='/home/erschultz/sequences_to_contact_maps/results/MLP/92/out.log', log_file=<_io.TextIOWrapper name='/home/erschultz/sequences_to_contact_maps/results/MLP/92/out.log' mode='a' encoding='UTF-8'>, param_file=<_io.TextIOWrapper name='/home/erschultz/sequences_to_contact_maps/results/MLP/92/params.log' mode='a' encoding='UTF-8'>, split_neg_pos_edges=False, criterion=<function mse_loss at 0x7f8f92ee4310>, channels=1, edge_transforms=[], node_transforms=[], edge_dim=0, transforms_processed=None, pre_transforms_processed=None, cuda=True, use_parallel=False, device=device(type='cuda'))

split sizes: train=2268, val=252, test=0, N=2520
#### TRAINING/VALIDATION ####
Epoch 2, loss = 8.3106
Mean test/val loss: 5.0475

Epoch 4, loss = 3.1054
Mean test/val loss: 3.0376

Epoch 6, loss = 2.6183
Mean test/val loss: 2.7152

Epoch 8, loss = 2.2101
Mean test/val loss: 2.6581

Epoch 10, loss = 2.1685
Mean test/val loss: 2.0826

Epoch 12, loss = 1.9737
Mean test/val loss: 2.0022

Epoch 14, loss = 1.7749
Mean test/val loss: 1.9943

Epoch 16, loss = 1.7750
Mean test/val loss: 1.9817

Epoch 18, loss = 1.7019
Mean test/val loss: 2.0567

Epoch 20, loss = 1.6479
Mean test/val loss: 1.9626

Epoch 22, loss = 1.7852
Mean test/val loss: 1.8686

Epoch 24, loss = 1.5934
Mean test/val loss: 1.8056

Epoch 26, loss = 1.6198
Mean test/val loss: 1.9240

Epoch 28, loss = 1.6278
Mean test/val loss: 1.8282

Epoch 30, loss = 1.6312
Mean test/val loss: 1.7861

Epoch 32, loss = 1.4729
Mean test/val loss: 1.7871

Epoch 34, loss = 1.4800
Mean test/val loss: 1.6818

Epoch 36, loss = 1.4853
Mean test/val loss: 1.7034

Epoch 38, loss = 1.4341
Mean test/val loss: 1.9243

Epoch 40, loss = 1.5284
Mean test/val loss: 1.7937

Epoch 42, loss = 1.4398
Mean test/val loss: 1.8679

Epoch 44, loss = 1.4274
Mean test/val loss: 1.7216

Epoch 46, loss = 1.4334
Mean test/val loss: 2.0721

Epoch 48, loss = 1.4205
Mean test/val loss: 2.2224

Epoch 50, loss = 1.3371
Mean test/val loss: 1.7693

Epoch 52, loss = 1.1955
Mean test/val loss: 1.5875

Epoch 54, loss = 1.1948
Mean test/val loss: 1.6131

Epoch 56, loss = 1.1886
Mean test/val loss: 1.5841

Epoch 58, loss = 1.1791
Mean test/val loss: 1.6470

Epoch 60, loss = 1.1745
Mean test/val loss: 1.5893

Epoch 62, loss = 1.1689
Mean test/val loss: 1.5743

Epoch 64, loss = 1.1713
Mean test/val loss: 1.6239

Epoch 66, loss = 1.1707
Mean test/val loss: 1.5886

Epoch 68, loss = 1.1790
Mean test/val loss: 1.6295

Epoch 70, loss = 1.1573
Mean test/val loss: 1.5927

Epoch 72, loss = 1.1573
Mean test/val loss: 1.6143

Epoch 74, loss = 1.1523
Mean test/val loss: 1.5681

Epoch 76, loss = 1.1378
Mean test/val loss: 1.5854

Epoch 78, loss = 1.1345
Mean test/val loss: 1.5603

Epoch 80, loss = 1.1410
Mean test/val loss: 1.5955

Epoch 82, loss = 1.1348
Mean test/val loss: 1.5795

Epoch 84, loss = 1.1284
Mean test/val loss: 1.6188

Epoch 86, loss = 1.1458
Mean test/val loss: 1.6196

Epoch 88, loss = 1.1339
Mean test/val loss: 1.5709

Epoch 90, loss = 1.1255
Mean test/val loss: 1.5567

Epoch 92, loss = 1.1247
Mean test/val loss: 1.5670

Epoch 94, loss = 1.1166
Mean test/val loss: 1.6104

Epoch 96, loss = 1.1044
Mean test/val loss: 1.5756

Epoch 98, loss = 1.1065
Mean test/val loss: 1.5556

Epoch 100, loss = 1.0953
Mean test/val loss: 1.6122


Total parameters: 256,431
Total training + validation time: 0.0 hours
Final val loss: 1.6122271418571472

split sizes: train=2268, val=252, test=0, N=2520
#### Plotting Script ####
Prediction Results:
/home/erschultz/sequences_to_contact_maps/results/MLP/92/sample552: <function plot_diag_chi at 0x7f8e4db14550>
/home/erschultz/sequences_to_contact_maps/results/MLP/92/sample1794: <function plot_diag_chi at 0x7f8e4db14550>
/home/erschultz/sequences_to_contact_maps/results/MLP/92/sample1128: <function plot_diag_chi at 0x7f8e4db14550>
/home/erschultz/sequences_to_contact_maps/results/MLP/92/sample1938: <function plot_diag_chi at 0x7f8e4db14550>
/home/erschultz/sequences_to_contact_maps/results/MLP/92/sample1131: <function plot_diag_chi at 0x7f8e4db14550>
Loss: 0.7729271560226179 +- 0.9756567593769288

