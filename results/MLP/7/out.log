Took 0.0 seconds to move data to scratch
#### ARCHITECTURE ####
Sequential(
  (0): LinearBlock(
    (model): Sequential(
      (0): Linear(in_features=1024, out_features=2000, bias=True)
      (1): PReLU(num_parameters=1)
    )
  )
  (1): LinearBlock(
    (model): Sequential(
      (0): Linear(in_features=2000, out_features=2000, bias=True)
      (1): PReLU(num_parameters=1)
    )
  )
  (2): LinearBlock(
    (model): Sequential(
      (0): Linear(in_features=2000, out_features=2000, bias=True)
      (1): PReLU(num_parameters=1)
    )
  )
  (3): LinearBlock(
    (model): Sequential(
      (0): Linear(in_features=2000, out_features=2000, bias=True)
      (1): PReLU(num_parameters=1)
    )
  )
  (4): LinearBlock(
    (model): Sequential(
      (0): Linear(in_features=2000, out_features=2000, bias=True)
      (1): PReLU(num_parameters=1)
    )
  )
  (5): LinearBlock(
    (model): Sequential(
      (0): Linear(in_features=2000, out_features=2000, bias=True)
      (1): PReLU(num_parameters=1)
    )
  )
  (6): LinearBlock(
    (model): Sequential(
      (0): Linear(in_features=2000, out_features=20, bias=True)
      (1): PReLU(num_parameters=1)
    )
  )
)
Namespace(GNN_mode=False, transforms=[], pre_transforms=[], sparsify_threshold=None, sparsify_threshold_upper=None, top_k=None, use_node_features=False, use_edge_weights=True, use_edge_attr=False, relabel_11_to_00=False, split_edges_for_feature_augmentation=False, data_folder='/home/erschultz/scratch/dataset_test_diag1024', scratch='/home/erschultz/scratch', root_name=None, delete_root=True, toxx=False, toxx_mode='mean', y_preprocessing=None, y_zero_diag_count=4, log_preprocessing='ln', preprocessing_norm='instance', min_subtraction=True, x_reshape=True, ydtype=torch.float32, y_reshape=True, crop=None, classes=10, use_scratch=True, use_scratch_parallel=False, split_percents=[0.8, 0.2, 0], split_sizes=None, random_split=True, shuffle=True, batch_size=10, num_workers=4, start_epoch=1, n_epochs=80, save_mod=5, print_mod=2, lr=0.001, gpus=0, milestones=[20, 50], gamma=0.1, loss='mse', autoencoder_mode=False, verbose=False, print_params=True, output_mode='diag_chi', model_type='MLP', id=7, pretrained=False, resume_training=False, k=None, m=1024, seed=42, act='prelu', inner_act=None, out_act='prelu', training_norm=None, dropout=None, dropout_p=0.2, parameter_sharing=False, use_bias=True, message_passing='GCN', head_architecture=None, head_hidden_sizes_list=None, encoder_hidden_sizes_list=None, update_hidden_sizes_list=None, head_act='relu', num_heads=1, concat_heads=True, kernel_w_list=None, hidden_sizes_list=[2000, 2000, 2000, 2000, 2000, 2000, 20], nf=None, dilation_list=None, dilation_list_trunk=None, bottleneck=None, dilation_list_head=None, down_sampling=None, plot=True, plot_predictions=True, ofile_folder='/home/erschultz/sequences_to_contact_maps/results/MLP/7', log_file_path='/home/erschultz/sequences_to_contact_maps/results/MLP/7/out.log', log_file=<_io.TextIOWrapper name='/home/erschultz/sequences_to_contact_maps/results/MLP/7/out.log' mode='a' encoding='UTF-8'>, param_file=<_io.TextIOWrapper name='/home/erschultz/sequences_to_contact_maps/results/MLP/7/params.log' mode='a' encoding='UTF-8'>, split_neg_pos_edges=False, criterion=<function mse_loss at 0x7f1fd9fa5310>, channels=1, edge_transforms=[], node_transforms=[], edge_dim=0, transforms_processed=None, pre_transforms_processed=None, cuda=False, use_parallel=False, device=device(type='cpu'))

#### TRAINING/VALIDATION ####
Epoch 2, loss = 167.4352
Mean test/val loss: 71.3014

Epoch 4, loss = 5379.3486
Mean test/val loss: 194.2486

Epoch 6, loss = 646.7097
Mean test/val loss: 861.1111

Epoch 8, loss = 159.1010
Mean test/val loss: 100.0417

Epoch 10, loss = 50.1090
Mean test/val loss: 41.2060

Epoch 12, loss = 115.8284
Mean test/val loss: 56.9787

Epoch 14, loss = 31.8093
Mean test/val loss: 17.1791

Epoch 16, loss = 4.0661
Mean test/val loss: 3.3867

Epoch 18, loss = 1.7642
Mean test/val loss: 1.4601

Epoch 20, loss = 1.1403
Mean test/val loss: 1.0116

Epoch 22, loss = 0.7943
Mean test/val loss: 0.8319

Epoch 24, loss = 1.0485
Mean test/val loss: 0.7939

Epoch 26, loss = 0.7982
Mean test/val loss: 0.8606

Epoch 28, loss = 0.6926
Mean test/val loss: 0.7529

Epoch 30, loss = 0.7909
Mean test/val loss: 0.7996

Epoch 32, loss = 0.7405
Mean test/val loss: 0.7788

Epoch 34, loss = 0.7593
Mean test/val loss: 0.7787

Epoch 36, loss = 0.7231
Mean test/val loss: 0.7993

Epoch 38, loss = 0.6646
Mean test/val loss: 0.7649

Epoch 40, loss = 0.6864
Mean test/val loss: 0.7803

Epoch 42, loss = 0.7376
Mean test/val loss: 0.7880

Epoch 44, loss = 0.7061
Mean test/val loss: 0.7990

Epoch 46, loss = 0.7669
Mean test/val loss: 0.7455

Epoch 48, loss = 0.6468
Mean test/val loss: 0.7882

Epoch 50, loss = 0.6525
Mean test/val loss: 0.7595

Epoch 52, loss = 0.7643
Mean test/val loss: 0.7597

Epoch 54, loss = 0.6932
Mean test/val loss: 0.7695

Epoch 56, loss = 0.6471
Mean test/val loss: 0.7676

Epoch 58, loss = 0.9399
Mean test/val loss: 0.7804

Epoch 60, loss = 0.6647
Mean test/val loss: 0.7289

Epoch 62, loss = 0.6840
Mean test/val loss: 0.7608

Epoch 64, loss = 0.7664
Mean test/val loss: 0.7520

Epoch 66, loss = 0.6561
Mean test/val loss: 0.7219

Epoch 68, loss = 0.6566
Mean test/val loss: 0.7707

Epoch 70, loss = 0.6863
Mean test/val loss: 0.7643

Epoch 72, loss = 0.6694
Mean test/val loss: 0.7799

Epoch 74, loss = 0.6896
Mean test/val loss: 0.7287

Epoch 76, loss = 0.6619
Mean test/val loss: 0.7436

Epoch 78, loss = 0.6385
Mean test/val loss: 0.7557

Epoch 80, loss = 0.7129
Mean test/val loss: 0.7275


Total parameters: 22,100,027
Total training + validation time: 0.0 hours
Final val loss: 0.7275483459234238

#### Plotting Script ####
Prediction Results:
/home/erschultz/sequences_to_contact_maps/results/MLP/7/sample251: 0.16878575086593628
/home/erschultz/sequences_to_contact_maps/results/MLP/7/sample215: 1.2010740041732788
/home/erschultz/sequences_to_contact_maps/results/MLP/7/sample252: 0.22000393271446228
/home/erschultz/sequences_to_contact_maps/results/MLP/7/sample289: 1.1301504373550415
/home/erschultz/sequences_to_contact_maps/results/MLP/7/sample230: 0.17989549040794373
Loss: 0.5799819231033325 +- 0.4789940221956319

